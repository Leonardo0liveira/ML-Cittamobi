import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import (roc_auc_score, accuracy_score, precision_score, 
                             recall_score, f1_score, confusion_matrix, classification_report, roc_curve)
from sklearn.model_selection import TimeSeriesSplit
from catboost import CatBoostClassifier, Pool
from collections import Counter
from google.cloud import bigquery
import os
import warnings
warnings.filterwarnings('ignore')

# ===========================================================================
# MODELO V6 CATBOOST - SEM VAZAMENTO DE DADOS (LEAK-FREE)
# ===========================================================================
print(f"\n{'='*80}")
print(f"MODELO V6 CATBOOST - SEM VAZAMENTO DE DADOS (LEAK-FREE)")
print(f"{'='*80}")
print(f"CORRE√á√ÉO: Removendo features que usam target + Expanding Windows")
print(f"{'='*80}")

# Carregar dados do BigQuery
project_id = "proj-ml-469320"
client = bigquery.Client(project=project_id)

query = """
    SELECT * FROM `proj-ml-469320.app_cittamobi.dataset-updated` 
    TABLESAMPLE SYSTEM (20 PERCENT)
    LIMIT 50000
"""

print("Carregando dados do BigQuery...")
df = client.query(query).to_dataframe()
print(f"‚úì Dados carregados: {len(df):,} registros")

# ===========================================================================
# AN√ÅLISE DO VAZAMENTO DE DADOS
# ===========================================================================
print(f"\n{'='*70}")
print(f"AN√ÅLISE DO VAZAMENTO DE DADOS ANTERIOR")
print(f"{'='*70}")

print("üö® FEATURES COM VAZAMENTO DETECTADAS:")
print("   1. user_conversion_rate = target.mean() por usu√°rio")
print("   2. user_total_conversions = target.sum() por usu√°rio") 
print("   3. stop_conversion_rate = target.mean() por parada")
print("   4. conversion_interaction = user_conversion_rate * stop_conversion_rate")
print("   5. Todas calculadas usando o pr√≥prio target!")

print("\nüí° SOLU√á√ÉO: EXPANDING WINDOWS")
print("   - Para cada evento no tempo T, usar apenas dados < T")
print("   - Nunca usar informa√ß√µes do futuro")
print("   - Simular ambiente de produ√ß√£o real")

# ===========================================================================
# PREPARA√á√ÉO TEMPORAL DOS DADOS
# ===========================================================================
print(f"\n{'='*70}")
print(f"ETAPA 1: PREPARA√á√ÉO TEMPORAL DOS DADOS")
print(f"{'='*70}")

target = "target"

# Converter timestamp e ordenar temporalmente
df['event_timestamp'] = pd.to_datetime(df['event_timestamp'], format='ISO8601')
df = df.sort_values('event_timestamp').reset_index(drop=True)

print(f"‚úì Dados ordenados temporalmente")
print(f"‚úì Per√≠odo: {df['event_timestamp'].min()} at√© {df['event_timestamp'].max()}")

# Features b√°sicas temporais
df['year'] = df['event_timestamp'].dt.year
df['month'] = df['event_timestamp'].dt.month
df['day'] = df['event_timestamp'].dt.day
df['hour'] = df['event_timestamp'].dt.hour
df['dayofweek'] = df['event_timestamp'].dt.dayofweek
df['minute'] = df['event_timestamp'].dt.minute
df['week_of_year'] = df['event_timestamp'].dt.isocalendar().week

# Features c√≠clicas
if 'time_day_of_month' in df.columns:
    df['day_of_month_sin'] = np.sin(2 * np.pi * df['time_day_of_month'] / 31)
    df['day_of_month_cos'] = np.cos(2 * np.pi * df['time_day_of_month'] / 31)

df['week_sin'] = np.sin(2 * np.pi * df['week_of_year'] / 52)
df['week_cos'] = np.cos(2 * np.pi * df['week_of_year'] / 52)
df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)

print(f"‚úì Features temporais criadas")

# ===========================================================================
# EXPANDING WINDOWS - SEM VAZAMENTO
# ===========================================================================
print(f"\n{'='*70}")
print(f"ETAPA 2: EXPANDING WINDOWS (SEM VAZAMENTO)")
print(f"{'='*70}")

def create_expanding_features_leak_free(df):
    """
    Cria features usando expanding windows para prevenir vazamento de dados.
    Para cada linha i, usa apenas dados das linhas 0 at√© i-1.
    """
    df_result = df.copy()
    
    # Inicializar colunas
    user_cols = ['user_historical_events', 'user_historical_conversions', 
                 'user_historical_conversion_rate', 'user_avg_hour_hist', 'user_avg_dist_hist']
    
    stop_cols = ['stop_historical_events', 'stop_historical_conversions',
                 'stop_historical_conversion_rate', 'stop_avg_freq_hist']
    
    for col in user_cols + stop_cols:
        df_result[col] = 0.0
    
    print("üìä Calculando expanding windows...")
    
    # Para cada linha, calcular features baseadas apenas no hist√≥rico anterior
    for i in range(len(df)):
        if i % 10000 == 0:
            print(f"   Processando linha {i:,}/{len(df):,} ({i/len(df)*100:.1f}%)")
        
        if i == 0:
            # Primeira linha: sem hist√≥rico
            continue
            
        # Dados hist√≥ricos (apenas linhas anteriores)
        hist_data = df.iloc[:i]  # Apenas dados ANTERIORES √† linha atual
        current_user = df.iloc[i]['user_pseudo_id']
        current_stop = df.iloc[i]['gtfs_stop_id']
        
        # Features do usu√°rio (hist√≥rico)
        if current_user in hist_data['user_pseudo_id'].values:
            user_hist = hist_data[hist_data['user_pseudo_id'] == current_user]
            df_result.iloc[i, df_result.columns.get_loc('user_historical_events')] = len(user_hist)
            df_result.iloc[i, df_result.columns.get_loc('user_historical_conversions')] = user_hist[target].sum()
            
            if len(user_hist) > 0:
                df_result.iloc[i, df_result.columns.get_loc('user_historical_conversion_rate')] = user_hist[target].mean()
                df_result.iloc[i, df_result.columns.get_loc('user_avg_hour_hist')] = user_hist['hour'].mean()
                
                if 'dist_device_stop' in user_hist.columns:
                    df_result.iloc[i, df_result.columns.get_loc('user_avg_dist_hist')] = user_hist['dist_device_stop'].mean()
        
        # Features da parada (hist√≥rico)  
        if current_stop in hist_data['gtfs_stop_id'].values:
            stop_hist = hist_data[hist_data['gtfs_stop_id'] == current_stop]
            df_result.iloc[i, df_result.columns.get_loc('stop_historical_events')] = len(stop_hist)
            df_result.iloc[i, df_result.columns.get_loc('stop_historical_conversions')] = stop_hist[target].sum()
            
            if len(stop_hist) > 0:
                df_result.iloc[i, df_result.columns.get_loc('stop_historical_conversion_rate')] = stop_hist[target].mean()
                
                if 'user_frequency' in stop_hist.columns:
                    df_result.iloc[i, df_result.columns.get_loc('stop_avg_freq_hist')] = stop_hist['user_frequency'].mean()
    
    return df_result

print("\nüîÑ Criando expanding windows (pode demorar alguns minutos)...")
df_with_expanding = create_expanding_features_leak_free(df)

print(f"‚úì Expanding windows criadas!")
print(f"‚úì Features hist√≥ricas sem vazamento de dados")

# Features de intera√ß√£o baseadas no hist√≥rico (SEM VAZAMENTO)
df_with_expanding['historical_interaction'] = (
    df_with_expanding['user_historical_conversion_rate'] * 
    df_with_expanding['stop_historical_conversion_rate']
)

df_with_expanding['user_stop_historical_affinity'] = (
    df_with_expanding['user_historical_events'] * 
    df_with_expanding['stop_historical_events']
)

# Desvio da dist√¢ncia baseado no hist√≥rico
if 'dist_device_stop' in df_with_expanding.columns:
    df_with_expanding['dist_deviation_hist'] = abs(
        df_with_expanding['dist_device_stop'] - df_with_expanding['user_avg_dist_hist']
    )

print(f"‚úì Features de intera√ß√£o hist√≥ricas criadas")

# ===========================================================================
# LIMPEZA E PREPARA√á√ÉO FINAL
# ===========================================================================
print(f"\n{'='*70}")
print(f"ETAPA 3: LIMPEZA E PREPARA√á√ÉO FINAL")
print(f"{'='*70}")

# Filtros moderados (como antes)
df_clean = df_with_expanding.copy()

if 'user_frequency' in df_clean.columns:
    user_freq_threshold = df_clean['user_frequency'].quantile(0.10)
    df_clean = df_clean[df_clean['user_frequency'] >= user_freq_threshold]

if 'device_lat' in df_clean.columns and 'device_lon' in df_clean.columns:
    df_clean = df_clean[~((df_clean['device_lat'].isna()) | (df_clean['device_lon'].isna()))]
    df_clean = df_clean[~((df_clean['device_lat'] == 0) & (df_clean['device_lon'] == 0))]

if 'dist_device_stop' in df_clean.columns:
    dist_threshold = df_clean['dist_device_stop'].quantile(0.98)
    df_clean = df_clean[df_clean['dist_device_stop'] <= dist_threshold]

print(f"‚úì Dados limpos: {len(df_clean):,} registros mantidos")

# Preparar features (SEM as que causam vazamento)
features_to_drop = [
    'y_pred', 'y_pred_proba', 'ctm_service_route', 'direction', 'lotacao_proxy_binaria',
    'event_timestamp',  # Temporal j√° processado
    # REMOVENDO FEATURES COM VAZAMENTO:
    'user_conversion_rate', 'user_total_conversions', 'stop_conversion_rate',
    'conversion_interaction', 'user_stop_affinity'  # Baseadas em target
]

X = df_clean.drop(columns=[target] + features_to_drop, errors='ignore')
y = df_clean[target]

# Identificar categ√≥ricas
categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()
X = X.replace([np.inf, -np.inf], np.nan)

print(f"‚úì Features finais: {X.shape[1]}")
print(f"‚úì Features categ√≥ricas: {len(categorical_cols)}")
print(f"‚úì FEATURES COM VAZAMENTO REMOVIDAS!")

print(f"\n=== Distribui√ß√£o do Target (Final) ===")
target_dist = y.value_counts()
print(f"Classe 0: {target_dist[0]:,} ({target_dist[0]/len(y)*100:.2f}%)")
print(f"Classe 1: {target_dist[1]:,} ({target_dist[1]/len(y)*100:.2f}%)")

# ===========================================================================
# DIVIS√ÉO TEMPORAL
# ===========================================================================
print(f"\n{'='*70}")
print(f"ETAPA 4: DIVIS√ÉO TEMPORAL (TimeSeriesSplit)")
print(f"{'='*70}")

tscv = TimeSeriesSplit(n_splits=3)
for fold, (train_index, test_index) in enumerate(tscv.split(X)):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    if fold == 2:
        break

print(f"‚úì Train: {len(X_train):,} | Test: {len(X_test):,}")

# ===========================================================================
# TREINAMENTO CATBOOST LEAK-FREE
# ===========================================================================
print(f"\n{'='*70}")
print(f"ETAPA 5: TREINAMENTO CATBOOST (LEAK-FREE)")
print(f"{'='*70}")

# Criar pools
train_pool = Pool(data=X_train, label=y_train, cat_features=categorical_cols if categorical_cols else None)
test_pool = Pool(data=X_test, label=y_test, cat_features=categorical_cols if categorical_cols else None)

print("\nüõ°Ô∏è CONFIGURA√á√ÉO LEAK-FREE:")
print("="*50)
print("‚úÖ SEM user_conversion_rate (calculada com target)")
print("‚úÖ SEM stop_conversion_rate (calculada com target)")  
print("‚úÖ SEM conversion_interaction (baseada em target)")
print("‚úÖ APENAS features hist√≥ricas (expanding windows)")
print("‚úÖ TimeSeriesSplit para valida√ß√£o temporal")
print("="*50)

# Modelo com configura√ß√£o otimizada (mas sem vazamento)
model = CatBoostClassifier(
    iterations=100,           # Mais itera√ß√µes pois agora √© mais dif√≠cil
    learning_rate=0.08,       # LR moderado sem vazamento
    depth=12,                 # Profundidade moderada
    loss_function='Logloss',
    eval_metric='AUC',
    auto_class_weights='Balanced',
    l2_leaf_reg=1.5,         # Mais regulariza√ß√£o
    border_count=128,
    subsample=0.85,
    rsm=0.85,
    random_strength=1.0,
    leaf_estimation_iterations=5,
    min_data_in_leaf=20,
    bootstrap_type='Bernoulli',
    task_type='CPU',
    verbose=50,
    early_stopping_rounds=20,
    random_seed=42,
    thread_count=-1
)

print("\nüöÄ Iniciando treinamento LEAK-FREE...")
model.fit(train_pool, eval_set=test_pool, verbose=True, plot=False)

print(f"\n‚úÖ Treinamento conclu√≠do!")
print(f"‚úì Melhor itera√ß√£o: {model.get_best_iteration()}")
print(f"‚úì Melhor score: {model.get_best_score()['validation']['AUC']:.4f}")

# ===========================================================================
# PREDI√á√ïES E AVALIA√á√ÉO
# ===========================================================================
print(f"\n{'='*70}")
print(f"ETAPA 6: PREDI√á√ïES E AVALIA√á√ÉO (LEAK-FREE)")
print(f"{'='*70}")

y_pred_proba = model.predict_proba(X_test)[:, 1]

# Otimizar threshold
best_threshold = 0.5
best_f1_macro = 0

for threshold in [0.3, 0.4, 0.5, 0.6, 0.7]:
    y_pred_temp = (y_pred_proba >= threshold).astype(int)
    f1_macro = f1_score(y_test, y_pred_temp, average='macro')
    if f1_macro > best_f1_macro:
        best_f1_macro = f1_macro
        best_threshold = threshold

y_pred = (y_pred_proba >= best_threshold).astype(int)

# M√©tricas finais
roc_auc = roc_auc_score(y_test, y_pred_proba)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
f1_macro = f1_score(y_test, y_pred, average='macro')

print(f"\nüìä M√âTRICAS FINAIS (LEAK-FREE):")
print(f"   ROC-AUC:      {roc_auc:.4f} üõ°Ô∏è")
print(f"   Accuracy:     {accuracy:.4f}")
print(f"   Precision:    {precision:.4f}")
print(f"   Recall:       {recall:.4f}")
print(f"   F1-Score:     {f1:.4f}")
print(f"   F1-Macro:     {f1_macro:.4f}")
print(f"   Threshold:    {best_threshold}")

cm = confusion_matrix(y_test, y_pred)
print(f"\nüìä Matriz de Confus√£o:")
print(cm)

# ===========================================================================
# COMPARA√á√ÉO COM VERS√ÉO COM VAZAMENTO
# ===========================================================================
print(f"\n{'='*70}")
print(f"COMPARA√á√ÉO: COM VAZAMENTO vs SEM VAZAMENTO")
print(f"{'='*70}")

print(f"üö® COM VAZAMENTO (V6 anterior):")
print(f"   ROC-AUC: 98.07% (IRREALISTICAMENTE ALTO)")
print(f"   Features: user_conversion_rate, stop_conversion_rate, etc.")
print(f"   Problema: Usava target para criar features!")

print(f"\nüõ°Ô∏è SEM VAZAMENTO (V6 corrigido):")
print(f"   ROC-AUC: {roc_auc:.4f} (REAL√çSTICO)")
print(f"   Features: Apenas hist√≥ricas (expanding windows)")
print(f"   Solu√ß√£o: Simula ambiente de produ√ß√£o real!")

print(f"\nüí° DIFEREN√áA: {98.07 - roc_auc*100:.2f} pontos percentuais")
print(f"   Esta diferen√ßa representa o VAZAMENTO DE DADOS!")

# ===========================================================================
# VISUALIZA√á√ïES LEAK-FREE
# ===========================================================================
print(f"\n{'='*70}")
print(f"ETAPA 7: VISUALIZA√á√ïES (LEAK-FREE)")
print(f"{'='*70}")

# ROC Curve REAL√çSTICA
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
plt.figure(figsize=(10, 8))
plt.plot(fpr, tpr, color='blue', lw=3, label=f'ROC curve LEAK-FREE (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.title('V6 CatBoost LEAK-FREE - ROC Curve\n(SEM Vazamento de Dados - REAL√çSTICA)', 
          fontsize=14, fontweight='bold')
plt.legend(loc="lower right", fontsize=11)
plt.grid(alpha=0.3)
plt.text(0.6, 0.2, f'REAL√çSTICO!\nSem Data Leakage\nAUC = {roc_auc:.4f}', 
         fontsize=12, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue"))
plt.tight_layout()
plt.savefig('../../visualizations/v6/roc_curve_v6_leak_free.png', dpi=300, bbox_inches='tight')
print("‚úì ROC Curve LEAK-FREE salva")
plt.close()

# Feature Importance LEAK-FREE
feature_importance = model.get_feature_importance()
importance_df = pd.DataFrame({
    'feature': X_train.columns,
    'importance': feature_importance
}).sort_values('importance', ascending=False).head(20)

plt.figure(figsize=(12, 8))
plt.barh(range(len(importance_df)), importance_df['importance'], color='blue', alpha=0.7)
plt.yticks(range(len(importance_df)), importance_df['feature'])
plt.xlabel('Importance', fontsize=12)
plt.title('V6 CatBoost LEAK-FREE - Feature Importance\n(SEM Features com Vazamento)', 
          fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.savefig('../../visualizations/v6/feature_importance_v6_leak_free.png', dpi=300, bbox_inches='tight')
print("‚úì Feature Importance LEAK-FREE salva")
plt.close()

# ===========================================================================
# SALVAR MODELO E RELAT√ìRIO
# ===========================================================================
print(f"\n{'='*70}")
print(f"ETAPA 8: SALVANDO MODELO LEAK-FREE")
print(f"{'='*70}")

model.save_model('catboost_model_v6_leak_free.cbm')
print("‚úì Modelo LEAK-FREE salvo")

# Relat√≥rio final
with open('../../reports/v6_catboost_leak_free_report.txt', 'w') as f:
    f.write("="*80 + "\n")
    f.write("MODELO V6 CATBOOST - SEM VAZAMENTO DE DADOS (LEAK-FREE)\n")
    f.write("="*80 + "\n\n")
    
    f.write("CORRE√á√ïES APLICADAS:\n")
    f.write("="*40 + "\n")
    f.write("‚úÖ REMOVIDO: user_conversion_rate (calculada com target)\n")
    f.write("‚úÖ REMOVIDO: stop_conversion_rate (calculada com target)\n")
    f.write("‚úÖ REMOVIDO: conversion_interaction (baseada em target)\n")
    f.write("‚úÖ ADICIONADO: Expanding windows hist√≥ricas\n")
    f.write("‚úÖ ADICIONADO: TimeSeriesSplit para valida√ß√£o temporal\n\n")
    
    f.write("M√âTRICAS FINAIS (LEAK-FREE):\n")
    f.write("="*40 + "\n")
    f.write(f"ROC-AUC:      {roc_auc:.4f} (REAL√çSTICO!)\n")
    f.write(f"Accuracy:     {accuracy:.4f}\n")
    f.write(f"Precision:    {precision:.4f}\n")
    f.write(f"Recall:       {recall:.4f}\n")
    f.write(f"F1-Score:     {f1:.4f}\n")
    f.write(f"F1-Macro:     {f1_macro:.4f}\n")
    f.write(f"Threshold:    {best_threshold}\n\n")
    
    f.write("COMPARA√á√ÉO:\n")
    f.write("="*20 + "\n")
    f.write(f"COM VAZAMENTO:    98.07% AUC (IRREAL)\n")
    f.write(f"SEM VAZAMENTO:    {roc_auc:.4f} AUC (REAL)\n")
    f.write(f"DIFEREN√áA:        {98.07 - roc_auc*100:.2f} pontos (= VAZAMENTO)\n\n")
    
    f.write("TOP FEATURES (SEM VAZAMENTO):\n")
    for idx, row in importance_df.iterrows():
        f.write(f"  {row['feature']}: {row['importance']:.2f}\n")

print("‚úì Relat√≥rio LEAK-FREE salvo")

print(f"\n{'='*80}")
print(f"‚úÖ MODELO V6 CATBOOST LEAK-FREE CONCLU√çDO!")
print(f"{'='*80}")
print(f"\nüõ°Ô∏è RESULTADO REAL√çSTICO (SEM VAZAMENTO):")
print(f"   ROC-AUC: {roc_auc:.4f}")
print(f"   F1-Macro: {f1_macro:.4f}")

print(f"\nüö® VAZAMENTO CORRIGIDO:")
print(f"   Anterior: 98.07% (COM vazamento)")
print(f"   Atual:    {roc_auc:.4f} (SEM vazamento)")
print(f"   Diferen√ßa: {98.07 - roc_auc*100:.2f} pontos = VAZAMENTO!")

print(f"\nüìÅ Arquivos salvos:")
print(f"   - Modelo: catboost_model_v6_leak_free.cbm")
print(f"   - Relat√≥rio: reports/v6_catboost_leak_free_report.txt")
print(f"   - ROC Curve: visualizations/v6/roc_curve_v6_leak_free.png")

print(f"\n‚úÖ AGORA O MODELO √â REAL√çSTICO E PODE SER USADO EM PRODU√á√ÉO!")