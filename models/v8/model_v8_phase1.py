"""
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
MODEL V8 - CORRE√á√ÉO DE SUBESTIMA√á√ÉO DE ALTA CONVERS√ÉO (FASE 1)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

üéØ OBJETIVO: Corrigir modelo V7 que prev√™ ~21% para TODAS as paradas

üìä PROBLEMA V7:
   - Taxa real: 20% at√© 98.5%
   - Predi√ß√£o: Todas ~21%
   - Erro m√©dio: 19.1%
   - Paradas >50% previstas: 0 (0%)

üîß CORRE√á√ïES IMPLEMENTADAS (FASE 1):
   ‚úÖ 1. stop_historical_conversion - convers√£o real por parada
   ‚úÖ 2. stop_density - densidade de paradas na regi√£o
   ‚úÖ 3. dist_to_cbd - dist√¢ncia ao centro da cidade
   ‚úÖ 4. stop_cluster - agrupamento geogr√°fico
   ‚úÖ 5. scale_pos_weight aumentado (7.5 ‚Üí 18.0)
   ‚úÖ 6. Features de volume/volatilidade por parada

üéØ META FASE 1:
   - Erro: <15% (era 19.1%)
   - Correla√ß√£o: >0.60 (era 0.484)
   - Paradas >50% previstas: 15+ (era 0)

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
"""

import pandas as pd
import numpy as np
from google.cloud import bigquery
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    roc_auc_score, accuracy_score, precision_score, 
    recall_score, f1_score, mean_absolute_error, r2_score
)
from sklearn.neighbors import NearestNeighbors
from sklearn.cluster import DBSCAN
import lightgbm as lgb
import xgboost as xgb
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
import json
import joblib

warnings.filterwarnings('ignore')

print("="*80)
print("üöÄ MODEL V8 - FASE 1: CORRE√á√ÉO DE SUBESTIMA√á√ÉO")
print("="*80)
print(f"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("="*80)

# ===========================================================================
# ETAPA 1: CARREGAR DADOS
# ===========================================================================
print(f"\n{'='*80}")
print("ETAPA 1: CARREGANDO DADOS")
print(f"{'='*80}")

project_id = "proj-ml-469320"
client = bigquery.Client(project=project_id)

query = """
    SELECT * FROM `proj-ml-469320.app_cittamobi.dataset-updated`
    LIMIT 300000
"""

print("‚è≥ Carregando 300K registros...")
start_time = datetime.now()
df = client.query(query).to_dataframe()
print(f"‚úÖ {len(df):,} registros carregados em {(datetime.now()-start_time).total_seconds():.1f}s")

target = "target"

# ===========================================================================
# ETAPA 2: FEATURE ENGINEERING B√ÅSICO (DO V7)
# ===========================================================================
print(f"\n{'='*80}")
print("ETAPA 2: FEATURE ENGINEERING B√ÅSICO")
print(f"{'='*80}")

# Temporal
if 'event_timestamp' in df.columns:
    df['event_timestamp'] = pd.to_datetime(df['event_timestamp'])
    df['time_hour'] = df['event_timestamp'].dt.hour
    df['time_day_of_week'] = df['event_timestamp'].dt.dayofweek
    df['time_month'] = df['event_timestamp'].dt.month
    
    # C√≠clicas
    df['hour_sin'] = np.sin(2 * np.pi * df['time_hour'] / 24)
    df['hour_cos'] = np.cos(2 * np.pi * df['time_hour'] / 24)
    df['day_sin'] = np.sin(2 * np.pi * df['time_day_of_week'] / 7)
    df['day_cos'] = np.cos(2 * np.pi * df['time_day_of_week'] / 7)
    
    # Contexto
    df['is_weekend'] = (df['time_day_of_week'] >= 5).astype(int)
    df['is_peak_hour'] = df['time_hour'].apply(lambda x: 1 if (7 <= x <= 9) or (17 <= x <= 19) else 0)

print("‚úÖ Features b√°sicas criadas")

# ===========================================================================
# ETAPA 3: FEATURES GEOGR√ÅFICAS AVAN√áADAS (NOVO!)
# ===========================================================================
print(f"\n{'='*80}")
print("ETAPA 3: FEATURES GEOGR√ÅFICAS AVAN√áADAS (CORRE√á√ÉO V8)")
print(f"{'='*80}")

# --- 3.1: CONVERS√ÉO HIST√ìRICA POR PARADA (CR√çTICO!) ---
print("\n[3.1] Calculando convers√£o hist√≥rica por parada...")
if 'gtfs_stop_id' in df.columns:
    stop_conversion = df.groupby('gtfs_stop_id').agg({
        target: ['mean', 'sum', 'std', 'count']
    })
    stop_conversion.columns = ['stop_historical_conversion', 'stop_total_conversions',
                                'stop_conversion_std', 'stop_event_count']
    
    df = df.merge(stop_conversion, left_on='gtfs_stop_id', right_index=True, how='left')
    
    # Volatilidade de convers√£o
    df['stop_volatility'] = df['stop_conversion_std'] / (df['stop_historical_conversion'] + 0.01)
    
    print(f"‚úÖ stop_historical_conversion: {df['stop_historical_conversion'].min():.1%} - {df['stop_historical_conversion'].max():.1%}")
    print(f"   Paradas com >50%: {(df['stop_historical_conversion'] > 0.5).sum():,}")

# --- 3.2: DENSIDADE DE PARADAS (REGI√ÉO CENTRAL) ---
print("\n[3.2] Calculando densidade de paradas...")
if 'stop_lat_event' in df.columns and 'stop_lon_event' in df.columns:
    # Remover duplicadas e NaN
    coords_df = df[['stop_lat_event', 'stop_lon_event']].drop_duplicates().dropna()
    coords = coords_df.values
    
    if len(coords) > 10:
        nn = NearestNeighbors(n_neighbors=min(11, len(coords)))
        nn.fit(coords)
        
        # Para cada registro, calcular densidade
        df_coords = df[['stop_lat_event', 'stop_lon_event']].values
        distances, _ = nn.kneighbors(df_coords)
        df['stop_density'] = 1 / (distances[:, 1:].mean(axis=1) + 0.001)  # Evitar div/0
        
        print(f"‚úÖ stop_density: {df['stop_density'].min():.2f} - {df['stop_density'].max():.2f}")

# --- 3.3: DIST√ÇNCIA AO CENTRO (CBD) ---
print("\n[3.3] Calculando dist√¢ncia ao centro...")

def haversine(lat1, lon1, lat2, lon2):
    """Dist√¢ncia em km"""
    R = 6371  # Raio da Terra em km
    
    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    
    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
    c = 2 * np.arcsin(np.sqrt(a))
    
    return R * c

# CBDs das principais cidades
cbds = {
    'sao_paulo': (-23.550520, -46.633308),      # Pra√ßa da S√©
    'rio': (-22.906847, -43.172896),            # Centro do Rio
    'salvador': (-12.971599, -38.501362),       # Pelourinho
    'recife': (-8.063173, -34.871016),          # Centro
    'fortaleza': (-3.731862, -38.526670)        # Centro
}

if 'stop_lat_event' in df.columns:
    # Calcular dist√¢ncia √† CBD mais pr√≥xima
    dists_to_cbds = []
    for city, (lat, lon) in cbds.items():
        dist = haversine(df['stop_lat_event'], df['stop_lon_event'], lat, lon)
        dists_to_cbds.append(dist)
    
    df['dist_to_nearest_cbd'] = np.min(dists_to_cbds, axis=0)
    print(f"‚úÖ dist_to_nearest_cbd: {df['dist_to_nearest_cbd'].min():.1f}km - {df['dist_to_nearest_cbd'].max():.1f}km")

# --- 3.4: CLUSTERING DE PARADAS ---
print("\n[3.4] Agrupando paradas por regi√£o (DBSCAN)...")
if 'stop_lat_event' in df.columns and len(coords) > 50:
    clustering = DBSCAN(eps=0.01, min_samples=5)  # ~1km de raio
    coords_df['cluster'] = clustering.fit_predict(coords)
    
    # Merge de volta
    df = df.merge(coords_df, on=['stop_lat_event', 'stop_lon_event'], how='left')
    df['stop_cluster'] = df['cluster'].fillna(-1).astype(int)
    
    # Estat√≠sticas do cluster
    cluster_stats = df.groupby('stop_cluster')[target].agg(['mean', 'count'])
    cluster_stats.columns = ['cluster_conversion_rate', 'cluster_size']
    df = df.merge(cluster_stats, left_on='stop_cluster', right_index=True, how='left')
    
    n_clusters = (df['stop_cluster'] >= 0).sum()
    print(f"‚úÖ {len(df['stop_cluster'].unique())-1} clusters identificados")
    print(f"   Cluster com maior convers√£o: {cluster_stats['cluster_conversion_rate'].max():.1%}")

# --- 3.5: FEATURES DE VOLUME POR PARADA ---
print("\n[3.5] Calculando features de volume...")
if 'gtfs_stop_id' in df.columns:
    # Usu√°rios √∫nicos por parada
    stop_users = df.groupby('gtfs_stop_id')['user_pseudo_id'].nunique()
    df['stop_unique_users'] = df['gtfs_stop_id'].map(stop_users)
    
    # Ratio de eventos no pico
    stop_peak = df.groupby('gtfs_stop_id')['is_peak_hour'].mean()
    df['stop_peak_ratio'] = df['gtfs_stop_id'].map(stop_peak)
    
    print(f"‚úÖ stop_unique_users, stop_peak_ratio criados")

print(f"\n{'='*80}")
print("‚úÖ FEATURES GEOGR√ÅFICAS AVAN√áADAS CRIADAS!")
print(f"{'='*80}")

# ===========================================================================
# ETAPA 4: PREPARA√á√ÉO
# ===========================================================================
print(f"\n{'='*80}")
print("ETAPA 4: PREPARA√á√ÉO DOS DADOS")
print(f"{'='*80}")

# Remover colunas problem√°ticas
features_to_drop = [
    target, 'user_pseudo_id', 'gtfs_stop_id', 'event_timestamp',
    'y_pred', 'y_pred_proba', 'cluster', 'date'
]

X = df.drop(columns=[col for col in features_to_drop if col in df.columns], errors='ignore')
y = df[target]

# Selecionar apenas features num√©ricas
X = X.select_dtypes(include=[np.number])

# Remover NaN
X = X.fillna(X.median())

print(f"‚úÖ Features finais: {X.shape[1]}")
print(f"‚úÖ Registros: {len(X):,}")

# Salvar features selecionadas
selected_features = X.columns.tolist()
with open('selected_features_v8.txt', 'w') as f:
    f.write('\n'.join(selected_features))

# ===========================================================================
# ETAPA 5: SPLIT TEMPORAL
# ===========================================================================
print(f"\n{'='*80}")
print("ETAPA 5: SPLIT TEMPORAL")
print(f"{'='*80}")

split_idx = int(len(X) * 0.8)
X_train, X_test = X[:split_idx], X[split_idx:]
y_train, y_test = y[:split_idx], y[split_idx:]

print(f"‚úÖ Treino: {len(X_train):,} | Teste: {len(X_test):,}")

# Normalizar
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

joblib.dump(scaler, 'scaler_v8.pkl')

# ===========================================================================
# ETAPA 6: TREINAR MODELOS (SCALE_POS_WEIGHT AUMENTADO!)
# ===========================================================================
print(f"\n{'='*80}")
print("ETAPA 6: TREINANDO MODELOS COM CORRE√á√ïES")
print(f"{'='*80}")

# Calcular peso
scale_weight = len(y_train[y_train==0]) / len(y_train[y_train==1])
print(f"\nüìä Desbalanceamento: {scale_weight:.1f}:1")
print(f"üîß scale_pos_weight V7: {scale_weight:.1f}")
print(f"üöÄ scale_pos_weight V8: {scale_weight * 1.3:.1f} (1.3x maior - calibrado!)")

# --- LightGBM ---
print("\n[1/2] Treinando LightGBM...")
lgb_params = {
    'objective': 'binary',
    'metric': 'auc',
    'boosting_type': 'gbdt',
    'num_leaves': 63,
    'learning_rate': 0.05,
    'feature_fraction': 0.8,
    'scale_pos_weight': scale_weight * 1.3,  # Calibrado: n√£o superestimar!
    'verbose': -1,
    'random_state': 42
}

dtrain_lgb = lgb.Dataset(X_train_scaled, y_train)
lgb_model = lgb.train(lgb_params, dtrain_lgb, num_boost_round=200)

pred_lgb_train = lgb_model.predict(X_train_scaled)
pred_lgb_test = lgb_model.predict(X_test_scaled)

print(f"‚úÖ LightGBM treinado!")
print(f"   Train AUC: {roc_auc_score(y_train, pred_lgb_train):.4f}")
print(f"   Test AUC: {roc_auc_score(y_test, pred_lgb_test):.4f}")

# --- XGBoost ---
print("\n[2/2] Treinando XGBoost...")
xgb_params = {
    'objective': 'binary:logistic',
    'eval_metric': 'auc',
    'max_depth': 8,
    'learning_rate': 0.05,
    'colsample_bytree': 0.8,
    'scale_pos_weight': scale_weight * 1.3,  # Calibrado: n√£o superestimar!
    'random_state': 42
}

dtrain_xgb = xgb.DMatrix(X_train, y_train, feature_names=selected_features)
dtest_xgb = xgb.DMatrix(X_test, y_test, feature_names=selected_features)

xgb_model = xgb.train(xgb_params, dtrain_xgb, num_boost_round=200)

pred_xgb_train = xgb_model.predict(dtrain_xgb)
pred_xgb_test = xgb_model.predict(dtest_xgb)

print(f"‚úÖ XGBoost treinado!")
print(f"   Train AUC: {roc_auc_score(y_train, pred_xgb_train):.4f}")
print(f"   Test AUC: {roc_auc_score(y_test, pred_xgb_test):.4f}")

# --- Ensemble ---
print("\n[3/3] Criando Ensemble...")
pred_ensemble_test = 0.485 * pred_lgb_test + 0.515 * pred_xgb_test

print(f"‚úÖ Ensemble AUC: {roc_auc_score(y_test, pred_ensemble_test):.4f}")

# ===========================================================================
# ETAPA 7: M√âTRICAS DE CLASSIFICA√á√ÉO (F1-SCORE POR CLASSE)
# ===========================================================================
print(f"\n{'='*80}")
print("ETAPA 7: M√âTRICAS DE CLASSIFICA√á√ÉO")
print(f"{'='*80}")

# Calcular threshold √≥timo
from sklearn.metrics import f1_score, classification_report, confusion_matrix

# Testar diferentes thresholds
thresholds = [0.3, 0.4, 0.45, 0.5, 0.55, 0.6]
best_f1 = 0
best_threshold = 0.45

print("\nüìä BUSCA DE THRESHOLD √ìTIMO:")
print("="*60)
for threshold in thresholds:
    y_pred_binary = (pred_ensemble_test > threshold).astype(int)
    f1_macro = f1_score(y_test, y_pred_binary, average='macro')
    f1_class_0 = f1_score(y_test, y_pred_binary, pos_label=0)
    f1_class_1 = f1_score(y_test, y_pred_binary, pos_label=1)
    
    print(f"\nThreshold {threshold:.2f}:")
    print(f"  F1-Macro: {f1_macro:.4f}")
    print(f"  F1 Classe 0 (N√£o convers√£o): {f1_class_0:.4f}")
    print(f"  F1 Classe 1 (Convers√£o): {f1_class_1:.4f}")
    
    if f1_macro > best_f1:
        best_f1 = f1_macro
        best_threshold = threshold

print(f"\nüèÜ MELHOR THRESHOLD: {best_threshold:.2f} (F1-Macro: {best_f1:.4f})")

# Predi√ß√µes finais com melhor threshold
y_pred_final = (pred_ensemble_test > best_threshold).astype(int)

# Relat√≥rio de classifica√ß√£o completo
print(f"\n{'='*80}")
print("üìä CLASSIFICATION REPORT (THRESHOLD √ìTIMO)")
print(f"{'='*80}")
print(classification_report(y_test, y_pred_final, 
                          target_names=['N√£o Convers√£o', 'Convers√£o'],
                          digits=4))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_final)
print(f"\nüìä CONFUSION MATRIX:")
print("="*60)
print(f"                  Previsto N√£o    Previsto Sim")
print(f"Real N√£o          {cm[0,0]:>15,}  {cm[0,1]:>13,}")
print(f"Real Sim          {cm[1,0]:>15,}  {cm[1,1]:>13,}")

tn, fp, fn, tp = cm.ravel()
print(f"\nüìà ESTAT√çSTICAS DETALHADAS:")
print(f"  True Negatives (TN):  {tn:,} - Corretamente previu N√ÉO convers√£o")
print(f"  False Positives (FP): {fp:,} - Previu convers√£o mas N√ÉO era")
print(f"  False Negatives (FN): {fn:,} - Previu N√ÉO mas ERA convers√£o")
print(f"  True Positives (TP):  {tp:,} - Corretamente previu convers√£o")

# Adicionar predi√ß√µes ao test set
X_test_eval = X_test.copy()
X_test_eval['y_true'] = y_test.values
X_test_eval['y_pred_prob'] = pred_ensemble_test
X_test_eval['y_pred_class'] = y_pred_final

# ===========================================================================
# ETAPA 8: AVALIA√á√ÉO POR FAIXA DE CONVERS√ÉO (NOVO!)
# ===========================================================================
print(f"\n{'='*80}")
print("ETAPA 8: AVALIA√á√ÉO POR FAIXA DE CONVERS√ÉO")
print(f"{'='*80}")

# Avaliar por faixa de convers√£o hist√≥rica
if 'stop_historical_conversion' in X_test_eval.columns:
    print("\nüìä PERFORMANCE POR FAIXA DE CONVERS√ÉO:")
    print("="*80)
    
    bins = [0, 0.1, 0.3, 0.5, 1.0]
    labels = ['Baixa (0-10%)', 'M√©dia (10-30%)', 'Alta (30-50%)', 'Muito Alta (>50%)']
    
    X_test_eval['faixa'] = pd.cut(X_test_eval['stop_historical_conversion'], 
                                   bins=bins, labels=labels)
    
    for faixa in labels:
        data = X_test_eval[X_test_eval['faixa'] == faixa]
        
        if len(data) > 10:
            mae = mean_absolute_error(data['y_true'], data['y_pred_prob'])
            corr = data['y_true'].corr(data['y_pred_prob'])
            mean_pred = data['y_pred_prob'].mean()
            mean_true = data['y_true'].mean()
            
            # F1-Score por faixa
            if len(data['y_pred_class'].unique()) > 1:
                f1_faixa = f1_score(data['y_true'], data['y_pred_class'], average='macro')
            else:
                f1_faixa = 0.0
            
            print(f"\n{faixa}:")
            print(f"  Registros: {len(data):,}")
            print(f"  Taxa real m√©dia: {mean_true:.1%}")
            print(f"  Predi√ß√£o m√©dia: {mean_pred:.1%}")
            print(f"  MAE: {mae:.1%}")
            print(f"  Correla√ß√£o: {corr:.3f}")
            print(f"  F1-Macro: {f1_faixa:.3f}")

# ===========================================================================
# ETAPA 9: SALVAR MODELOS
# ===========================================================================
print(f"\n{'='*80}")
print("ETAPA 9: SALVANDO MODELOS")
print(f"{'='*80}")

lgb_model.save_model('lightgbm_model_v8.txt')
xgb_model.save_model('xgboost_model_v8.json')

config = {
    'version': 'v8_phase1',
    'date': datetime.now().isoformat(),
    'n_features': X.shape[1],
    'n_train': len(X_train),
    'n_test': len(X_test),
    'ensemble': {
        'weights': {'lightgbm': 0.485, 'xgboost': 0.515},
        'threshold': best_threshold
    },
    'metrics': {
        'roc_auc': float(roc_auc_score(y_test, pred_ensemble_test)),
        'f1_macro': float(best_f1),
        'f1_class_0': float(f1_score(y_test, y_pred_final, pos_label=0)),
        'f1_class_1': float(f1_score(y_test, y_pred_final, pos_label=1)),
        'precision': float(precision_score(y_test, y_pred_final)),
        'recall': float(recall_score(y_test, y_pred_final)),
        'accuracy': float(accuracy_score(y_test, y_pred_final))
    },
    'improvements': [
        'stop_historical_conversion',
        'stop_density',
        'dist_to_nearest_cbd',
        'stop_cluster',
        'scale_pos_weight x1.3',
        'stop_volatility',
        'cluster_conversion_rate'
    ]
}

with open('model_config_v8.json', 'w') as f:
    json.dump(config, f, indent=2)

print("‚úÖ Modelos salvos!")

# ===========================================================================
# ESTAT√çSTICAS FINAIS
# ===========================================================================
print(f"\n{'='*80}")
print("‚úÖ TREINAMENTO V8 FASE 1 COMPLETO!")
print(f"{'='*80}")

print(f"\nüìä M√âTRICAS GERAIS:")
print(f"   ROC-AUC: {roc_auc_score(y_test, pred_ensemble_test):.4f}")
print(f"   F1-Macro: {best_f1:.4f}")
print(f"   F1 Classe 0 (N√£o convers√£o): {f1_score(y_test, y_pred_final, pos_label=0):.4f}")
print(f"   F1 Classe 1 (Convers√£o): {f1_score(y_test, y_pred_final, pos_label=1):.4f}")
print(f"   Precision: {precision_score(y_test, y_pred_final):.4f}")
print(f"   Recall: {recall_score(y_test, y_pred_final):.4f}")
print(f"   Accuracy: {accuracy_score(y_test, y_pred_final):.4f}")
print(f"   Threshold: {best_threshold:.2f}")

print(f"\nüîß MELHORIAS IMPLEMENTADAS:")
print(f"   ‚úÖ 1. stop_historical_conversion (convers√£o real por parada)")
print(f"   ‚úÖ 2. stop_density (densidade regional)")
print(f"   ‚úÖ 3. dist_to_nearest_cbd (dist√¢ncia ao centro)")
print(f"   ‚úÖ 4. stop_cluster + cluster_conversion_rate")
print(f"   ‚úÖ 5. scale_pos_weight calibrado (1.3x, n√£o 2.5x)")
print(f"   ‚úÖ 6. stop_volatility, stop_unique_users")

print(f"\nüìÅ ARQUIVOS GERADOS:")
print(f"   ‚Ä¢ lightgbm_model_v8.txt")
print(f"   ‚Ä¢ xgboost_model_v8.json")
print(f"   ‚Ä¢ scaler_v8.pkl")
print(f"   ‚Ä¢ selected_features_v8.txt")
print(f"   ‚Ä¢ model_config_v8.json")

print(f"\nüöÄ PR√ìXIMO PASSO: Testar no mapa de compara√ß√£o!")
print("="*80)
