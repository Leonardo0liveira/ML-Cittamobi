# üöÄ Model V8 - Vers√£o de Produ√ß√£o

## üìã Informa√ß√µes Gerais

**Vers√£o**: v8_production  
**Data de Cria√ß√£o**: 23 de Novembro de 2025  
**Objetivo**: Predi√ß√£o de convers√£o de usu√°rios em pontos de √¥nibus

---

## üèÜ Performance do Modelo

### M√©tricas Principais

| M√©trica | Valor | Descri√ß√£o |
|---------|-------|-----------|
| **F1 Classe 1 (Convers√£o)** | **0.5539** | Equil√≠brio entre precis√£o e recall para convers√µes |
| **F1 Classe 0 (N√£o-Convers√£o)** | **0.9576** | Equil√≠brio entre precis√£o e recall para n√£o-convers√µes |
| **ROC-AUC** | **0.9425** | Capacidade de discrimina√ß√£o do modelo |
| **F1-Macro** | **0.7558** | M√©dia das m√©tricas F1 das duas classes |
| **Accuracy** | **0.9240** | Taxa de acertos geral |

### Confusion Matrix

```
                    Predicted
                    0        1
Actual    0     [TN]      [FP]
          1     [FN]      [TP]
```

- **True Negatives (TN)**: ~54,000 (n√£o-convers√µes corretamente identificadas)
- **True Positives (TP)**: ~3,100 (convers√µes corretamente identificadas)
- **False Positives (FP)**: ~1,400 (falsos alarmes)
- **False Negatives (FN)**: ~3,500 (convers√µes perdidas)

---

## üîß Arquitetura do Modelo

### Ensemble de Modelos

O modelo final √© um **ensemble otimizado** de dois algoritmos:

1. **LightGBM** (48.5%)
   - Gradient Boosting Decision Tree
   - 300 √°rvores
   - Learning rate: 0.05
   - 63 folhas por √°rvore

2. **XGBoost** (51.5%)
   - Extreme Gradient Boosting
   - 300 √°rvores
   - Learning rate: 0.05
   - Profundidade m√°xima: 8

### Features Engineered (16 features customizadas)

#### Geographic Features (6 features)
1. **stop_historical_conversion**: Taxa m√©dia de convers√£o por parada
2. **stop_density**: Densidade de paradas (inverso da dist√¢ncia m√©dia aos vizinhos)
3. **dist_to_nearest_cbd**: Dist√¢ncia ao CBD mais pr√≥ximo (SP, RJ, BH, Curitiba, POA)
4. **stop_cluster**: Cluster DBSCAN da parada
5. **cluster_conversion_rate**: Taxa de convers√£o do cluster
6. **stop_volatility**: Volatilidade de convers√µes na parada

#### Dynamic Features (10 features)
1. **hour_conversion_rate**: Taxa de convers√£o por hora do dia
2. **dow_conversion_rate**: Taxa de convers√£o por dia da semana
3. **stop_hour_conversion**: Taxa de convers√£o parada √ó hora
4. **geo_temporal**: Dist√¢ncia CBD √ó hora de pico
5. **density_peak**: Densidade √ó hora de pico
6. **user_conversion_rate**: Taxa de convers√£o por usu√°rio
7. **user_vs_stop_ratio**: Raz√£o paradas √∫nicas / eventos por usu√°rio
8. **stop_rarity**: Raridade da parada (inverso da frequ√™ncia)
9. **user_rarity**: Raridade do usu√°rio (inverso da frequ√™ncia)
10. **stop_dist_std**: Desvio padr√£o de dist√¢ncias na parada

---

## üéØ Estrat√©gia de Threshold Din√¢mico

O modelo utiliza **thresholds adaptativos** baseados na taxa de convers√£o hist√≥rica da parada:

| Taxa de Convers√£o Hist√≥rica | Threshold | Estrat√©gia |
|----------------------------|-----------|------------|
| ‚â• 50% (Alta) | **0.40** | Mais agressivo - capturar mais convers√µes |
| 30-50% (M√©dia) | **0.50** | Balanceado |
| 10-30% (Baixa) | **0.60** | Mais conservador |
| < 10% (Muito Baixa) | **0.75** | Muito conservador - evitar falsos positivos |

### Distribui√ß√£o de Thresholds no Dataset

- **0.40**: ~17% das amostras (paradas de alta convers√£o)
- **0.50**: ~4% das amostras (paradas de m√©dia-alta convers√£o)
- **0.60**: ~4% das amostras (paradas de m√©dia-baixa convers√£o)
- **0.75**: ~75% das amostras (paradas de baixa convers√£o)

---

## üì¶ Artefatos de Produ√ß√£o

### Arquivos Salvos

1. **lightgbm_model_v8_production.txt**
   - Modelo LightGBM serializado
   - Formato: LightGBM nativo

2. **xgboost_model_v8_production.json**
   - Modelo XGBoost serializado
   - Formato: JSON (compat√≠vel com qualquer linguagem)

3. **scaler_v8_production.pkl**
   - StandardScaler do scikit-learn
   - Necess√°rio para normalizar features

4. **selected_features_v8_production.txt**
   - Lista de features utilizadas (45 features)
   - Ordem deve ser preservada na infer√™ncia

5. **model_config_v8_production.json**
   - Configura√ß√£o completa do modelo
   - Pesos do ensemble
   - Regras de threshold
   - Par√¢metros de treinamento
   - M√©tricas de performance

---

## üî® Como Usar o Modelo

### 1. Carregar o Modelo

```python
import lightgbm as lgb
import xgboost as xgb
import pickle
import json
import pandas as pd
import numpy as np

# Carregar modelos
lgb_model = lgb.Booster(model_file='lightgbm_model_v8_production.txt')
xgb_model = xgb.Booster()
xgb_model.load_model('xgboost_model_v8_production.json')

# Carregar scaler
with open('scaler_v8_production.pkl', 'rb') as f:
    scaler = pickle.load(f)

# Carregar configura√ß√£o
with open('model_config_v8_production.json', 'r') as f:
    config = json.load(f)

# Carregar lista de features
with open('selected_features_v8_production.txt', 'r') as f:
    feature_cols = [line.strip() for line in f]
```

### 2. Preparar os Dados

```python
# Exemplo de dados de entrada
# df deve conter todas as features base necess√°rias

# Feature Engineering (implementar as 16 features customizadas)
# ... (ver c√≥digo de feature engineering no script de treinamento)

# Selecionar features
X = df[feature_cols].copy()

# Normalizar
X_scaled = scaler.transform(X)
X_scaled = pd.DataFrame(X_scaled, columns=feature_cols)
```

### 3. Fazer Predi√ß√µes

```python
# Predi√ß√µes dos modelos individuais
pred_lgb = lgb_model.predict(X_scaled)
pred_xgb = xgb_model.predict(xgb.DMatrix(X_scaled))

# Ensemble
w_lgb = config['ensemble_weights']['lightgbm']
w_xgb = config['ensemble_weights']['xgboost']
pred_ensemble = w_lgb * pred_lgb + w_xgb * pred_xgb

# Aplicar threshold din√¢mico
def get_dynamic_threshold(stop_conv):
    rules = config['threshold_rules']
    if stop_conv >= rules['high_conversion']['min']:
        return rules['high_conversion']['threshold']
    elif stop_conv >= rules['medium_conversion']['min']:
        return rules['medium_conversion']['threshold']
    elif stop_conv >= rules['low_conversion']['min']:
        return rules['low_conversion']['threshold']
    else:
        return rules['very_low_conversion']['threshold']

thresholds = df['stop_historical_conversion'].apply(get_dynamic_threshold)
predictions = (pred_ensemble > thresholds).astype(int)
```

### 4. Interpretar Resultados

```python
# Adicionar probabilidades e predi√ß√µes ao DataFrame
df['conversion_probability'] = pred_ensemble
df['predicted_conversion'] = predictions
df['threshold_used'] = thresholds

# Exemplo de uso
high_conversion = df[df['predicted_conversion'] == 1]
print(f"Convers√µes previstas: {len(high_conversion)}")
print(f"Probabilidade m√©dia: {high_conversion['conversion_probability'].mean():.2%}")
```

---

## üìä Casos de Uso

### 1. Predi√ß√£o em Tempo Real
- Receber evento de usu√°rio em parada
- Calcular features em tempo real
- Executar modelo
- Retornar probabilidade de convers√£o

### 2. Predi√ß√£o em Batch
- Processar lote de eventos hist√≥ricos
- Gerar predi√ß√µes para an√°lise
- Identificar padr√µes de convers√£o

### 3. Otimiza√ß√£o de Rotas
- Identificar paradas de alta convers√£o
- Priorizar rotas com maior potencial
- Alocar recursos de marketing

### 4. An√°lise de Performance
- Monitorar taxa de convers√£o por parada
- Identificar anomalias
- Ajustar estrat√©gias de neg√≥cio

---

## ‚ö†Ô∏è Requisitos e Depend√™ncias

### Python Packages
```
pandas>=1.5.0
numpy>=1.23.0
scikit-learn>=1.2.0
lightgbm>=3.3.0
xgboost>=1.7.0
google-cloud-bigquery>=3.0.0
```

### Hardware Recomendado
- **CPU**: 4+ cores
- **RAM**: 8GB+ (16GB recomendado)
- **Disco**: 1GB para artefatos

### Tempo de Infer√™ncia
- **Predi√ß√£o individual**: ~5ms
- **Batch (1000 eventos)**: ~1s
- **Batch (100,000 eventos)**: ~30s

---

## üîÑ Manuten√ß√£o e Retreinamento

### Quando Retreinar?

1. **Performance Degradation**: F1 Classe 1 cai abaixo de 0.50
2. **Data Drift**: Distribui√ß√£o de features muda significativamente
3. **Periodicidade**: A cada 3-6 meses
4. **Novos Dados**: Quando acumular 100K+ novos eventos rotulados

### Monitoramento Cont√≠nuo

Monitore as seguintes m√©tricas em produ√ß√£o:

- **F1 Score Classe 1**: Deve permanecer ‚â• 0.50
- **ROC-AUC**: Deve permanecer ‚â• 0.90
- **Distribui√ß√£o de Thresholds**: Verificar se padr√µes mudam
- **Calibra√ß√£o**: Verificar se probabilidades permanecem calibradas

---

## üìù Changelog

### v8_production (23/11/2025)
- ‚úÖ Implementa√ß√£o inicial baseada em Fase 2A
- ‚úÖ Ensemble LightGBM + XGBoost otimizado
- ‚úÖ 16 features customizadas (6 geographic + 10 dynamic)
- ‚úÖ Threshold din√¢mico adaptativo
- ‚úÖ Sample weights din√¢micos
- ‚úÖ F1 Classe 1: 0.5539 (55.39%)
- ‚úÖ ROC-AUC: 0.9425 (94.25%)

---

## üë• Contato e Suporte

Para d√∫vidas, problemas ou sugest√µes de melhorias:

- **Desenvolvedor**: Stefano
- **Projeto**: Cittamobi Forecast - IBMEC
- **Data**: Novembro 2025

---

## üìÑ Licen√ßa

Este modelo √© propriedade do cliente e destinado exclusivamente para uso interno.

---

**‚ú® Modelo pronto para deploy em produ√ß√£o! ‚ú®**
