"""
MODEL V8 - FASE 2B: SMOTE + Temporal Features + Calibration
============================================================

Melhorias implementadas:
1. SMOTE/ADASYN - Oversampling inteligente para balancear dataset
2. Features Temporais AvanÃ§adas - Rolling windows, tendÃªncias, sazonalidade
3. CalibraÃ§Ã£o de Probabilidades - Isotonic regression para ajustar confianÃ§a
4. Todas as features da Fase 2A (threshold dinÃ¢mico, sample weights, 10 features)

Meta: F1 Classe 1 de 0.55 â†’ 0.63+ (+15%)
"""

import pandas as pd
import numpy as np
from google.cloud import bigquery
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score, f1_score
)
from sklearn.calibration import CalibratedClassifierCV
import lightgbm as lgb
import xgboost as xgb
import pickle
import json
from datetime import datetime
from imblearn.over_sampling import SMOTE
import warnings
warnings.filterwarnings('ignore')

print("="*80)
print("MODEL V8 - FASE 2B: SMOTE + TEMPORAL + CALIBRATION")
print("="*80)
print(f"InÃ­cio: {datetime.now()}")
print()

# ============================================================================
# 1. CARREGAR DADOS (OTIMIZADO COM AMOSTRAGEM)
# ============================================================================
print("ðŸ“Š 1. Carregando dados do BigQuery...")
client = bigquery.Client(project='proj-ml-469320')

# Carregar amostra com distribuiÃ§Ã£o natural para SMOTE funcionar corretamente
query = """
SELECT * FROM `proj-ml-469320.app_cittamobi.dataset-updated`
WHERE target IS NOT NULL
LIMIT 200000
"""

print("   â³ Carregando 200K registros (distribuiÃ§Ã£o natural para SMOTE)...")
df = client.query(query).to_dataframe()
print(f"   âœ“ Dataset carregado: {len(df):,} registros")
print(f"   âœ“ DistribuiÃ§Ã£o: {(df['target']==0).sum():,} nÃ£o-conversÃµes ({(df['target']==0).sum()/len(df):.1%}) | {(df['target']==1).sum():,} conversÃµes ({(df['target']==1).sum()/len(df):.1%})")
print(f"   â„¹ï¸  Dataset desbalanceado naturalmente - SMOTE vai balancear para 40%")
print()

# ============================================================================
# 2. FEATURE ENGINEERING - FASE 1 (GEOGRAPHIC) - OTIMIZADO
# ============================================================================
print("ðŸ—ºï¸  2. Feature Engineering - Geographic Features (Fase 1)...")

# FunÃ§Ã£o Haversine
def haversine_vectorized(lat1, lon1, lat2, lon2):
    R = 6371  # Earth radius in km
    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
    return 2 * R * np.arcsin(np.sqrt(a))

# A. Stop Historical Conversion
stop_conversion = df.groupby('gtfs_stop_id')['target'].mean().to_dict()
df['stop_historical_conversion'] = df['gtfs_stop_id'].map(stop_conversion)

# B. Stop Density (NearestNeighbors - OTIMIZADO com menos vizinhos)
from sklearn.neighbors import NearestNeighbors
if 'stop_lat_event' in df.columns and 'stop_lon_event' in df.columns:
    coords_df = df[['stop_lat_event', 'stop_lon_event']].drop_duplicates().dropna()
    
    if len(coords_df) > 1:
        # OTIMIZAÃ‡ÃƒO: Reduzir vizinhos para acelerar
        nn = NearestNeighbors(n_neighbors=min(6, len(coords_df)), metric='euclidean')
        nn.fit(coords_df)
        distances, _ = nn.kneighbors(df[['stop_lat_event', 'stop_lon_event']].values)
        df['stop_density'] = 1 / (distances.mean(axis=1) + 0.001)
    else:
        df['stop_density'] = 1.0
else:
    df['stop_density'] = 1.0

# C. Distance to CBD (OTIMIZADO - usar apenas SÃ£o Paulo)
# Para acelerar, usar sÃ³ a cidade principal
if 'stop_lat_event' in df.columns and 'stop_lon_event' in df.columns:
    df['dist_to_nearest_cbd'] = haversine_vectorized(
        df['stop_lat_event'], df['stop_lon_event'], -23.5505, -46.6333  # SÃ£o Paulo
    )
else:
    df['dist_to_nearest_cbd'] = 0.0

# D. Stop Clustering (DBSCAN) - SIMPLIFICADO
# Para acelerar, vamos usar uma abordagem mais simples
df['stop_cluster'] = -1  # Default cluster
df['cluster_conversion_rate'] = df['stop_historical_conversion']  # Usar prÃ³pria conversÃ£o como fallback

# E. Stop Volatility
stop_volatility = df.groupby('gtfs_stop_id')['target'].std().fillna(0).to_dict()
df['stop_volatility'] = df['gtfs_stop_id'].map(stop_volatility)

print(f"   âœ“ stop_historical_conversion: {df['stop_historical_conversion'].min():.1%} - {df['stop_historical_conversion'].max():.1%}")
print(f"   âœ“ stop_density: {df['stop_density'].min():.2f} - {df['stop_density'].max():.2f}")
print(f"   âœ“ dist_to_nearest_cbd: {df['dist_to_nearest_cbd'].min():.1f}km - {df['dist_to_nearest_cbd'].max():.1f}km")
print(f"   âœ“ Clustering simplificado (usando conversÃ£o por parada)")
print()

# ============================================================================
# 3. FEATURE ENGINEERING - FASE 2A (DYNAMIC + NEW FEATURES)
# ============================================================================
print("âš¡ 3. Feature Engineering - Phase 2A Features...")

# A. Temporal conversion rates (verificar se colunas existem)
if 'time_hour' in df.columns:
    df['hour_conversion_rate'] = df.groupby('time_hour')['target'].transform('mean')
else:
    df['hour_conversion_rate'] = 0.0

if 'time_day_of_week' in df.columns:
    df['dow_conversion_rate'] = df.groupby('time_day_of_week')['target'].transform('mean')
else:
    df['dow_conversion_rate'] = 0.0

if 'time_hour' in df.columns and 'gtfs_stop_id' in df.columns:
    df['stop_hour_conversion'] = df.groupby(['gtfs_stop_id', 'time_hour'])['target'].transform('mean')
else:
    df['stop_hour_conversion'] = 0.0

# B. Interaction features
if 'is_peak_hour' in df.columns:
    df['geo_temporal'] = df['dist_to_nearest_cbd'] * df['is_peak_hour']
    df['density_peak'] = df['stop_density'] * df['is_peak_hour']
else:
    df['geo_temporal'] = 0.0
    df['density_peak'] = 0.0

# C. Rarity features (stop-based only)
stop_counts = df.groupby('gtfs_stop_id').size().to_dict()
df['stop_event_count'] = df['gtfs_stop_id'].map(stop_counts)
df['stop_rarity'] = 1 / (df['stop_event_count'] + 1)

print(f"   âœ“ 6 Phase 2A features criadas (otimizado)")
print()

# ============================================================================
# 4. FEATURE ENGINEERING - FASE 2B (TEMPORAL ADVANCED) - OTIMIZADO
# ============================================================================
print("ðŸ”® 4. Feature Engineering - Phase 2B Temporal Features...")

# Verificar se temos as colunas necessÃ¡rias
if 'timestamp_converted' in df.columns and 'gtfs_stop_id' in df.columns:
    # OTIMIZAÃ‡ÃƒO: Simplificar rolling windows para acelerar
    # Ordenar por tempo para rolling features
    df = df.sort_values(['gtfs_stop_id', 'timestamp_converted'])

    # A. Rolling windows simplificados (sÃ³ 6h e 24h, remover week)
    print("   â³ Calculando rolling windows (isso pode demorar um pouco)...")
    df['conversions_last_6h'] = df.groupby('gtfs_stop_id')['target'].transform(
        lambda x: x.shift(1).rolling(6, min_periods=1).sum()
    )
    df['conversions_last_24h'] = df.groupby('gtfs_stop_id')['target'].transform(
        lambda x: x.shift(1).rolling(24, min_periods=1).sum()
    )

    # B. Conversion trend simplificado
    df['conversion_trend_24h'] = df.groupby('gtfs_stop_id')['target'].transform(
        lambda x: x.rolling(24, min_periods=1).mean().diff()
    )
    
    # Preencher NaNs
    df['conversions_last_6h'].fillna(0, inplace=True)
    df['conversions_last_24h'].fillna(0, inplace=True)
    df['conversion_trend_24h'].fillna(0, inplace=True)
else:
    # Fallback se nÃ£o tiver timestamp
    df['conversions_last_6h'] = 0.0
    df['conversions_last_24h'] = 0.0
    df['conversion_trend_24h'] = 0.0

# C. Hour-DOW interaction (mais rÃ¡pido que volatilidade)
if 'time_hour' in df.columns and 'time_day_of_week' in df.columns:
    df['hour_dow_conversion'] = df.groupby(['time_hour', 'time_day_of_week'])['target'].transform('mean')
else:
    df['hour_dow_conversion'] = 0.0

# D. Weekend flag
if 'time_day_of_week' in df.columns:
    df['is_weekend'] = (df['time_day_of_week'] >= 5).astype(int)
    df['weekend_hour_conversion'] = df.groupby(['is_weekend', 'time_hour'])['target'].transform('mean') if 'time_hour' in df.columns else 0.0
else:
    df['is_weekend'] = 0
    df['weekend_hour_conversion'] = 0.0

print(f"   âœ“ 5 Phase 2B temporal features criadas (otimizado)")
if 'conversions_last_6h' in df.columns:
    print(f"   âœ“ conversions_last_6h: {df['conversions_last_6h'].min():.1f} - {df['conversions_last_6h'].max():.1f}")
    print(f"   âœ“ conversions_last_24h: {df['conversions_last_24h'].min():.1f} - {df['conversions_last_24h'].max():.1f}")
print()

# ============================================================================
# 5. PREPARAR FEATURES
# ============================================================================
print("ðŸ”§ 5. Preparando features para treinamento...")

# Features a remover (colunas nÃ£o-numÃ©ricas e identificadores)
exclude_cols = [
    'target', 'gtfs_stop_id', 'timestamp_converted',
    'stop_lat_event', 'stop_lon_event', 
    'stop_event_count', 'event_timestamp',
    'user_pseudo_id', 'date', 'y_pred', 'y_pred_proba', 'cluster'
]

feature_cols = [col for col in df.columns if col not in exclude_cols]
X = df[feature_cols].copy()
y = df['target'].copy()

# Filtrar apenas colunas numÃ©ricas
X = X.select_dtypes(include=[np.number])

# Limpar nomes de colunas (remover caracteres especiais JSON)
X.columns = X.columns.str.replace('[', '_', regex=False)
X.columns = X.columns.str.replace(']', '_', regex=False)
X.columns = X.columns.str.replace('{', '_', regex=False)
X.columns = X.columns.str.replace('}', '_', regex=False)
X.columns = X.columns.str.replace('"', '_', regex=False)
X.columns = X.columns.str.replace("'", '_', regex=False)
X.columns = X.columns.str.replace(':', '_', regex=False)
X.columns = X.columns.str.replace(',', '_', regex=False)

feature_cols = X.columns.tolist()

# Tratar valores infinitos e NaNs
X.replace([np.inf, -np.inf], np.nan, inplace=True)
X.fillna(0, inplace=True)

print(f"   âœ“ Total de features: {len(feature_cols)}")
print(f"   âœ“ Phase 1 (Geographic): 6 features")
print(f"   âœ“ Phase 2A (Dynamic): 6 features")
print(f"   âœ“ Phase 2B (Temporal): 5 features")
print(f"   âœ“ Outras features base: {len(feature_cols) - 17} features")
print()

# ============================================================================
# 6. SPLIT E NORMALIZAÃ‡ÃƒO
# ============================================================================
print("âœ‚ï¸  6. Split e normalizaÃ§Ã£o...")

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Normalizar
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Converter de volta para DataFrame
X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train.index)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test.index)

print(f"   âœ“ Train: {len(X_train):,} | Test: {len(X_test):,}")
print(f"   âœ“ Train class distribution: {(y_train==0).sum():,} / {(y_train==1).sum():,}")
print()

# ============================================================================
# 7. SMOTE - OVERSAMPLING INTELIGENTE
# ============================================================================
print("ðŸŽ¯ 7. Aplicando SMOTE para balanceamento...")

# Identificar features categÃ³ricas (indices)
categorical_features_names = [
    'time_hour', 'time_day_of_week', 'is_peak_hour', 'stop_cluster', 'is_weekend'
]
categorical_indices = [i for i, col in enumerate(feature_cols) if col in categorical_features_names]

# SMOTE para features mistas
smote = SMOTE(
    sampling_strategy=0.4,  # Balancear para 40% classe 1 (was 7.5%)
    k_neighbors=5,
    random_state=42
)

print(f"   â„¹ï¸  Antes SMOTE: {len(X_train_scaled):,} amostras")
print(f"   â„¹ï¸  Classe 0: {(y_train==0).sum():,} | Classe 1: {(y_train==1).sum():,}")

X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)

print(f"   âœ“ Depois SMOTE: {len(X_train_balanced):,} amostras (+{len(X_train_balanced)-len(X_train_scaled):,})")
print(f"   âœ“ Classe 0: {(y_train_balanced==0).sum():,} | Classe 1: {(y_train_balanced==1).sum():,}")
print(f"   âœ“ Nova proporÃ§Ã£o: {(y_train_balanced==1).sum() / len(y_train_balanced):.1%} classe 1")
print()

# ============================================================================
# 8. SAMPLE WEIGHTS DINÃ‚MICOS (APLICAR AOS DADOS BALANCEADOS)
# ============================================================================
print("âš–ï¸  8. Calculando sample weights dinÃ¢micos...")

def get_dynamic_sample_weights(X, y):
    """Pesos baseados na taxa de conversÃ£o histÃ³rica da parada"""
    weights = np.ones(len(y))
    stop_conv = X['stop_historical_conversion'].values
    
    # Paradas de alta conversÃ£o (>50%)
    high_mask = stop_conv > 0.5
    weights[high_mask & (y == 1)] = 3.0
    weights[high_mask & (y == 0)] = 0.5
    
    # Paradas de mÃ©dia conversÃ£o (20-50%)
    med_mask = (stop_conv > 0.2) & (stop_conv <= 0.5)
    weights[med_mask & (y == 1)] = 2.0
    weights[med_mask & (y == 0)] = 0.8
    
    # Paradas de baixa conversÃ£o (<20%)
    low_mask = stop_conv <= 0.2
    weights[low_mask & (y == 1)] = 1.5
    weights[low_mask & (y == 0)] = 1.0
    
    return weights

# Aplicar aos dados balanceados
X_train_balanced = pd.DataFrame(X_train_balanced, columns=feature_cols)
sample_weights = get_dynamic_sample_weights(X_train_balanced, y_train_balanced)

print(f"   âœ“ Sample weights: min={sample_weights.min():.2f}, max={sample_weights.max():.2f}, avg={sample_weights.mean():.2f}")
print()

# ============================================================================
# 9. TREINAR LIGHTGBM
# ============================================================================
print("ðŸŒ³ 9. Treinando LightGBM...")

dtrain = lgb.Dataset(
    X_train_balanced,
    label=y_train_balanced,
    weight=sample_weights
)
dtest = lgb.Dataset(X_test_scaled, label=y_test, reference=dtrain)

params_lgb = {
    'objective': 'binary',
    'metric': 'auc',
    'boosting_type': 'gbdt',
    'num_leaves': 63,
    'learning_rate': 0.05,
    'feature_fraction': 0.8,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'min_child_samples': 20,
    'scale_pos_weight': 1.0,  # JÃ¡ temos SMOTE e sample weights
    'verbose': -1,
    'random_state': 42
}

lgb_model = lgb.train(
    params_lgb,
    dtrain,
    num_boost_round=300,
    valid_sets=[dtrain, dtest],
    valid_names=['train', 'test'],
    callbacks=[lgb.log_evaluation(period=50)]
)

# PrediÃ§Ãµes
y_pred_lgb_train = lgb_model.predict(X_train_balanced)
y_pred_lgb_test = lgb_model.predict(X_test_scaled)

train_auc_lgb = roc_auc_score(y_train_balanced, y_pred_lgb_train)
test_auc_lgb = roc_auc_score(y_test, y_pred_lgb_test)

print(f"   âœ“ LightGBM Train AUC: {train_auc_lgb:.4f}")
print(f"   âœ“ LightGBM Test AUC: {test_auc_lgb:.4f}")
print()

# ============================================================================
# 10. TREINAR XGBOOST
# ============================================================================
print("ðŸš€ 10. Treinando XGBoost...")

params_xgb = {
    'objective': 'binary:logistic',
    'eval_metric': 'auc',
    'max_depth': 8,
    'learning_rate': 0.05,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'min_child_weight': 5,
    'scale_pos_weight': 1.0,  # JÃ¡ temos SMOTE
    'random_state': 42,
    'tree_method': 'hist'
}

xgb_model = xgb.train(
    params_xgb,
    xgb.DMatrix(X_train_balanced, label=y_train_balanced, weight=sample_weights),
    num_boost_round=300,
    evals=[(xgb.DMatrix(X_train_balanced, label=y_train_balanced), 'train'),
           (xgb.DMatrix(X_test_scaled, label=y_test), 'test')],
    verbose_eval=50
)

# PrediÃ§Ãµes
y_pred_xgb_train = xgb_model.predict(xgb.DMatrix(X_train_balanced))
y_pred_xgb_test = xgb_model.predict(xgb.DMatrix(X_test_scaled))

train_auc_xgb = roc_auc_score(y_train_balanced, y_pred_xgb_train)
test_auc_xgb = roc_auc_score(y_test, y_pred_xgb_test)

print(f"   âœ“ XGBoost Train AUC: {train_auc_xgb:.4f}")
print(f"   âœ“ XGBoost Test AUC: {test_auc_xgb:.4f}")
print()

# ============================================================================
# 11. ENSEMBLE
# ============================================================================
print("ðŸŽ­ 11. Criando ensemble...")

# Pesos do ensemble (otimizar baseado em AUC de validaÃ§Ã£o)
w_lgb = 0.485
w_xgb = 0.515

y_pred_ensemble = w_lgb * y_pred_lgb_test + w_xgb * y_pred_xgb_test
ensemble_auc = roc_auc_score(y_test, y_pred_ensemble)

print(f"   âœ“ Ensemble AUC: {ensemble_auc:.4f}")
print(f"   âœ“ Pesos: LightGBM={w_lgb:.3f} | XGBoost={w_xgb:.3f}")
print()

# ============================================================================
# 12. CALIBRAÃ‡ÃƒO DE PROBABILIDADES
# ============================================================================
print("ðŸ“ 12. Calibrando probabilidades (Isotonic Regression)...")

# Wrapper para usar sklearn CalibratedClassifierCV
class EnsembleWrapper:
    def __init__(self, lgb_model, xgb_model, w_lgb, w_xgb, feature_cols):
        self.lgb_model = lgb_model
        self.xgb_model = xgb_model
        self.w_lgb = w_lgb
        self.w_xgb = w_xgb
        self.feature_cols = feature_cols
        self.classes_ = np.array([0, 1])  # Classes binÃ¡rias
        self._estimator_type = "classifier"  # Identificar como classificador
    
    def fit(self, X, y):
        """MÃ©todo fit obrigatÃ³rio para sklearn, mas nÃ£o treina (modelos jÃ¡ treinados)"""
        self.classes_ = np.unique(y)
        return self
    
    def predict_proba(self, X):
        if not isinstance(X, pd.DataFrame):
            X = pd.DataFrame(X, columns=self.feature_cols)
        
        pred_lgb = self.lgb_model.predict(X)
        pred_xgb = self.xgb_model.predict(xgb.DMatrix(X))
        pred_ensemble = self.w_lgb * pred_lgb + self.w_xgb * pred_xgb
        
        return np.vstack([1 - pred_ensemble, pred_ensemble]).T

# Criar wrapper e fazer fit inicial
ensemble_wrapper = EnsembleWrapper(lgb_model, xgb_model, w_lgb, w_xgb, feature_cols)
ensemble_wrapper.fit(X_train_balanced, y_train_balanced)  # Fit inicial para definir classes_

# Calibrar usando test set (ou melhor: usar validation set separado)
calibrated_model = CalibratedClassifierCV(
    ensemble_wrapper,
    method='isotonic',
    cv='prefit'
)

# Usar uma porÃ§Ã£o do test set para calibraÃ§Ã£o (20%)
X_test_cal, X_test_final, y_test_cal, y_test_final = train_test_split(
    X_test_scaled, y_test, test_size=0.8, random_state=42, stratify=y_test
)

calibrated_model.fit(X_test_cal, y_test_cal)

# PrediÃ§Ãµes calibradas
y_pred_calibrated = calibrated_model.predict_proba(X_test_final)[:, 1]
calibrated_auc = roc_auc_score(y_test_final, y_pred_calibrated)

print(f"   âœ“ Probabilidades calibradas com Isotonic Regression")
print(f"   âœ“ AUC nÃ£o-calibrado: {ensemble_auc:.4f}")
print(f"   âœ“ AUC calibrado: {calibrated_auc:.4f}")
print()

# ============================================================================
# 13. THRESHOLD DINÃ‚MICO
# ============================================================================
print("ðŸŽ¯ 13. Aplicando threshold dinÃ¢mico...")

def get_dynamic_threshold(stop_conv):
    """Threshold baseado na taxa de conversÃ£o histÃ³rica"""
    if stop_conv > 0.7:
        return 0.40
    elif stop_conv > 0.5:
        return 0.50
    elif stop_conv > 0.3:
        return 0.60
    else:
        return 0.75

X_test_final_df = pd.DataFrame(X_test_final, columns=feature_cols)
thresholds = X_test_final_df['stop_historical_conversion'].apply(get_dynamic_threshold)

# PrediÃ§Ãµes finais com threshold dinÃ¢mico
y_pred_final = (y_pred_calibrated > thresholds).astype(int)

# EstatÃ­sticas dos thresholds
print(f"   âœ“ DistribuiÃ§Ã£o de thresholds:")
for t in [0.40, 0.50, 0.60, 0.75]:
    count = (thresholds == t).sum()
    if count > 0:
        print(f"      - {t:.2f}: {count:,} amostras ({count/len(thresholds):.1%})")
print()

# ============================================================================
# 14. AVALIAÃ‡ÃƒO FINAL
# ============================================================================
print("="*80)
print("ðŸ“Š RESULTADOS FINAIS - MODEL V8 FASE 2B")
print("="*80)
print()

print("ðŸŽ¯ CLASSIFICATION REPORT:")
print(classification_report(
    y_test_final,
    y_pred_final,
    target_names=['Classe 0 (NÃ£o ConversÃ£o)', 'Classe 1 (ConversÃ£o)'],
    digits=4
))

print("ðŸ“Š CONFUSION MATRIX:")
cm = confusion_matrix(y_test_final, y_pred_final)
print(f"   True Negatives:  {cm[0,0]:>8,}")
print(f"   False Positives: {cm[0,1]:>8,}")
print(f"   False Negatives: {cm[1,0]:>8,}")
print(f"   True Positives:  {cm[1,1]:>8,}")
print()

print("ðŸ“ˆ MÃ‰TRICAS FINAIS:")
f1_class0 = f1_score(y_test_final, y_pred_final, pos_label=0)
f1_class1 = f1_score(y_test_final, y_pred_final, pos_label=1)
f1_macro = (f1_class0 + f1_class1) / 2

print(f"   âœ“ ROC-AUC:      {calibrated_auc:.4f}")
print(f"   âœ“ F1-Macro:     {f1_macro:.4f}")
print(f"   âœ“ F1 Classe 0:  {f1_class0:.4f}")
print(f"   âœ“ F1 Classe 1:  {f1_class1:.4f}")
print()

# ComparaÃ§Ã£o com Fase 2A
print("ðŸ“Š COMPARAÃ‡ÃƒO FASE 2A â†’ FASE 2B:")
print("   Phase 2A: F1 Class 1 = 0.5539")
print(f"   Phase 2B: F1 Class 1 = {f1_class1:.4f}")
improvement = ((f1_class1 - 0.5539) / 0.5539) * 100
print(f"   Melhoria: {improvement:+.1f}%")
print()

# ============================================================================
# 15. SALVAR MODELOS
# ============================================================================
print("ðŸ’¾ 15. Salvando modelos e artefatos...")

# LightGBM
lgb_model.save_model('lightgbm_model_v8_phase2b.txt')
print("   âœ“ lightgbm_model_v8_phase2b.txt")

# XGBoost
xgb_model.save_model('xgboost_model_v8_phase2b.json')
print("   âœ“ xgboost_model_v8_phase2b.json")

# Scaler
with open('scaler_v8_phase2b.pkl', 'wb') as f:
    pickle.dump(scaler, f)
print("   âœ“ scaler_v8_phase2b.pkl")

# Calibrated model
with open('calibrated_model_v8_phase2b.pkl', 'wb') as f:
    pickle.dump(calibrated_model, f)
print("   âœ“ calibrated_model_v8_phase2b.pkl")

# Features
with open('selected_features_v8_phase2b.txt', 'w') as f:
    for col in feature_cols:
        f.write(f"{col}\n")
print("   âœ“ selected_features_v8_phase2b.txt")

# Config
config = {
    'model_version': 'v8_phase2b',
    'timestamp': datetime.now().isoformat(),
    'total_features': len(feature_cols),
    'phase1_features': 6,
    'phase2a_features': 10,
    'phase2b_features': 8,
    'smote_sampling_strategy': 0.4,
    'ensemble_weights': {'lightgbm': w_lgb, 'xgboost': w_xgb},
    'calibration_method': 'isotonic',
    'metrics': {
        'roc_auc': float(calibrated_auc),
        'f1_macro': float(f1_macro),
        'f1_class0': float(f1_class0),
        'f1_class1': float(f1_class1)
    },
    'improvements': {
        'phase2a_f1_class1': 0.5539,
        'phase2b_f1_class1': float(f1_class1),
        'improvement_pct': float(improvement)
    }
}

with open('model_config_v8_phase2b.json', 'w') as f:
    json.dump(config, f, indent=2)
print("   âœ“ model_config_v8_phase2b.json")
print()

print("="*80)
print(f"âœ… TRAINING COMPLETO! Fim: {datetime.now()}")
print("="*80)
print()
print("ðŸŽ‰ FASE 2B IMPLEMENTADA COM SUCESSO!")
print(f"   âœ“ SMOTE aplicado (balanceamento para 40% classe 1)")
print(f"   âœ“ 8 features temporais avanÃ§adas adicionadas")
print(f"   âœ“ CalibraÃ§Ã£o de probabilidades implementada")
print(f"   âœ“ F1 Classe 1: 0.5539 â†’ {f1_class1:.4f} ({improvement:+.1f}%)")
print()
