# ğŸ“š DOCUMENTAÃ‡ÃƒO COMPLETA - MODEL V8 PRODUCTION

## ğŸ“‹ Ãndice
1. [VisÃ£o Geral](#visÃ£o-geral)
2. [Arquitetura do Modelo](#arquitetura-do-modelo)
3. [Fluxo de ExecuÃ§Ã£o](#fluxo-de-execuÃ§Ã£o)
4. [Feature Engineering Detalhado](#feature-engineering-detalhado)
5. [ValidaÃ§Ã£o Cruzada Temporal](#validaÃ§Ã£o-cruzada-temporal)
6. [Ensemble e Threshold DinÃ¢mico](#ensemble-e-threshold-dinÃ¢mico)
7. [MÃ©tricas e AvaliaÃ§Ã£o](#mÃ©tricas-e-avaliaÃ§Ã£o)
8. [Artefatos Gerados](#artefatos-gerados)
9. [Como Usar em ProduÃ§Ã£o](#como-usar-em-produÃ§Ã£o)

---

## ğŸ¯ VisÃ£o Geral

### Objetivo
Prever se um usuÃ¡rio do app Cittamobi vai **converter** (comprar uma passagem) apÃ³s visualizar informaÃ§Ãµes de uma parada de Ã´nibus.

### Performance do Modelo
```
âœ“ F1 Classe 1 (ConversÃ£o): ~55.39%
âœ“ F1 Classe 0 (NÃ£o-ConversÃ£o): ~95.76%
âœ“ ROC-AUC: ~94.25%
âœ“ F1-Macro: ~75.58%
âœ“ Accuracy: ~92.40%
```

### Diferencial Principal
- **ValidaÃ§Ã£o Cruzada Temporal (TimeSeriesSplit)** com 5 folds
- **Split sequencial** que respeita a ordem temporal dos dados
- **Ensemble otimizado** de LightGBM + XGBoost
- **Threshold dinÃ¢mico** baseado em histÃ³rico de conversÃ£o

---

## ğŸ—ï¸ Arquitetura do Modelo

### Componentes Principais

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DADOS BRUTOS                             â”‚
â”‚              (BigQuery ou CSV Local)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FEATURE ENGINEERING                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Geographic  â”‚  â”‚   Dynamic    â”‚  â”‚     Base     â”‚     â”‚
â”‚  â”‚  (6 features)â”‚  â”‚ (10 features)â”‚  â”‚  (restantes) â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            TIME SERIES SPLIT (5 FOLDS)                      â”‚
â”‚  Fold 1 â†’ Fold 2 â†’ Fold 3 â†’ Fold 4 â†’ Fold 5                â”‚
â”‚  (MantÃ©m ordem temporal dos dados)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 NORMALIZAÃ‡ÃƒO                                â”‚
â”‚            (StandardScaler por fold)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SAMPLE WEIGHTS DINÃ‚MICOS                       â”‚
â”‚  Alto conv. â†’ Classe 1: 3.0x | Classe 0: 0.5x              â”‚
â”‚  MÃ©dio conv. â†’ Classe 1: 2.0x | Classe 0: 0.8x             â”‚
â”‚  Baixo conv. â†’ Classe 1: 1.5x | Classe 0: 1.0x             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                TREINAMENTO ENSEMBLE                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  LightGBM    â”‚              â”‚   XGBoost    â”‚            â”‚
â”‚  â”‚  (48.5%)     â”‚              â”‚   (51.5%)    â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            THRESHOLD DINÃ‚MICO                               â”‚
â”‚  Alta conv. (â‰¥50%): threshold = 0.40                        â”‚
â”‚  MÃ©dia conv. (â‰¥30%): threshold = 0.50                       â”‚
â”‚  Baixa conv. (â‰¥10%): threshold = 0.60                       â”‚
â”‚  Muito baixa (<10%): threshold = 0.75                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PREDIÃ‡ÃƒO FINAL                            â”‚
â”‚              (0 = NÃ£o ConversÃ£o / 1 = ConversÃ£o)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Fluxo de ExecuÃ§Ã£o

### Etapa 1: Carregamento de Dados

```python
# OpÃ§Ã£o 1: CSV Local (para testes)
USE_CSV = True
df = pd.read_csv('dataset-updated.csv')

# OpÃ§Ã£o 2: BigQuery (para produÃ§Ã£o)
USE_CSV = False
client = bigquery.Client(project='proj-ml-469320')
df = client.query(query).to_dataframe()
```

**O que acontece:**
- Carrega os dados do Cittamobi
- Filtra apenas registros com `target` vÃ¡lido (0 ou 1)
- Remove NaNs na coluna target
- Mostra estatÃ­sticas bÃ¡sicas (total de registros, taxa de conversÃ£o)

**VariÃ¡veis importantes:**
- `target`: 1 = conversÃ£o, 0 = nÃ£o conversÃ£o
- `gtfs_stop_id`: ID Ãºnico da parada
- `device_id`: ID Ãºnico do usuÃ¡rio
- `stop_lat_event`, `stop_lon_event`: Coordenadas da parada
- `time_hour`, `time_day_of_week`: InformaÃ§Ãµes temporais

---

### Etapa 2: Feature Engineering - FASE 1 (Geographic)

#### 2A. Stop Historical Conversion
```python
stop_conversion = df.groupby('gtfs_stop_id')['target'].mean().to_dict()
df['stop_historical_conversion'] = df['gtfs_stop_id'].map(stop_conversion)
```
**O que faz:** Calcula a taxa mÃ©dia de conversÃ£o de cada parada.
**Exemplo:** Se a parada "ABC123" teve 100 eventos e 30 conversÃµes, essa feature serÃ¡ 0.30 (30%).

#### 2B. Stop Density
```python
nn = NearestNeighbors(n_neighbors=11, metric='euclidean')
nn.fit(coords_df)
distances, _ = nn.kneighbors(...)
df['stop_density'] = 1 / (distances.mean(axis=1) + 0.001)
```
**O que faz:** Mede quÃ£o "densa" Ã© a Ã¡rea ao redor da parada (quantas outras paradas existem prÃ³ximas).
**InterpretaÃ§Ã£o:** 
- Valor ALTO = muitas paradas prÃ³ximas (centro urbano)
- Valor BAIXO = paradas isoladas (periferia)

#### 2C. Distance to Nearest CBD
```python
cbd_coords = [
    (-23.5505, -46.6333),  # SÃ£o Paulo
    (-22.9068, -43.1729),  # Rio de Janeiro
    ...
]
df['dist_to_nearest_cbd'] = haversine_vectorized(...)
```
**O que faz:** Calcula a distÃ¢ncia (em km) da parada atÃ© o centro comercial mais prÃ³ximo.
**HipÃ³tese:** Paradas mais prÃ³ximas de centros comerciais tÃªm maior conversÃ£o.

#### 2D. Stop Clustering (DBSCAN)
```python
clustering = DBSCAN(eps=0.01, min_samples=5)
cluster_labels = clustering.fit_predict(coords_for_clustering)
```
**O que faz:** Agrupa paradas geograficamente prÃ³ximas em clusters.
**Resultado:**
- `stop_cluster`: ID do cluster da parada (-1 = outlier)
- `cluster_conversion_rate`: Taxa de conversÃ£o mÃ©dia do cluster

#### 2E. Stop Volatility
```python
stop_volatility = df.groupby('gtfs_stop_id')['target'].std().to_dict()
```
**O que faz:** Mede a variabilidade das conversÃµes de uma parada.
**InterpretaÃ§Ã£o:**
- Volatilidade ALTA = conversÃ£o imprevisÃ­vel
- Volatilidade BAIXA = comportamento consistente

---

### Etapa 3: Feature Engineering - FASE 2A (Dynamic + Interactions)

#### 3A. Temporal Conversion Rates
```python
df['hour_conversion_rate'] = df.groupby('time_hour')['target'].transform('mean')
df['dow_conversion_rate'] = df.groupby('time_day_of_week')['target'].transform('mean')
df['stop_hour_conversion'] = df.groupby(['gtfs_stop_id', 'time_hour'])['target'].transform('mean')
```
**O que faz:** 
- `hour_conversion_rate`: Taxa de conversÃ£o por hora do dia (0-23)
- `dow_conversion_rate`: Taxa de conversÃ£o por dia da semana (0-6)
- `stop_hour_conversion`: Taxa especÃ­fica da parada naquele horÃ¡rio

**Exemplo:**
- Parada "ABC123" Ã s 8h da manhÃ£ pode ter taxa de 40%
- A mesma parada Ã s 23h pode ter taxa de 5%

#### 3B. Geo-Temporal Interactions
```python
df['geo_temporal'] = df['dist_to_nearest_cbd'] * df['is_peak_hour']
df['density_peak'] = df['stop_density'] * df['is_peak_hour']
```
**O que faz:** Combina features geogrÃ¡ficas com temporais.
**HipÃ³tese:** O impacto da localizaÃ§Ã£o muda durante horÃ¡rios de pico.

#### 3C. User Features
```python
df['user_conversion_rate'] = df['device_id'].map(user_conversion)
df['user_vs_stop_ratio'] = df['device_id'].map(user_stop_ratio)
```
**O que faz:**
- `user_conversion_rate`: HistÃ³rico de conversÃ£o do usuÃ¡rio
- `user_vs_stop_ratio`: Diversidade de paradas visitadas pelo usuÃ¡rio

**InterpretaÃ§Ã£o:**
- UsuÃ¡rio que sempre converte â†’ valor alto
- UsuÃ¡rio que visita muitas paradas diferentes â†’ explorador

#### 3D. Rarity Features
```python
df['stop_rarity'] = 1 / (df['stop_event_count'] + 1)
df['user_rarity'] = 1 / (df['user_frequency'] + 1)
```
**O que faz:** Mede quÃ£o "rara" Ã© a parada/usuÃ¡rio.
**InterpretaÃ§Ã£o:**
- Parada rara = poucas visualizaÃ§Ãµes
- UsuÃ¡rio raro = pouco uso do app

#### 3E. Distance Deviation
```python
df['stop_dist_std'] = ...  # Desvio padrÃ£o das coordenadas da parada
```
**O que faz:** Mede a variaÃ§Ã£o nas coordenadas reportadas para a mesma parada.
**InterpretaÃ§Ã£o:** Paradas com alta variaÃ§Ã£o podem ter GPS impreciso.

---

### Etapa 4: PreparaÃ§Ã£o de Features

```python
# Remover colunas que nÃ£o devem ser features
exclude_cols = ['target', 'gtfs_stop_id', 'timestamp_converted', 'device_id', ...]

# Selecionar apenas numÃ©ricas
X = X.select_dtypes(include=[np.number])

# Limpar caracteres especiais dos nomes
X.columns = X.columns.str.replace('[', '_', regex=False)

# Tratar infinitos e NaNs
X.replace([np.inf, -np.inf], np.nan, inplace=True)
X.fillna(0, inplace=True)
```

**Por que isso Ã© importante:**
- Modelos ML sÃ³ aceitam valores numÃ©ricos
- Caracteres especiais em nomes de colunas quebram LightGBM/XGBoost
- Infinitos e NaNs causam erros no treinamento

---

### Etapa 5: Time Series Split

```python
tscv = TimeSeriesSplit(n_splits=5)

for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):
    # Treina e valida em cada fold
```

**Como funciona:**
```
Dados: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

Fold 1:
  Train: [1, 2]
  Val:   [3, 4]

Fold 2:
  Train: [1, 2, 3, 4]
  Val:   [5, 6]

Fold 3:
  Train: [1, 2, 3, 4, 5, 6]
  Val:   [7, 8]

Fold 4:
  Train: [1, 2, 3, 4, 5, 6, 7, 8]
  Val:   [9, 10]

Fold 5:
  Train: [1, 2, 3, 4, 5, 6, 7, 8, 9]
  Val:   [10]
```

**Por que usar TimeSeriesSplit:**
- Respeita a ordem temporal dos dados
- Evita "vazamento de informaÃ§Ã£o do futuro"
- Simula cenÃ¡rio real de prediÃ§Ã£o (treinar no passado, prever no futuro)

---

### Etapa 6: ValidaÃ§Ã£o Cruzada

Para cada fold:

#### 6.1. NormalizaÃ§Ã£o
```python
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_fold)
X_val_scaled = scaler.transform(X_val_fold)
```
**O que faz:** Padroniza features para mÃ©dia=0 e desvio=1.
**Por que:** Melhora convergÃªncia e performance dos modelos.

#### 6.2. Sample Weights DinÃ¢micos
```python
def get_dynamic_sample_weights(X, y):
    weights = np.ones(len(y))
    stop_conv = X['stop_historical_conversion'].values
    
    # Paradas com alta conversÃ£o
    high_mask = stop_conv > 0.5
    weights[high_mask & (y == 1)] = 3.0  # ConversÃµes valem 3x
    weights[high_mask & (y == 0)] = 0.5  # NÃ£o-conversÃµes valem 0.5x
    
    # Paradas com mÃ©dia conversÃ£o
    med_mask = (stop_conv > 0.2) & (stop_conv <= 0.5)
    weights[med_mask & (y == 1)] = 2.0
    weights[med_mask & (y == 0)] = 0.8
    
    # Paradas com baixa conversÃ£o
    low_mask = stop_conv <= 0.2
    weights[low_mask & (y == 1)] = 1.5
    weights[low_mask & (y == 0)] = 1.0
    
    return weights
```

**Por que isso Ã© crucial:**
- DÃ¡ mais "peso" para conversÃµes em paradas com alto potencial
- Reduz peso de falsos positivos em paradas ruins
- Melhora F1 da classe minoritÃ¡ria (conversÃµes)

#### 6.3. Scale Pos Weight
```python
scale_weight = len(y[y==0]) / len(y[y==1])
# Se 90% sÃ£o nÃ£o-conversÃµes e 10% conversÃµes: scale_weight = 9.0
```
**O que faz:** Compensa o desbalanceamento de classes.
**AplicaÃ§Ã£o:** Multiplicado por 1.3 para dar ainda mais Ãªnfase Ã  classe minoritÃ¡ria.

#### 6.4. Treinamento LightGBM
```python
params_lgb = {
    'objective': 'binary',
    'metric': 'auc',
    'boosting_type': 'gbdt',
    'num_leaves': 63,
    'learning_rate': 0.05,
    'feature_fraction': 0.8,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'min_child_samples': 20,
    'scale_pos_weight': scale_weight * 1.3,
}
```

**ParÃ¢metros principais:**
- `num_leaves: 63`: Complexidade da Ã¡rvore (mais folhas = mais complexo)
- `learning_rate: 0.05`: Taxa de aprendizado (menor = mais conservador)
- `feature_fraction: 0.8`: Usa 80% das features em cada Ã¡rvore (evita overfitting)
- `bagging_fraction: 0.8`: Usa 80% dos dados em cada iteraÃ§Ã£o
- `min_child_samples: 20`: MÃ­nimo de amostras por folha (evita overfitting)

#### 6.5. Treinamento XGBoost
```python
params_xgb = {
    'objective': 'binary:logistic',
    'eval_metric': 'auc',
    'max_depth': 8,
    'learning_rate': 0.05,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'min_child_weight': 5,
    'scale_pos_weight': scale_weight * 1.3,
    'tree_method': 'hist',
}
```

**DiferenÃ§as do LightGBM:**
- `max_depth` ao invÃ©s de `num_leaves`: Controla profundidade mÃ¡xima
- `tree_method: 'hist'`: Algoritmo mais rÃ¡pido para grandes datasets

#### 6.6. Ensemble
```python
w_lgb = 0.485
w_xgb = 0.515
pred_ensemble = w_lgb * pred_lgb + w_xgb * pred_xgb
```
**Pesos otimizados empiricamente:** XGBoost um pouco mais forte que LightGBM.

#### 6.7. Threshold DinÃ¢mico
```python
def get_dynamic_threshold(stop_conv):
    if stop_conv >= 0.5:
        return 0.40  # Paradas excelentes: threshold baixo
    elif stop_conv >= 0.3:
        return 0.50  # Paradas boas: threshold mÃ©dio
    elif stop_conv >= 0.1:
        return 0.60  # Paradas regulares: threshold alto
    else:
        return 0.75  # Paradas ruins: threshold muito alto
```

**LÃ³gica:**
- Paradas boas: Ser "mais otimista" â†’ aceitar probabilidades mais baixas como conversÃ£o
- Paradas ruins: Ser "mais conservador" â†’ exigir probabilidades muito altas

---

### Etapa 7-10: Treinamento do Modelo Final

ApÃ³s a validaÃ§Ã£o cruzada, treina o modelo final com 80% dos dados:

```python
split_idx = int(len(X) * 0.8)
X_train_final = X.iloc[:split_idx]  # 80% primeiros (mais antigos)
X_test_final = X.iloc[split_idx:]   # 20% Ãºltimos (mais recentes)
```

**Por que 80/20 sequencial:**
- Simula produÃ§Ã£o: treinar no passado, testar no futuro
- Evita data leakage

---

### Etapa 11-12: AvaliaÃ§Ã£o Final

#### MÃ©tricas Principais

**1. ROC-AUC (~94.25%)**
- Mede capacidade geral de separar classes
- Valor excelente (prÃ³ximo de 1.0)

**2. F1-Score Classe 1 (~55.39%)**
- EquilÃ­brio entre Precision e Recall para CONVERSÃ•ES
- Mais importante para o negÃ³cio (encontrar quem vai comprar)

**3. F1-Score Classe 0 (~95.76%)**
- EquilÃ­brio entre Precision e Recall para NÃƒO-CONVERSÃ•ES
- Muito alto (fÃ¡cil prever quem NÃƒO vai comprar)

**4. F1-Macro (~75.58%)**
- MÃ©dia das duas classes
- Mostra equilÃ­brio geral do modelo

**Confusion Matrix:**
```
                 Predito: 0    Predito: 1
Real: 0 (nÃ£o)    TN            FP
Real: 1 (sim)    FN            TP
```
- **TN (True Negative):** Acertou que nÃ£o iria converter
- **FP (False Positive):** Errou, disse que ia converter mas nÃ£o converteu
- **FN (False Negative):** Errou, disse que nÃ£o ia converter mas converteu
- **TP (True Positive):** Acertou que iria converter

---

## ğŸ’¾ Artefatos Gerados

### 1. `lightgbm_model_v8_production.txt`
**Formato:** Texto proprietÃ¡rio do LightGBM
**ConteÃºdo:** Estrutura completa das Ã¡rvores do LightGBM
**Uso:** Carregar com `lgb.Booster(model_file='...')`

### 2. `xgboost_model_v8_production.json`
**Formato:** JSON
**ConteÃºdo:** Estrutura completa das Ã¡rvores do XGBoost
**Uso:** Carregar com `xgb.Booster(); booster.load_model('...')`

### 3. `scaler_v8_production.pkl`
**Formato:** Pickle (Python)
**ConteÃºdo:** Objeto StandardScaler treinado
**Uso:** Normalizar novos dados antes de prever
```python
with open('scaler_v8_production.pkl', 'rb') as f:
    scaler = pickle.load(f)
X_new_scaled = scaler.transform(X_new)
```

### 4. `selected_features_v8_production.txt`
**Formato:** Texto (uma feature por linha)
**ConteÃºdo:** Lista ordenada das features usadas
**Uso:** Garantir que novos dados tenham as mesmas features na mesma ordem

### 5. `model_config_v8_production.json`
**Formato:** JSON
**ConteÃºdo:**
```json
{
    "model_version": "v8_production_timeseries_cv",
    "creation_date": "2025-11-23 14:30:00",
    "n_features": 42,
    "ensemble_weights": {
        "lightgbm": 0.485,
        "xgboost": 0.515
    },
    "cross_validation": {
        "method": "TimeSeriesSplit",
        "n_splits": 5,
        "fold_results": [...],
        "cv_metrics_mean": {
            "auc_ensemble": 0.9425,
            "f1_macro": 0.7558,
            ...
        }
    },
    "final_test_metrics": {...},
    "threshold_strategy": "dynamic",
    "threshold_rules": {...},
    "training_params": {...}
}
```

---

## ğŸš€ Como Usar em ProduÃ§Ã£o

### Script de InferÃªncia BÃ¡sico

```python
import pandas as pd
import numpy as np
import lightgbm as lgb
import xgboost as xgb
import pickle
import json

# 1. CARREGAR ARTEFATOS
print("Carregando modelos...")
lgb_model = lgb.Booster(model_file='lightgbm_model_v8_production.txt')
xgb_model = xgb.Booster()
xgb_model.load_model('xgboost_model_v8_production.json')

with open('scaler_v8_production.pkl', 'rb') as f:
    scaler = pickle.load(f)

with open('selected_features_v8_production.txt', 'r') as f:
    feature_cols = [line.strip() for line in f]

with open('model_config_v8_production.json', 'r') as f:
    config = json.load(f)

w_lgb = config['ensemble_weights']['lightgbm']
w_xgb = config['ensemble_weights']['xgboost']

# 2. CARREGAR NOVOS DADOS
print("Carregando novos dados...")
df_new = pd.read_csv('novos_eventos.csv')

# 3. FEATURE ENGINEERING
print("Criando features...")
# [Repetir TODAS as etapas de feature engineering do treinamento]
# - stop_historical_conversion
# - stop_density
# - dist_to_nearest_cbd
# - ... todas as 16 features criadas

# 4. PREPARAR FEATURES
X_new = df_new[feature_cols].copy()
X_new = X_new.select_dtypes(include=[np.number])
X_new.replace([np.inf, -np.inf], np.nan, inplace=True)
X_new.fillna(0, inplace=True)

# 5. NORMALIZAR
X_new_scaled = scaler.transform(X_new)
X_new_scaled = pd.DataFrame(X_new_scaled, columns=feature_cols)

# 6. PREDIÃ‡ÃƒO
print("Fazendo prediÃ§Ãµes...")
pred_lgb = lgb_model.predict(X_new_scaled)
pred_xgb = xgb_model.predict(xgb.DMatrix(X_new_scaled))
pred_ensemble = w_lgb * pred_lgb + w_xgb * pred_xgb

# 7. THRESHOLD DINÃ‚MICO
def get_dynamic_threshold(stop_conv):
    if stop_conv >= 0.5:
        return 0.40
    elif stop_conv >= 0.3:
        return 0.50
    elif stop_conv >= 0.1:
        return 0.60
    else:
        return 0.75

thresholds = X_new_scaled['stop_historical_conversion'].apply(get_dynamic_threshold)
predictions = (pred_ensemble > thresholds.values).astype(int)

# 8. RESULTADOS
df_new['probabilidade_conversao'] = pred_ensemble
df_new['predicao_conversao'] = predictions
df_new['threshold_usado'] = thresholds

print("\nğŸ“Š ESTATÃSTICAS:")
print(f"Total de eventos: {len(df_new):,}")
print(f"ConversÃµes previstas: {predictions.sum():,} ({predictions.sum()/len(df_new):.2%})")
print(f"Probabilidade mÃ©dia: {pred_ensemble.mean():.4f}")

# 9. SALVAR
df_new[['gtfs_stop_id', 'device_id', 'probabilidade_conversao', 
        'predicao_conversao', 'threshold_usado']].to_csv('predicoes.csv', index=False)
print("âœ“ PrediÃ§Ãµes salvas em predicoes.csv")
```

---

## âš ï¸ ConsideraÃ§Ãµes Importantes

### 1. Ordem Temporal
**CRÃTICO:** Sempre manter ordem temporal dos dados.
- âŒ NÃƒO usar `train_test_split` com `shuffle=True`
- âœ… SIM usar split sequencial ou `TimeSeriesSplit`

### 2. Feature Engineering Consistente
**OBRIGATÃ“RIO:** Em produÃ§Ã£o, criar exatamente as mesmas features do treinamento.
- Usar os mesmos dicionÃ¡rios de conversÃ£o
- Aplicar as mesmas transformaÃ§Ãµes
- Manter a mesma ordem das features

### 3. NormalizaÃ§Ã£o
**SEMPRE** usar o scaler treinado, nunca criar um novo:
```python
# âŒ ERRADO
scaler_new = StandardScaler()
X_new_scaled = scaler_new.fit_transform(X_new)

# âœ… CORRETO
X_new_scaled = scaler.transform(X_new)
```

### 4. Threshold DinÃ¢mico
O threshold muda para cada parada baseado em seu histÃ³rico:
- Paradas boas (â‰¥50% conv.): threshold = 0.40
- Paradas ruins (<10% conv.): threshold = 0.75

### 5. AtualizaÃ§Ãµes PeriÃ³dicas
RecomendaÃ§Ãµes:
- **Mensal:** Retreinar modelo com dados recentes
- **Semanal:** Atualizar dicionÃ¡rios de conversÃ£o (stop_historical_conversion, etc.)
- **DiÃ¡rio:** Monitorar mÃ©tricas de prediÃ§Ã£o vs realidade

---

## ğŸ“Š InterpretaÃ§Ã£o das MÃ©tricas

### Quando o Modelo Ã‰ Bom?

**ROC-AUC:**
- 0.90-0.95: Excelente (nosso caso: 0.9425)
- 0.80-0.90: Bom
- 0.70-0.80: RazoÃ¡vel
- <0.70: Ruim

**F1-Score Classe 1 (ConversÃµes):**
- >0.60: Excelente para classe desbalanceada
- 0.50-0.60: Bom (nosso caso: 0.5539)
- 0.40-0.50: RazoÃ¡vel
- <0.40: Ruim

**F1-Macro:**
- >0.75: Excelente (nosso caso: 0.7558)
- 0.65-0.75: Bom
- 0.55-0.65: RazoÃ¡vel
- <0.55: Ruim

---

## ğŸ”§ Troubleshooting

### Problema: F1 Classe 1 muito baixo

**SoluÃ§Ãµes:**
1. Aumentar `scale_pos_weight`
2. Ajustar sample weights (dar mais peso Ã  classe 1)
3. Diminuir thresholds dinÃ¢micos

### Problema: Muitos falsos positivos

**SoluÃ§Ãµes:**
1. Aumentar thresholds dinÃ¢micos
2. Reduzir peso da classe 1 em sample weights
3. Adicionar features que diferenciem melhor as classes

### Problema: Modelo nÃ£o generaliza

**SoluÃ§Ãµes:**
1. Verificar se TimeSeriesSplit estÃ¡ sendo usado
2. Aumentar `min_child_samples` / `min_child_weight`
3. Reduzir `num_leaves` / `max_depth`
4. Aumentar regularizaÃ§Ã£o (`lambda_l1`, `lambda_l2`)

---

## ğŸ“ˆ PrÃ³ximos Passos

### Melhorias PossÃ­veis:

1. **Features Adicionais:**
   - Clima/Tempo (temperatura, chuva)
   - Eventos especiais (feriados, shows, jogos)
   - PadrÃµes de trÃ¡fego (congestionamento)

2. **Modelos Alternativos:**
   - CatBoost (melhor para categÃ³ricas)
   - Neural Networks (MLP, LSTM)
   - Stacking de mÃºltiplos modelos

3. **Threshold Adaptativo:**
   - Ajustar threshold baseado em ROI
   - Otimizar para maximizar lucro, nÃ£o apenas F1

4. **Monitoramento:**
   - Dashboard em tempo real
   - Alertas de degradaÃ§Ã£o de performance
   - A/B testing de novas versÃµes

---

## âœ… Checklist de Deploy

- [ ] Treinar modelo com dataset completo
- [ ] Validar mÃ©tricas em holdout set temporal
- [ ] Salvar todos os artefatos
- [ ] Testar script de inferÃªncia com dados reais
- [ ] Documentar versÃ£o e data do modelo
- [ ] Configurar monitoramento de performance
- [ ] Definir critÃ©rios de retreinamento
- [ ] Criar fallback para falhas do modelo
- [ ] Configurar logging de prediÃ§Ãµes
- [ ] Validar latÃªncia em produÃ§Ã£o (<100ms ideal)

---

## ğŸ“ Contato e Suporte

Para dÃºvidas sobre este modelo:
1. Revisar esta documentaÃ§Ã£o
2. Analisar comentÃ¡rios no cÃ³digo
3. Verificar mÃ©tricas de validaÃ§Ã£o cruzada
4. Comparar com versÃµes anteriores (v7, v6, etc.)

---

**VersÃ£o da DocumentaÃ§Ã£o:** 1.0  
**Data:** Novembro 2025  
**Modelo:** V8 Production (TimeSeriesSplit)
