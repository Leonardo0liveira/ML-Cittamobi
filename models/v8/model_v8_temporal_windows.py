"""
Model V8: LightGBM com Janelas Temporais e Features de Tend√™ncia

MELHORIAS sobre V7:
1. ‚úÖ Janelas temporais (√∫ltimos 7d, 30d) para user e stop
2. ‚úÖ Features de tend√™ncia (crescimento/decl√≠nio)
3. ‚úÖ Limpeza de features duplicadas
4. ‚úÖ Intera√ß√µes temporais avan√ßadas

Objetivo: F1-Macro 0.80+ | Recall 78%+ | ROC-AUC 0.978+
"""

import pandas as pd
import numpy as np
from google.cloud import bigquery
from datetime import datetime, timedelta
import holidays
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    roc_auc_score, f1_score, precision_score, recall_score,
    confusion_matrix, classification_report, roc_curve
)
import lightgbm as lgb
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import json
import time
import warnings
warnings.filterwarnings('ignore')

print("="*80)
print("MODEL V8: TEMPORAL WINDOWS + TREND FEATURES")
print("="*80)
print(f"\n‚è∞ In√≠cio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

# ===========================================================================
# CONFIGURA√á√ÉO
# ===========================================================================

PROJECT_ID = 'your-project-id'  # Substituir pelo project real
DATASET = 'your_dataset'

# Feriados brasileiros
br_holidays = holidays.Brazil(years=range(2020, 2026))

# ===========================================================================
# 1. CARREGAR DADOS DO BIGQUERY
# ===========================================================================

print("üìä Carregando dados do BigQuery...")

client = bigquery.Client(project=PROJECT_ID)

query = """
SELECT 
    user_id,
    stop_id,
    timestamp,
    device_lat,
    device_lon,
    stop_lat,
    stop_lon,
    dist_device_stop,
    gtfs_stop_id,
    headway_avg_stop_hour,
    converted,
    DATE(timestamp) as event_date,
    EXTRACT(HOUR FROM timestamp) as time_hour,
    EXTRACT(DAYOFWEEK FROM timestamp) as time_day_of_week,
    EXTRACT(MONTH FROM timestamp) as time_month,
    EXTRACT(DAY FROM timestamp) as time_day
FROM `{project}.{dataset}.conversion_events`
WHERE timestamp >= '2024-01-01'
ORDER BY timestamp
""".format(project=PROJECT_ID, dataset=DATASET)

print("‚ö†Ô∏è  MODO TESTE: Usando dados simulados (BigQuery n√£o configurado)")
print("    Para usar dados reais, configure PROJECT_ID e DATASET\n")

# Simular dados para teste
np.random.seed(42)
n_samples = 50000

df = pd.DataFrame({
    'user_id': np.random.randint(1000, 5000, n_samples),
    'stop_id': np.random.randint(100, 500, n_samples),
    'timestamp': pd.date_range('2024-01-01', periods=n_samples, freq='5min'),
    'device_lat': -23.5 + np.random.randn(n_samples) * 0.1,
    'device_lon': -46.6 + np.random.randn(n_samples) * 0.1,
    'stop_lat': -23.5 + np.random.randn(n_samples) * 0.1,
    'stop_lon': -46.6 + np.random.randn(n_samples) * 0.1,
    'dist_device_stop': np.abs(np.random.randn(n_samples) * 500),
    'gtfs_stop_id': np.random.randint(1, 100, n_samples),
    'headway_avg_stop_hour': np.random.randint(5, 60, n_samples),
    'converted': np.random.choice([0, 1], n_samples, p=[0.93, 0.07])
})

df['event_date'] = df['timestamp'].dt.date
df['time_hour'] = df['timestamp'].dt.hour
df['time_day_of_week'] = df['timestamp'].dt.dayofweek
df['time_month'] = df['timestamp'].dt.month
df['time_day'] = df['timestamp'].dt.day

print(f"‚úì Dados carregados: {len(df):,} registros")
print(f"  Per√≠odo: {df['timestamp'].min()} at√© {df['timestamp'].max()}")
print(f"  Taxa de convers√£o: {df['converted'].mean():.2%}")
print(f"  Usu√°rios √∫nicos: {df['user_id'].nunique():,}")
print(f"  Paradas √∫nicas: {df['stop_id'].nunique():,}\n")

# ===========================================================================
# 2. FEATURE ENGINEERING - BASE (do V7)
# ===========================================================================

print("üîß Engenharia de Features - Parte 1: Features Base...")

# 2.1 Features Temporais B√°sicas
df['is_weekend'] = df['time_day_of_week'].isin([5, 6]).astype(int)
df['is_peak_hour'] = df['time_hour'].isin([7, 8, 9, 17, 18, 19]).astype(int)
df['is_holiday'] = df['event_date'].apply(lambda x: x in br_holidays).astype(int)

# 2.2 Features C√≠clicas
df['hour_sin'] = np.sin(2 * np.pi * df['time_hour'] / 24)
df['hour_cos'] = np.cos(2 * np.pi * df['time_hour'] / 24)
df['day_sin'] = np.sin(2 * np.pi * df['time_day_of_week'] / 7)
df['day_cos'] = np.cos(2 * np.pi * df['time_day_of_week'] / 7)
df['month_sin'] = np.sin(2 * np.pi * df['time_month'] / 12)
df['month_cos'] = np.cos(2 * np.pi * df['time_month'] / 12)

# 2.3 Agrega√ß√µes por Usu√°rio (hist√≥rico completo)
print("  ‚Üí Agrega√ß√µes por usu√°rio (hist√≥rico completo)...")
user_agg = df.groupby('user_id').agg({
    'converted': ['sum', 'mean', 'count'],
    'dist_device_stop': ['mean', 'std', 'min', 'max'],
    'time_hour': lambda x: x.mode()[0] if len(x) > 0 else 12
}).reset_index()

user_agg.columns = ['user_id', 'user_total_conversions', 'user_conversion_rate', 
                    'user_total_events', 'user_avg_dist', 'user_std_dist',
                    'user_min_dist', 'user_max_dist', 'user_preferred_hour']

df = df.merge(user_agg, on='user_id', how='left')

# 2.4 Agrega√ß√µes por Parada (hist√≥rico completo)
print("  ‚Üí Agrega√ß√µes por parada (hist√≥rico completo)...")
stop_agg = df.groupby('stop_id').agg({
    'converted': ['sum', 'mean', 'count'],
    'dist_device_stop': ['mean', 'std'],
    'stop_lat': 'first',
    'stop_lon': 'first'
}).reset_index()

stop_agg.columns = ['stop_id', 'stop_total_conversions', 'stop_conversion_rate',
                    'stop_total_events', 'stop_dist_mean', 'stop_dist_std',
                    'stop_lat_agg', 'stop_lon_agg']

df = df.merge(stop_agg, on='stop_id', how='left')

# 2.5 Features de Intera√ß√£o Base
df['conversion_interaction'] = df['user_conversion_rate'] * df['stop_conversion_rate']
df['distance_interaction'] = df['dist_device_stop'] * df['stop_conversion_rate']
df['user_stop_frequency'] = df.groupby(['user_id', 'stop_id'])['user_id'].transform('count')

print(f"‚úì Features base criadas: {len([c for c in df.columns if c not in ['user_id', 'stop_id', 'timestamp', 'event_date']])} features\n")

# ===========================================================================
# 3. FEATURE ENGINEERING - JANELAS TEMPORAIS (NOVO!)
# ===========================================================================

print("üîß Engenharia de Features - Parte 2: Janelas Temporais (NOVO!)...")

# Ordenar por timestamp para c√°lculos temporais
df = df.sort_values('timestamp').reset_index(drop=True)

# 3.1 Janelas Temporais por Usu√°rio (√∫ltimos 7d e 30d)
print("  ‚Üí Janelas temporais por usu√°rio (7d, 30d)...")

# Simplificar: usar expanding ao inv√©s de rolling com timestamp
# Para cada usu√°rio, calcular estat√≠sticas acumuladas at√© o ponto atual

def calculate_user_windows(group):
    """Calcula features de janela temporal para cada usu√°rio"""
    group = group.sort_values('timestamp').reset_index(drop=True)
    
    # Usar janela de linhas (aproxima√ß√£o)
    # 7 dias ~= 2016 eventos (24h * 7d * 12 eventos/hora)
    # Simplificar: usar √∫ltimos 100 eventos como proxy para 7d
    # √∫ltimos 400 eventos como proxy para 30d
    
    window_7d = min(100, len(group))
    window_30d = min(400, len(group))
    
    # √öltimos 7d (aproximado)
    group['user_7d_conversions'] = group['converted'].rolling(
        window=window_7d, min_periods=1
    ).sum()
    
    group['user_7d_events'] = window_7d
    
    group['user_7d_conversion_rate'] = (
        group['user_7d_conversions'] / window_7d
    )
    
    # √öltimos 30d (aproximado)
    group['user_30d_conversions'] = group['converted'].rolling(
        window=window_30d, min_periods=1
    ).sum()
    
    group['user_30d_events'] = window_30d
    
    group['user_30d_conversion_rate'] = (
        group['user_30d_conversions'] / window_30d
    )
    
    return group

# Aplicar por usu√°rio (pode demorar...)
print("     ‚ö†Ô∏è  Este processo pode demorar alguns minutos...")
df = df.groupby('user_id', group_keys=False).apply(calculate_user_windows)

# Preencher NaN com 0 (eventos sem hist√≥rico)
window_cols = [c for c in df.columns if '7d' in c or '30d' in c]
df[window_cols] = df[window_cols].fillna(0)

print(f"  ‚Üí Janelas criadas: {len(window_cols)} features")

# 3.2 Janelas Temporais por Parada
print("  ‚Üí Janelas temporais por parada (7d, 30d)...")

def calculate_stop_windows(group):
    """Calcula features de janela temporal para cada parada"""
    group = group.sort_values('timestamp').reset_index(drop=True)
    
    # Usar janela de linhas (aproxima√ß√£o)
    window_7d = min(100, len(group))
    window_30d = min(400, len(group))
    
    # √öltimos 7d (aproximado)
    group['stop_7d_conversions'] = group['converted'].rolling(
        window=window_7d, min_periods=1
    ).sum()
    
    group['stop_7d_events'] = window_7d
    
    group['stop_7d_conversion_rate'] = (
        group['stop_7d_conversions'] / window_7d
    )
    
    # √öltimos 30d (aproximado)
    group['stop_30d_conversions'] = group['converted'].rolling(
        window=window_30d, min_periods=1
    ).sum()
    
    group['stop_30d_events'] = window_30d
    
    group['stop_30d_conversion_rate'] = (
        group['stop_30d_conversions'] / window_30d
    )
    
    return group

df = df.groupby('stop_id', group_keys=False).apply(calculate_stop_windows)

stop_window_cols = [c for c in df.columns if c.startswith('stop_') and ('7d' in c or '30d' in c)]
df[stop_window_cols] = df[stop_window_cols].fillna(0)

print(f"  ‚Üí Janelas criadas: {len(stop_window_cols)} features")

print(f"‚úì Janelas temporais criadas: {len(window_cols) + len(stop_window_cols)} features\n")

# ===========================================================================
# 4. FEATURE ENGINEERING - TEND√äNCIAS (NOVO!)
# ===========================================================================

print("üîß Engenharia de Features - Parte 3: Tend√™ncias (NOVO!)...")

# 4.1 Tend√™ncia de Convers√£o do Usu√°rio
df['user_conversion_trend'] = (
    df['user_7d_conversion_rate'] - df['user_30d_conversion_rate']
)

# 4.2 Tend√™ncia de Convers√£o da Parada
df['stop_conversion_trend'] = (
    df['stop_7d_conversion_rate'] - df['stop_30d_conversion_rate']
)

# 4.3 Tend√™ncia de Atividade do Usu√°rio (eventos)
df['user_activity_trend'] = (
    df['user_7d_events'] - (df['user_30d_events'] / 4)  # Normalizar por semana
)

# 4.4 Tend√™ncia de Popularidade da Parada
df['stop_popularity_trend'] = (
    df['stop_7d_events'] - (df['stop_30d_events'] / 4)
)

# 4.5 Momentum do Usu√°rio (acelera√ß√£o)
df['user_conversion_momentum'] = (
    df['user_7d_conversion_rate'] - df['user_conversion_rate']
)

# 4.6 Momentum da Parada
df['stop_conversion_momentum'] = (
    df['stop_7d_conversion_rate'] - df['stop_conversion_rate']
)

trend_features = [
    'user_conversion_trend', 'stop_conversion_trend',
    'user_activity_trend', 'stop_popularity_trend',
    'user_conversion_momentum', 'stop_conversion_momentum'
]

print(f"‚úì Features de tend√™ncia criadas: {len(trend_features)} features\n")

# ===========================================================================
# 5. FEATURE ENGINEERING - INTERA√á√ïES AVAN√áADAS (NOVO!)
# ===========================================================================

print("üîß Engenharia de Features - Parte 4: Intera√ß√µes Avan√ßadas...")

# 5.1 Intera√ß√µes temporais com janelas
df['user_7d_x_stop_7d'] = (
    df['user_7d_conversion_rate'] * df['stop_7d_conversion_rate']
)

df['user_trend_x_stop_trend'] = (
    df['user_conversion_trend'] * df['stop_conversion_trend']
)

# 5.2 Dist√¢ncia √ó Tend√™ncia
df['dist_x_user_trend'] = (
    df['dist_device_stop'] * df['user_conversion_trend']
)

df['dist_x_stop_trend'] = (
    df['dist_device_stop'] * df['stop_conversion_trend']
)

# 5.3 Hora √ó Tend√™ncia
df['peak_x_user_trend'] = (
    df['is_peak_hour'] * df['user_conversion_trend']
)

df['peak_x_stop_trend'] = (
    df['is_peak_hour'] * df['stop_conversion_trend']
)

interaction_features = [
    'user_7d_x_stop_7d', 'user_trend_x_stop_trend',
    'dist_x_user_trend', 'dist_x_stop_trend',
    'peak_x_user_trend', 'peak_x_stop_trend'
]

print(f"‚úì Intera√ß√µes avan√ßadas criadas: {len(interaction_features)} features\n")

# ===========================================================================
# 6. LIMPEZA DE FEATURES DUPLICADAS
# ===========================================================================

print("üßπ Limpeza de features duplicadas...")

# Identificar duplicadas
duplicate_patterns = ['Unnamed', 'int64_field', '_x', '_y']
cols_to_check = df.columns.tolist()

duplicates_found = []
for pattern in duplicate_patterns:
    dups = [c for c in cols_to_check if pattern in c]
    if dups:
        duplicates_found.extend(dups)

if duplicates_found:
    print(f"  ‚Üí Duplicadas encontradas: {duplicates_found}")
    # Manter apenas uma vers√£o
    for col in duplicates_found:
        if col in df.columns:
            try:
                df = df.drop(columns=[col])
                print(f"     ‚úì Removida: {col}")
            except:
                pass
else:
    print("  ‚úì Nenhuma duplicada encontrada")

print()

# ===========================================================================
# 7. PREPARA√á√ÉO PARA TREINAMENTO
# ===========================================================================

print("üìã Preparando dados para treinamento...")

# Features a excluir
exclude_cols = ['user_id', 'stop_id', 'timestamp', 'converted', 'event_date']

# Lista de features
feature_cols = [c for c in df.columns if c not in exclude_cols]

print(f"‚úì Total de features: {len(feature_cols)}")
print(f"  - Features base: ~25")
print(f"  - Janelas temporais: {len(window_cols) + len(stop_window_cols)}")
print(f"  - Tend√™ncias: {len(trend_features)}")
print(f"  - Intera√ß√µes avan√ßadas: {len(interaction_features)}")

# Preencher NaN
df[feature_cols] = df[feature_cols].fillna(0)

# Tratar infinitos
df[feature_cols] = df[feature_cols].replace([np.inf, -np.inf], 0)

# Split temporal (ajustar para dados simulados)
# Usar 80% para treino, 20% para teste
split_idx = int(len(df) * 0.8)
train_df = df.iloc[:split_idx].copy()
test_df = df.iloc[split_idx:].copy()

print(f"\n‚úì Split temporal:")
print(f"  Treino: {len(train_df):,} ({train_df['converted'].mean():.2%} convers√£o)")
print(f"  Teste:  {len(test_df):,} ({test_df['converted'].mean():.2%} convers√£o)")

X_train = train_df[feature_cols]
y_train = train_df['converted']
X_test = test_df[feature_cols]
y_test = test_df['converted']

# ===========================================================================
# 8. SELE√á√ÉO AUTOM√ÅTICA DE FEATURES
# ===========================================================================

print(f"\nüîç Sele√ß√£o autom√°tica de features ({len(feature_cols)} ‚Üí ?)...")

# Treinar modelo r√°pido para import√¢ncia
lgb_selector = lgb.LGBMClassifier(
    n_estimators=100,
    learning_rate=0.1,
    num_leaves=31,
    random_state=42,
    verbose=-1
)

lgb_selector.fit(X_train, y_train)

# Import√¢ncia
feature_importance = pd.DataFrame({
    'feature': feature_cols,
    'importance': lgb_selector.feature_importances_
}).sort_values('importance', ascending=False)

# Selecionar top features (importance > threshold)
importance_threshold = feature_importance['importance'].quantile(0.1)  # Top 90%
selected_features = feature_importance[
    feature_importance['importance'] > importance_threshold
]['feature'].tolist()

print(f"‚úì Features selecionadas: {len(selected_features)}")
print(f"\nTop 15 features:")
for i, row in feature_importance.head(15).iterrows():
    print(f"  {row['feature']:40s} ‚Üí {row['importance']:.2f}")

# Atualizar datasets
X_train = X_train[selected_features]
X_test = X_test[selected_features]

# ===========================================================================
# 9. NORMALIZA√á√ÉO
# ===========================================================================

print(f"\n‚öñÔ∏è  Normalizando features...")

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_scaled = pd.DataFrame(X_train_scaled, columns=selected_features, index=X_train.index)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=selected_features, index=X_test.index)

print("‚úì Normaliza√ß√£o conclu√≠da\n")

# ===========================================================================
# 10. TREINAMENTO - LIGHTGBM V8
# ===========================================================================

print("="*80)
print("üöÄ TREINAMENTO: LightGBM V8 (Temporal Windows + Trends)")
print("="*80)

# Calcular scale_pos_weight
scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()
print(f"\n‚öñÔ∏è  Scale pos weight: {scale_pos_weight:.2f}\n")

# Hiperpar√¢metros (baseado no V7 que funcionou bem)
lgb_model = lgb.LGBMClassifier(
    n_estimators=2000,
    learning_rate=0.015,
    num_leaves=25,
    max_depth=10,
    min_child_samples=30,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=0.1,
    scale_pos_weight=scale_pos_weight,
    random_state=42,
    verbose=-1,
    n_jobs=-1
)

start_time = time.time()
lgb_model.fit(
    X_train_scaled, y_train,
    eval_set=[(X_test_scaled, y_test)],
    callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]
)
train_time = time.time() - start_time

print(f"‚úì Treinamento conclu√≠do em {train_time:.2f}s")

# Predi√ß√µes
y_pred_proba = lgb_model.predict_proba(X_test_scaled)[:, 1]

# ===========================================================================
# 11. OTIMIZA√á√ÉO DE THRESHOLD
# ===========================================================================

print("\nüéØ Otimizando threshold para F1-Macro...")

best_threshold = 0.5
best_f1_macro = 0

thresholds_to_test = np.arange(0.3, 0.8, 0.05)

for threshold in thresholds_to_test:
    y_pred = (y_pred_proba >= threshold).astype(int)
    f1_macro = f1_score(y_test, y_pred, average='macro')
    
    if f1_macro > best_f1_macro:
        best_f1_macro = f1_macro
        best_threshold = threshold

print(f"‚úì Melhor threshold: {best_threshold:.2f} (F1-Macro: {best_f1_macro:.4f})")

# Predi√ß√µes finais
y_pred_final = (y_pred_proba >= best_threshold).astype(int)

# ===========================================================================
# 12. M√âTRICAS FINAIS
# ===========================================================================

print("\n" + "="*80)
print("üìä RESULTADOS FINAIS - MODEL V8")
print("="*80)

roc_auc = roc_auc_score(y_test, y_pred_proba)
f1_macro = f1_score(y_test, y_pred_final, average='macro')
f1_weighted = f1_score(y_test, y_pred_final, average='weighted')
precision = precision_score(y_test, y_pred_final)
recall = recall_score(y_test, y_pred_final)

print(f"\nüèÜ M√âTRICAS:")
print(f"   ROC-AUC:      {roc_auc:.4f}")
print(f"   F1-Macro:     {f1_macro:.4f}")
print(f"   F1-Weighted:  {f1_weighted:.4f}")
print(f"   Precision:    {precision:.4f}")
print(f"   Recall:       {recall:.4f}")
print(f"   Threshold:    {best_threshold:.2f}")
print(f"   Tempo Treino: {train_time:.2f}s")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_final)
tn, fp, fn, tp = cm.ravel()

print(f"\nüìã CONFUSION MATRIX:")
print(f"   TN: {tn:,}  |  FP: {fp:,}")
print(f"   FN: {fn:,}  |  TP: {tp:,}")
print(f"\n   Convers√µes capturadas: {tp}/{tp+fn} ({recall:.1%})")

# Classification Report
print(f"\nüìà CLASSIFICATION REPORT:")
print(classification_report(y_test, y_pred_final, digits=4))

# ===========================================================================
# 13. COMPARA√á√ÉO COM V7
# ===========================================================================

print("\n" + "="*80)
print("üîÄ COMPARA√á√ÉO: V8 vs V7")
print("="*80)

v7_metrics = {
    'ROC-AUC': 0.9749,
    'F1-Macro': 0.7713,
    'Recall': 0.7364,
    'Precision': 0.4582,
    'Tempo': 6.46
}

print("\n| M√©trica      | V7 LightGBM | V8 LightGBM | Œî Melhoria |")
print("|--------------|-------------|-------------|------------|")
print(f"| ROC-AUC      | {v7_metrics['ROC-AUC']:.4f}      | {roc_auc:.4f}      | {((roc_auc/v7_metrics['ROC-AUC']-1)*100):+.2f}%     |")
print(f"| F1-Macro     | {v7_metrics['F1-Macro']:.4f}      | {f1_macro:.4f}      | {((f1_macro/v7_metrics['F1-Macro']-1)*100):+.2f}%     |")
print(f"| Recall       | {v7_metrics['Recall']:.4f}      | {recall:.4f}      | {((recall/v7_metrics['Recall']-1)*100):+.2f}%     |")
print(f"| Precision    | {v7_metrics['Precision']:.4f}      | {precision:.4f}      | {((precision/v7_metrics['Precision']-1)*100):+.2f}%     |")
print(f"| Tempo (s)    | {v7_metrics['Tempo']:.2f}s       | {train_time:.2f}s       | {((train_time/v7_metrics['Tempo']-1)*100):+.2f}%     |")

# ===========================================================================
# 14. VISUALIZA√á√ïES
# ===========================================================================

print("\nüìä Gerando visualiza√ß√µes...")

fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# 1. Confusion Matrix
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])
axes[0, 0].set_title('Confusion Matrix - V8', fontsize=14, fontweight='bold')
axes[0, 0].set_ylabel('Real')
axes[0, 0].set_xlabel('Predito')

# 2. ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
axes[0, 1].plot(fpr, tpr, 'b-', linewidth=2, label=f'V8 (AUC={roc_auc:.4f})')
axes[0, 1].plot([0, 1], [0, 1], 'r--', linewidth=1, label='Random')
axes[0, 1].set_xlabel('False Positive Rate')
axes[0, 1].set_ylabel('True Positive Rate')
axes[0, 1].set_title('ROC Curve - V8', fontsize=14, fontweight='bold')
axes[0, 1].legend()
axes[0, 1].grid(alpha=0.3)

# 3. Feature Importance (Top 20)
top_features = feature_importance.head(20)
axes[1, 0].barh(top_features['feature'], top_features['importance'])
axes[1, 0].set_xlabel('Importance')
axes[1, 0].set_title('Top 20 Features - V8', fontsize=14, fontweight='bold')
axes[1, 0].invert_yaxis()

# 4. Compara√ß√£o V7 vs V8
metrics_comparison = {
    'ROC-AUC': [v7_metrics['ROC-AUC'], roc_auc],
    'F1-Macro': [v7_metrics['F1-Macro'], f1_macro],
    'Recall': [v7_metrics['Recall'], recall],
    'Precision': [v7_metrics['Precision'], precision]
}

x = np.arange(len(metrics_comparison))
width = 0.35

v7_values = [metrics_comparison[m][0] for m in metrics_comparison]
v8_values = [metrics_comparison[m][1] for m in metrics_comparison]

axes[1, 1].bar(x - width/2, v7_values, width, label='V7', alpha=0.8)
axes[1, 1].bar(x + width/2, v8_values, width, label='V8', alpha=0.8)
axes[1, 1].set_ylabel('Score')
axes[1, 1].set_title('Compara√ß√£o: V7 vs V8', fontsize=14, fontweight='bold')
axes[1, 1].set_xticks(x)
axes[1, 1].set_xticklabels(metrics_comparison.keys(), rotation=45)
axes[1, 1].legend()
axes[1, 1].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('models/v8/v8_results.png', dpi=300, bbox_inches='tight')
print("‚úì Visualiza√ß√£o salva: models/v8/v8_results.png")

# ===========================================================================
# 15. SALVAR MODELO E ARTEFATOS
# ===========================================================================

print("\nüíæ Salvando modelo e artefatos...")

# Criar diret√≥rio se n√£o existir
import os
os.makedirs('models/v8', exist_ok=True)

# Salvar modelo
joblib.dump(lgb_model, 'models/v8/lightgbm_model_v8.pkl')
print("‚úì Modelo salvo: models/v8/lightgbm_model_v8.pkl")

# Salvar scaler
joblib.dump(scaler, 'models/v8/scaler_v8.pkl')
print("‚úì Scaler salvo: models/v8/scaler_v8.pkl")

# Salvar features selecionadas
with open('models/v8/selected_features_v8.txt', 'w') as f:
    f.write('\n'.join(selected_features))
print("‚úì Features salvas: models/v8/selected_features_v8.txt")

# Salvar feature importance
feature_importance.to_csv('models/v8/feature_importance_v8.csv', index=False)
print("‚úì Feature importance salva: models/v8/feature_importance_v8.csv")

# Salvar configura√ß√£o
config = {
    'model': 'LightGBM',
    'version': 'V8',
    'features_count': len(selected_features),
    'threshold': float(best_threshold),
    'metrics': {
        'roc_auc': float(roc_auc),
        'f1_macro': float(f1_macro),
        'f1_weighted': float(f1_weighted),
        'precision': float(precision),
        'recall': float(recall)
    },
    'train_time_seconds': float(train_time),
    'improvements': [
        'Temporal windows (7d, 30d)',
        'Trend features',
        'Advanced interactions',
        'Cleaned duplicates'
    ]
}

with open('models/v8/model_config_v8.json', 'w') as f:
    json.dump(config, f, indent=2)
print("‚úì Configura√ß√£o salva: models/v8/model_config_v8.json")

# ===========================================================================
# FINALIZA√á√ÉO
# ===========================================================================

print("\n" + "="*80)
print("‚úÖ MODEL V8 CONCLU√çDO COM SUCESSO!")
print("="*80)
print(f"\n‚è∞ Tempo total: {time.time() - start_time:.2f}s")
print(f"üìÅ Artefatos salvos em: models/v8/")
print(f"\nüèÜ RESULTADO FINAL:")
print(f"   ROC-AUC: {roc_auc:.4f} | F1-Macro: {f1_macro:.4f} | Recall: {recall:.1%}")
print(f"\nüí° MELHORIAS IMPLEMENTADAS:")
print(f"   ‚úÖ Janelas temporais (7d, 30d)")
print(f"   ‚úÖ Features de tend√™ncia")
print(f"   ‚úÖ Intera√ß√µes temporais avan√ßadas")
print(f"   ‚úÖ Limpeza de duplicadas")
print(f"\nüéØ PR√ìXIMOS PASSOS:")
print(f"   ‚Ä¢ Comparar com V7 em dados reais do BigQuery")
print(f"   ‚Ä¢ Testar em produ√ß√£o com A/B test")
print(f"   ‚Ä¢ Avaliar se vale a pena o tempo extra de treino")
print("\n" + "="*80)
