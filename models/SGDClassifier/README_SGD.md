# üìà SGD Classifier - Modelo Leak-Free

## üìã Vis√£o Geral

Modelo **Stochastic Gradient Descent (SGD) Classifier** otimizado para predi√ß√£o de convers√£o de usu√°rios em aplicativo de transporte p√∫blico (Cittamobi).

- **Algoritmo**: SGD Classifier (Logistic Regression via SGD)
- **Melhor Config**: HIGH_REGULARIZATION
- **Loss Function**: log_loss (logistic regression)
- **Penalty**: l2
- **ROC-AUC**: 0.7963
- **F1-Macro**: 0.6726
- **Status**: ‚úÖ Leak-Free (sem vazamento de dados)

---

## üö® Preven√ß√£o de Data Leakage

### ‚ùå Problema Identificado
Features como `user_conversion_rate` e `stop_conversion_rate` eram calculadas usando o pr√≥prio target, causando **vazamento de dados** e ROC-AUC artificialmente alto (>98%).

### ‚úÖ Solu√ß√£o Implementada
1. **Expanding Windows**: Para cada evento em tempo T, usar apenas dados hist√≥ricos < T
2. **TimeSeriesSplit**: Valida√ß√£o temporal que respeita ordem cronol√≥gica
3. **Features Hist√≥ricas**: Substitui√ß√£o por agrega√ß√µes baseadas apenas no passado
4. **Normaliza√ß√£o**: StandardScaler essencial para SGD funcionar corretamente

---

## üìä M√©tricas de Performance

| M√©trica | Valor |
|---------|-------|
| **ROC-AUC** | **0.7963** |
| Accuracy | 0.8863 |
| Precision | 0.3923 |
| Recall | 0.4253 |
| F1-Score | 0.4081 |
| F1-Macro | 0.6726 |
| Threshold | 0.75 |

### Matriz de Confus√£o

```
                 Predito
                 0        1
Real  0      10,394      745
      1         650      481
```

- **True Negatives**: 10,394
- **False Positives**: 745
- **False Negatives**: 650
- **True Positives**: 481

---

## üîç Compara√ß√£o de Configura√ß√µes

| Config | ROC-AUC | F1-Macro | Alpha | Penalty | Tempo (s) |
|--------|---------|----------|-------|---------|----------|
| HIGH_REGULARIZATION üèÜ | 0.7963 | 0.6726 | 0.001 | l2 | 0.2 |
| ELASTIC_NET | 0.7841 | 0.6446 | 0.0001 | elasticnet | 0.3 |
| L1_PENALTY | 0.7669 | 0.6604 | 0.0001 | elasticnet | 0.3 |
| LOW_REGULARIZATION | 0.7108 | 0.5247 | 1e-05 | l2 | 0.2 |
| BASELINE | 0.6997 | 0.5513 | 0.0001 | l2 | 0.3 |

### Insights sobre Configura√ß√µes
- **BASELINE**: Configura√ß√£o padr√£o com alpha=0.0001
- **HIGH_REGULARIZATION**: Maior alpha (0.001) previne overfitting
- **LOW_REGULARIZATION**: Menor alpha (0.00001) permite mais complexidade
- **ELASTIC_NET**: Combina L1 e L2 (l1_ratio=0.5)
- **L1_PENALTY**: Lasso (l1_ratio=1.0) para sele√ß√£o de features

---

## üîß Configura√ß√£o T√©cnica

### Par√¢metros SGD Classifier
```python
SGDClassifier(
    loss='log_loss',            # Regress√£o log√≠stica
    penalty='l2',           # Regulariza√ß√£o
    alpha=0.001,          # Taxa de regulariza√ß√£o
    l1_ratio=0.0,            # Elastic Net ratio
    class_weight='balanced',    # Lida com desbalanceamento
    learning_rate='optimal',    # Taxa de aprendizado adaptativa
    max_iter=1000,              # M√°ximo de √©pocas
    early_stopping=True,        # Para se n√£o houver melhoria
    validation_fraction=0.1,    # 10% para valida√ß√£o
    n_iter_no_change=5,         # Paci√™ncia: 5 √©pocas
    random_state=42,
    n_jobs=-1                   # Usa todos os cores
)
```

### Pipeline de Pr√©-processamento
```python
Pipeline([
    ('scaler', StandardScaler()),  # Normaliza√ß√£o ESSENCIAL!
    ('sgd', SGDClassifier(...))
])
```

‚ö†Ô∏è **IMPORTANTE**: StandardScaler √© **obrigat√≥rio** para SGD! Sem normaliza√ß√£o, features com escalas diferentes dominam o gradiente.

---

## üìà Top 20 Features Mais Importantes

*(Baseado em coeficientes do modelo)*

| Rank | Feature | Coeficiente |
|------|---------|-------------|
| 1 | `stop_event_rate` | +1.143173 |
| 2 | `stop_event_count` | -0.487480 |
| 3 | `is_peak_hour` | -0.201753 |
| 4 | `day_of_week` | -0.163586 |
| 5 | `time_day_of_week` | +0.152031 |
| 6 | `headway_x_hour` | +0.151508 |
| 7 | `hour_cos` | +0.151393 |
| 8 | `stop_lon_event` | -0.148723 |
| 9 | `stop_lat_event` | +0.142275 |
| 10 | `time_hour` | +0.142269 |
| 11 | `stop_total_samples` | +0.139098 |
| 12 | `headway_x_weekend` | -0.116230 |
| 13 | `stop_dist_mean` | -0.116183 |
| 14 | `time_day_of_month` | +0.115950 |
| 15 | `day_cos` | -0.100169 |
| 16 | `int64_field_0` | -0.098772 |
| 17 | `device_lon` | -0.090760 |
| 18 | `is_weekend` | +0.086403 |
| 19 | `user_frequency` | -0.085427 |
| 20 | `user_recency_days` | +0.084441 |

- **Coeficiente Positivo**: Aumenta probabilidade de convers√£o
- **Coeficiente Negativo**: Diminui probabilidade de convers√£o

---

## üìä Compara√ß√£o com Outros Modelos

| Modelo | ROC-AUC | Observa√ß√µes |
|--------|---------|-------------|
| **V6 CatBoost** | **86.69%** | üèÜ Melhor modelo geral |
| **V5 LightGBM** | **86.42%** | Segundo melhor |
| **K-NN (K=31)** | **75.42%** | Mais simples |
| **SGD Classifier** | **79.63%** | R√°pido e eficiente |

### üí° Quando Usar SGD Classifier?

‚úÖ **Vantagens**:
- **Muito r√°pido**: Treina em mini-batches (ideal para dados grandes)
- **Leve**: Baixo consumo de mem√≥ria
- **Aprendizado online**: Pode ser atualizado com novos dados sem retreinar tudo
- **Regulariza√ß√£o flex√≠vel**: L1, L2 ou Elastic Net
- **Interpret√°vel**: Coeficientes mostram import√¢ncia e dire√ß√£o das features

‚ùå **Desvantagens**:
- **Modelo linear**: N√£o captura intera√ß√µes n√£o-lineares automaticamente
- **Performance inferior** a gradient boosting em problemas complexos
- **Sens√≠vel √† escala**: Requer normaliza√ß√£o obrigat√≥ria
- **Hiperpar√¢metros**: Requer tuning de alpha e learning rate

---

## üóÇÔ∏è Estrutura de Arquivos

```
SGDClassifier/
‚îú‚îÄ‚îÄ sgd_leak_free.py               # Script principal
‚îú‚îÄ‚îÄ README_SGD.md                   # Esta documenta√ß√£o
‚îú‚îÄ‚îÄ visualizations/
‚îÇ   ‚îú‚îÄ‚îÄ config_comparison.png       # Compara√ß√£o configura√ß√µes
‚îÇ   ‚îú‚îÄ‚îÄ roc_curve_sgd.png           # Curva ROC
‚îÇ   ‚îú‚îÄ‚îÄ confusion_matrix_sgd.png    # Matriz de confus√£o
‚îÇ   ‚îî‚îÄ‚îÄ feature_coefficients_sgd.png # Coeficientes features
‚îî‚îÄ‚îÄ reports/
    ‚îú‚îÄ‚îÄ sgd_leak_free_report.txt    # Relat√≥rio detalhado
    ‚îî‚îÄ‚îÄ sgd_config_comparison.csv    # Dados compara√ß√£o configs
```

---

## üöÄ Como Usar

### 1. Executar o Modelo
```bash
cd SGDClassifier
python sgd_leak_free.py
```

### 2. Ver Resultados
- **Visualiza√ß√µes**: `visualizations/*.png`
- **Relat√≥rio T√©cnico**: `reports/sgd_leak_free_report.txt`
- **Dados Compara√ß√£o**: `reports/sgd_config_comparison.csv`

### 3. Ajustar Par√¢metros
No c√≥digo `sgd_leak_free.py`, linha ~248:
```python
configs = [
    {'name': 'CUSTOM', 'alpha': 0.0005, 'l1_ratio': 0},
    # Adicionar mais configura√ß√µes
]
```

---

## ‚öôÔ∏è Requisitos T√©cnicos

```
Python >= 3.9
scikit-learn >= 1.0
pandas >= 1.3
numpy >= 1.21
matplotlib >= 3.4
seaborn >= 0.11
google-cloud-bigquery >= 3.0
```

---

## üìù Metodologia de Desenvolvimento

### 1. Prepara√ß√£o Temporal dos Dados
- Ordena√ß√£o cronol√≥gica por `event_timestamp`
- Features temporais e c√≠clicas (sin/cos)
- Per√≠odo: 3 meses de dados

### 2. Expanding Windows (Leak-Free)
Para cada evento em tempo T:
```python
# ‚úÖ CORRETO: Usa apenas hist√≥rico < T
hist_data = df.iloc[:i]  # Dados anteriores
user_hist_conversion_rate = hist_data[target].mean()

# ‚ùå ERRADO: Usa todos os dados (inclui futuro)
user_conversion_rate = df.groupby('user')[target].mean()
```

### 3. Valida√ß√£o Temporal
- **TimeSeriesSplit** com 3 folds
- Treino: 75% dos dados (temporalmente anteriores)
- Teste: 25% dos dados (temporalmente posteriores)

### 4. Otimiza√ß√£o de Hiperpar√¢metros
- Grid search manual em configura√ß√µes
- Threshold otimizado para maximizar F1-Macro
- StandardScaler aplicado em todas as features

---

## üéì Conceitos Importantes

### Stochastic Gradient Descent (SGD)
Algoritmo de otimiza√ß√£o que **atualiza pesos iterativamente** usando gradientes calculados em **mini-batches** de dados. Muito mais r√°pido que gradiente descendente tradicional.

### loss='log_loss'
Usa **log loss** (cross-entropy) como fun√ß√£o objetivo:
```
log_loss = -[y*log(p) + (1-y)*log(1-p)]
```
Equivalente a **regress√£o log√≠stica** treinada via SGD.

### Regulariza√ß√£o
Previne overfitting penalizando pesos grandes:
- **L2 (Ridge)**: penalty='l2' ‚Üí minimiza soma dos quadrados dos coeficientes
- **L1 (Lasso)**: penalty='l1' ‚Üí minimiza soma dos valores absolutos (feature selection)
- **Elastic Net**: combina L1 e L2 (l1_ratio controla propor√ß√£o)

### class_weight='balanced'
Ajusta pesos das classes automaticamente:
```
weight_class_i = n_samples / (n_classes * n_samples_class_i)
```
**Essencial** para datasets desbalanceados (90% vs 10%).

### early_stopping
Para o treinamento se n√£o houver melhoria:
- Usa 10% dos dados para valida√ß√£o (validation_fraction=0.1)
- Para ap√≥s 5 √©pocas sem melhoria (n_iter_no_change=5)
- Previne overfitting e economiza tempo

---

## üèÜ Resultados e Conclus√µes

### Performance Alcan√ßada
- **ROC-AUC**: 0.7963 (real√≠stico para o problema)
- **F1-Macro**: 0.6726 (bom balan√ßo entre classes)
- **Tempo de treino**: 0.2s (muito r√°pido)

### Compara√ß√£o com Gradient Boosting
SGD teve performance **similar ao K-NN** mas **inferior** a CatBoost/LightGBM:
- CatBoost: 86.69% vs SGD: 79.63%
- **Motivo**: SGD √© um modelo linear (n√£o captura intera√ß√µes n√£o-lineares)
- **Vantagem**: SGD √© **muito mais r√°pido** (~1s vs ~100s)

### Recomenda√ß√£o Final
- ‚úÖ **Para Produ√ß√£o (Performance)**: CatBoost ou LightGBM
- ‚úÖ **Para Produ√ß√£o (Velocidade)**: SGD Classifier
- ‚úÖ **Para Aprendizado Online**: SGD (pode ser atualizado incrementalmente)
- ‚úÖ **Para Interpretabilidade**: SGD (coeficientes transparentes)

---

## üìö Refer√™ncias

- [Scikit-learn SGD Documentation](https://scikit-learn.org/stable/modules/sgd.html)
- [SGD Classifier Theory](https://scikit-learn.org/stable/modules/linear_model.html#sgd)
- [Stochastic Gradient Descent Wikipedia](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)
- [StandardScaler Guide](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)
- [TimeSeriesSplit for Temporal Validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html)

---

## üë®‚Äçüíª Autor e Contato

**Projeto**: Cittamobi ML - Predi√ß√£o de Convers√£o de Usu√°rios
**Data**: Novembro 2025
**Status**: ‚úÖ Produ√ß√£o-Ready (Leak-Free)

---

## üìÑ Licen√ßa

Este projeto √© parte do portf√≥lio de Machine Learning Cittamobi.
