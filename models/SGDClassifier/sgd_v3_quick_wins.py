import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import (roc_auc_score, accuracy_score, precision_score, 
                             recall_score, f1_score, confusion_matrix, roc_curve)
from sklearn.model_selection import TimeSeriesSplit
from sklearn.linear_model import SGDClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.calibration import CalibratedClassifierCV
import warnings
import time
from google.cloud import bigquery
import os
warnings.filterwarnings('ignore')

os.makedirs('visualizations', exist_ok=True)
os.makedirs('reports', exist_ok=True)

# ===========================================================================
# SGD CLASSIFIER V3 - QUICK WINS
# ===========================================================================
print(f"\n{'='*80}")
print(f"SGD CLASSIFIER V3 - QUICK WINS (MELHORIAS R√ÅPIDAS)")
print(f"{'='*80}")
print(f"Melhorias implementadas:")
print(f"  1Ô∏è‚É£ Feature Engineering: geo_distance, stop_rate_per_sample, time_period")
print(f"  2Ô∏è‚É£ Calibra√ß√£o de Probabilidades: CalibratedClassifierCV")
print(f"  3Ô∏è‚É£ Threshold Otimizado: precision >= 0.35")
print(f"  4Ô∏è‚É£ Features Polinomiais: stop_event_rate^2, log(stop_event_rate)")
print(f"{'='*80}")

# ===========================================================================
# ETAPA 1: CARREGAR E PREPARAR DADOS
# ===========================================================================
print(f"\n{'='*70}")
print(f"ETAPA 1: CARREGAR DADOS")
print(f"{'='*70}")

project_id = "proj-ml-469320"
client = bigquery.Client(project=project_id)

query = """
    SELECT * FROM `proj-ml-469320.app_cittamobi.dataset-updated` 
    TABLESAMPLE SYSTEM (20 PERCENT)
    LIMIT 50000
"""

print("Carregando dados...")
df = client.query(query).to_dataframe()
print(f"‚úì Dados carregados: {len(df):,} registros")

target = "target"

df['event_timestamp'] = pd.to_datetime(df['event_timestamp'])
df = df.sort_values('event_timestamp').reset_index(drop=True)
print(f"‚úì Dados ordenados temporalmente")

# Features temporais b√°sicas
df['hour'] = df['event_timestamp'].dt.hour
df['day_of_week'] = df['event_timestamp'].dt.dayofweek
df['day_of_month'] = df['event_timestamp'].dt.day
df['month'] = df['event_timestamp'].dt.month

# Features c√≠clicas
df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)
df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)

print(f"‚úì Features temporais b√°sicas criadas")

# ===========================================================================
# ETAPA 2: EXPANDING WINDOWS
# ===========================================================================
print(f"\n{'='*70}")
print(f"ETAPA 2: EXPANDING WINDOWS (LEAK-FREE)")
print(f"{'='*70}")

df['user_hist_conversion_rate'] = 0.0
df['stop_hist_conversion_rate'] = 0.0
df['line_hist_conversion_rate'] = 0.0
df['user_hist_count'] = 0
df['stop_hist_count'] = 0
df['line_hist_count'] = 0
df['user_recency_days'] = 999
df['stop_recency_days'] = 999

print(f"üìä Calculando expanding windows...")
start_time = time.time()
sample_size = len(df)

for i in range(sample_size):
    if i % 5000 == 0 and i > 0:
        elapsed = time.time() - start_time
        eta = (elapsed / i) * (sample_size - i)
        print(f"   {i:,}/{sample_size:,} ({100*i/sample_size:.1f}%) - ETA: {eta/60:.1f} min")
    
    if i < 100:
        continue
    
    hist_data = df.iloc[:i].copy()
    current_row = df.iloc[i]
    
    user_hist = hist_data[hist_data['user_pseudo_id'] == current_row['user_pseudo_id']]
    if len(user_hist) > 0:
        df.at[i, 'user_hist_conversion_rate'] = user_hist[target].mean()
        df.at[i, 'user_hist_count'] = len(user_hist)
        last_event = user_hist['event_timestamp'].max()
        df.at[i, 'user_recency_days'] = (current_row['event_timestamp'] - last_event).days
    
    stop_hist = hist_data[hist_data['gtfs_stop_id'] == current_row['gtfs_stop_id']]
    if len(stop_hist) > 0:
        df.at[i, 'stop_hist_conversion_rate'] = stop_hist[target].mean()
        df.at[i, 'stop_hist_count'] = len(stop_hist)
        last_event = stop_hist['event_timestamp'].max()
        df.at[i, 'stop_recency_days'] = (current_row['event_timestamp'] - last_event).days
    
    if 'gtfs_route_id' in df.columns:
        line_hist = hist_data[hist_data['gtfs_route_id'] == current_row['gtfs_route_id']]
        if len(line_hist) > 0:
            df.at[i, 'line_hist_conversion_rate'] = line_hist[target].mean()
            df.at[i, 'line_hist_count'] = len(line_hist)

elapsed_time = time.time() - start_time
print(f"‚úì Expanding windows criadas em {elapsed_time/60:.1f} minutos")

df['user_stop_interaction'] = df['user_hist_conversion_rate'] * df['stop_hist_conversion_rate']
df['user_line_interaction'] = df['user_hist_conversion_rate'] * df['line_hist_conversion_rate']
df['stop_line_interaction'] = df['stop_hist_conversion_rate'] * df['line_hist_conversion_rate']
print(f"‚úì Features de intera√ß√£o criadas")

# ===========================================================================
# ETAPA 3: FEATURE ENGINEERING AVAN√áADO (QUICK WINS)
# ===========================================================================
print(f"\n{'='*70}")
print(f"ETAPA 3: FEATURE ENGINEERING V3 (QUICK WINS)")
print(f"{'='*70}")

# 1. DIST√ÇNCIA GEOGR√ÅFICA
if 'device_lat' in df.columns and 'device_lon' in df.columns:
    if 'stop_lat_event' in df.columns and 'stop_lon_event' in df.columns:
        df['geo_distance'] = np.sqrt(
            (df['device_lat'] - df['stop_lat_event'])**2 + 
            (df['device_lon'] - df['stop_lon_event'])**2
        )
        print(f"‚úì geo_distance criada")

# 2. RAZ√ïES E PROPOR√á√ïES (features mais inteligentes)
if 'stop_event_rate' in df.columns and 'stop_total_samples' in df.columns:
    df['stop_rate_per_sample'] = df['stop_event_rate'] / (df['stop_total_samples'] + 1)
    print(f"‚úì stop_rate_per_sample criada")

if 'stop_event_count' in df.columns and 'stop_total_samples' in df.columns:
    df['stop_density'] = df['stop_event_count'] / (df['stop_total_samples'] + 1)
    print(f"‚úì stop_density criada")

# 3. POLIN√îMIOS DAS TOP FEATURES
if 'stop_event_rate' in df.columns:
    df['stop_rate_squared'] = df['stop_event_rate'] ** 2
    df['stop_rate_log'] = np.log1p(df['stop_event_rate'])
    print(f"‚úì stop_rate_squared e stop_rate_log criadas")

# 4. PER√çODO DO DIA (mais granular que is_peak_hour)
df['time_period'] = pd.cut(df['hour'], 
                           bins=[0, 6, 9, 12, 14, 17, 19, 24],
                           labels=[0, 1, 2, 3, 4, 5, 6])  # numeric labels
df['time_period'] = df['time_period'].astype(float)
print(f"‚úì time_period criada (7 per√≠odos)")

# 5. INTERA√á√ÉO HORA X DIA DA SEMANA
df['hour_weekday_interaction'] = df['hour'] * df['day_of_week']
print(f"‚úì hour_weekday_interaction criada")

# 6. DIST√ÇNCIA DO HOR√ÅRIO DE PICO
df['distance_from_peak'] = df['hour'].apply(lambda h: min(abs(h-8), abs(h-18)))
print(f"‚úì distance_from_peak criada")

# 7. RECENCY SCORE (exponential decay)
df['user_recency_score'] = np.exp(-df['user_recency_days'] / 7)
df['stop_recency_score'] = np.exp(-df['stop_recency_days'] / 7)
print(f"‚úì recency_scores criadas (exponential decay)")

# 8. COMBINA√á√ïES INTELIGENTES
if 'user_recency_score' in df.columns and 'stop_event_rate' in df.columns:
    df['user_stop_recency_interaction'] = df['user_recency_score'] * df['stop_event_rate']
    print(f"‚úì user_stop_recency_interaction criada")

print(f"\n‚úÖ Total de novas features V3: 11")

# ===========================================================================
# ETAPA 4: LIMPEZA E PREPARA√á√ÉO
# ===========================================================================
print(f"\n{'='*70}")
print(f"ETAPA 4: LIMPEZA E PREPARA√á√ÉO")
print(f"{'='*70}")

# Tratar NaN nas features criadas
print(f"üìä Tratando valores NaN...")
before_nan = df.isna().sum().sum()
df = df.fillna(0)
after_nan = df.isna().sum().sum()
print(f"‚úì NaN tratados: {before_nan:,} ‚Üí {after_nan:,}")

if 'user_frequency' in df.columns:
    df = df[df['user_frequency'] >= 2].copy()
    print(f"‚úì Filtro user_frequency aplicado")

if 'device_lat' in df.columns and 'device_lon' in df.columns:
    df = df[~((df['device_lat'].isna()) | (df['device_lon'].isna()))].copy()
    df = df[~((df['device_lat'] == 0) & (df['device_lon'] == 0))].copy()
    print(f"‚úì Filtro coordenadas aplicado")

if 'dist_device_stop' in df.columns:
    df = df[df['dist_device_stop'] < df['dist_device_stop'].quantile(0.99)].copy()
    print(f"‚úì Filtro outliers aplicado")

print(f"‚úì Dados limpos: {len(df):,} registros")

# Selecionar features num√©ricas
numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()
numeric_features.remove(target)

id_cols = ['user_pseudo_id', 'gtfs_stop_id', 'gtfs_route_id', 'session_id']
for col in id_cols:
    if col in numeric_features:
        numeric_features.remove(col)
if 'event_timestamp' in numeric_features:
    numeric_features.remove('event_timestamp')

X = df[numeric_features].copy()
y = df[target].copy()

print(f"‚úì Features totais: {len(numeric_features)} (incluindo V3)")

target_dist = y.value_counts()
print(f"\n=== Distribui√ß√£o do Target ===")
for classe, count in target_dist.items():
    print(f"Classe {classe}: {count:,} ({100*count/len(y):.2f}%)")

# ===========================================================================
# ETAPA 5: DIVIS√ÉO TEMPORAL
# ===========================================================================
print(f"\n{'='*70}")
print(f"ETAPA 5: DIVIS√ÉO TEMPORAL")
print(f"{'='*70}")

tscv = TimeSeriesSplit(n_splits=3)
splits = list(tscv.split(X))
train_idx, test_idx = splits[-1]
X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

print(f"‚úì Train: {len(X_train):,} | Test: {len(X_test):,}")

# ===========================================================================
# ETAPA 6: TREINAR V2 (BASELINE PARA COMPARA√á√ÉO)
# ===========================================================================
print(f"\n{'='*70}")
print(f"ETAPA 6: BASELINE V2 (SEM MELHORIAS)")
print(f"{'='*70}")

# Usar apenas as 20 features do V2 para compara√ß√£o justa
v2_features = ['stop_event_rate', 'stop_event_count', 'stop_total_samples', 'hour', 
               'time_hour', 'is_weekend', 'stop_lat_event', 'headway_x_weekend',
               'time_day_of_month', 'device_lon', 'stop_lon_event', 'is_peak_hour',
               'hour_cos', 'time_month', 'device_lat', 'headway_x_hour',
               'stop_recency_days', 'stop_dist_mean', 'user_recency_days', 'time_day_of_week']

# Filtrar apenas features que existem
v2_features_available = [f for f in v2_features if f in X.columns]

X_train_v2 = X_train[v2_features_available]
X_test_v2 = X_test[v2_features_available]

print(f"üîÑ Treinando V2 baseline ({len(v2_features_available)} features)...")
start_time = time.time()

pipeline_v2 = Pipeline([
    ('scaler', StandardScaler()),
    ('sgd', SGDClassifier(
        loss='log_loss',
        penalty='l2',
        alpha=0.001,
        class_weight='balanced',
        learning_rate='optimal',
        max_iter=1000,
        early_stopping=True,
        validation_fraction=0.1,
        n_iter_no_change=5,
        random_state=42,
        n_jobs=-1
    ))
])

pipeline_v2.fit(X_train_v2, y_train)
y_pred_proba_v2 = pipeline_v2.predict_proba(X_test_v2)[:, 1]

# Threshold otimizado
thresholds = np.arange(0.1, 0.9, 0.05)
best_f1 = 0
best_thresh = 0.5
for t in thresholds:
    y_pred_temp = (y_pred_proba_v2 >= t).astype(int)
    f1_temp = f1_score(y_test, y_pred_temp, average='macro')
    if f1_temp > best_f1:
        best_f1 = f1_temp
        best_thresh = t

y_pred_v2 = (y_pred_proba_v2 >= best_thresh).astype(int)

roc_auc_v2 = roc_auc_score(y_test, y_pred_proba_v2)
f1_macro_v2 = f1_score(y_test, y_pred_v2, average='macro')
train_time_v2 = time.time() - start_time

print(f"\nüìä V2 BASELINE:")
print(f"   Features:     {len(v2_features_available)}")
print(f"   ROC-AUC:      {roc_auc_v2:.4f}")
print(f"   F1-Macro:     {f1_macro_v2:.4f}")
print(f"   Threshold:    {best_thresh:.2f}")
print(f"   Tempo:        {train_time_v2:.2f}s")

# ===========================================================================
# ETAPA 7: TREINAR V3 (COM MELHORIAS)
# ===========================================================================
print(f"\n{'='*70}")
print(f"ETAPA 7: V3 - COM FEATURE ENGINEERING AVAN√áADO")
print(f"{'='*70}")

print(f"üîÑ Treinando V3 ({len(numeric_features)} features)...")
start_time = time.time()

pipeline_v3 = Pipeline([
    ('scaler', StandardScaler()),
    ('sgd', SGDClassifier(
        loss='log_loss',
        penalty='l2',
        alpha=0.001,
        class_weight='balanced',
        learning_rate='optimal',
        max_iter=1000,
        early_stopping=True,
        validation_fraction=0.1,
        n_iter_no_change=5,
        random_state=42,
        n_jobs=-1
    ))
])

pipeline_v3.fit(X_train, y_train)
y_pred_proba_v3 = pipeline_v3.predict_proba(X_test)[:, 1]

# Threshold otimizado
best_f1 = 0
best_thresh = 0.5
for t in thresholds:
    y_pred_temp = (y_pred_proba_v3 >= t).astype(int)
    f1_temp = f1_score(y_test, y_pred_temp, average='macro')
    if f1_temp > best_f1:
        best_f1 = f1_temp
        best_thresh = t

y_pred_v3 = (y_pred_proba_v3 >= best_thresh).astype(int)

roc_auc_v3 = roc_auc_score(y_test, y_pred_proba_v3)
f1_macro_v3 = f1_score(y_test, y_pred_v3, average='macro')
train_time_v3 = time.time() - start_time

print(f"\nüìä V3 (SEM CALIBRA√á√ÉO):")
print(f"   Features:     {len(numeric_features)}")
print(f"   ROC-AUC:      {roc_auc_v3:.4f} ({roc_auc_v3 - roc_auc_v2:+.4f})")
print(f"   F1-Macro:     {f1_macro_v3:.4f} ({f1_macro_v3 - f1_macro_v2:+.4f})")
print(f"   Threshold:    {best_thresh:.2f}")
print(f"   Tempo:        {train_time_v3:.2f}s")

# ===========================================================================
# ETAPA 8: CALIBRA√á√ÉO DE PROBABILIDADES
# ===========================================================================
print(f"\n{'='*70}")
print(f"ETAPA 8: CALIBRA√á√ÉO DE PROBABILIDADES")
print(f"{'='*70}")

print(f"üîÑ Calibrando probabilidades (CalibratedClassifierCV)...")
start_time = time.time()

calibrated_v3 = CalibratedClassifierCV(
    pipeline_v3,
    method='sigmoid',
    cv=5
)

calibrated_v3.fit(X_train, y_train)
y_pred_proba_v3_cal = calibrated_v3.predict_proba(X_test)[:, 1]

# Threshold otimizado para precision >= 0.35
from sklearn.metrics import precision_recall_curve

precision, recall, thresholds_pr = precision_recall_curve(y_test, y_pred_proba_v3_cal)

# Encontrar threshold onde precision >= 0.35 e recall √© m√°ximo
valid_indices = np.where(precision >= 0.35)[0]
if len(valid_indices) > 0:
    best_idx = valid_indices[np.argmax(recall[valid_indices])]
    optimal_threshold = thresholds_pr[best_idx]
else:
    optimal_threshold = 0.5

y_pred_v3_cal = (y_pred_proba_v3_cal >= optimal_threshold).astype(int)

roc_auc_v3_cal = roc_auc_score(y_test, y_pred_proba_v3_cal)
accuracy_v3_cal = accuracy_score(y_test, y_pred_v3_cal)
precision_v3_cal = precision_score(y_test, y_pred_v3_cal, zero_division=0)
recall_v3_cal = recall_score(y_test, y_pred_v3_cal)
f1_v3_cal = f1_score(y_test, y_pred_v3_cal)
f1_macro_v3_cal = f1_score(y_test, y_pred_v3_cal, average='macro')
cm_v3_cal = confusion_matrix(y_test, y_pred_v3_cal)
train_time_v3_cal = time.time() - start_time

print(f"\nüìä V3 FINAL (COM CALIBRA√á√ÉO):")
print(f"   ROC-AUC:      {roc_auc_v3_cal:.4f} üéØ ({roc_auc_v3_cal - roc_auc_v2:+.4f})")
print(f"   F1-Macro:     {f1_macro_v3_cal:.4f} ({f1_macro_v3_cal - f1_macro_v2:+.4f})")
print(f"   Accuracy:     {accuracy_v3_cal:.4f}")
print(f"   Precision:    {precision_v3_cal:.4f}")
print(f"   Recall:       {recall_v3_cal:.4f}")
print(f"   F1-Score:     {f1_v3_cal:.4f}")
print(f"   Threshold:    {optimal_threshold:.2f} (precision >= 0.35)")
print(f"   Tempo:        {train_time_v3_cal:.2f}s")

print(f"\nüìä Matriz de Confus√£o:")
print(cm_v3_cal)
print(f"\nTrue Negatives:  {cm_v3_cal[0,0]:,}")
print(f"False Positives: {cm_v3_cal[0,1]:,}")
print(f"False Negatives: {cm_v3_cal[1,0]:,}")
print(f"True Positives:  {cm_v3_cal[1,1]:,}")

# ===========================================================================
# ETAPA 9: AN√ÅLISE DE FEATURES V3
# ===========================================================================
print(f"\n{'='*70}")
print(f"ETAPA 9: AN√ÅLISE DE FEATURES V3")
print(f"{'='*70}")

coefficients_v3 = pipeline_v3.named_steps['sgd'].coef_[0]
feature_importance_v3 = pd.DataFrame({
    'feature': numeric_features,
    'coefficient': coefficients_v3,
    'abs_coefficient': np.abs(coefficients_v3)
}).sort_values('abs_coefficient', ascending=False)

print(f"\nüìä TOP 25 FEATURES V3:")
print(f"{'='*70}")
for i, row in feature_importance_v3.head(25).iterrows():
    sign = "+" if row['coefficient'] > 0 else ""
    marker = "üÜï" if row['feature'] in ['geo_distance', 'stop_rate_per_sample', 'stop_density',
                                          'stop_rate_squared', 'stop_rate_log', 'time_period',
                                          'hour_weekday_interaction', 'distance_from_peak',
                                          'user_recency_score', 'stop_recency_score',
                                          'user_stop_recency_interaction'] else "  "
    print(f"{marker} {row['feature']:45s} | {sign}{row['coefficient']:+.6f}")

# ===========================================================================
# ETAPA 10: VISUALIZA√á√ïES
# ===========================================================================
print(f"\n{'='*70}")
print(f"ETAPA 10: VISUALIZA√á√ïES")
print(f"{'='*70}")

# 1. Compara√ß√£o V2 vs V3
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# ROC-AUC
ax1 = axes[0, 0]
models = ['V2\n(20 features)', 'V3\n(no calib)', 'V3\n(calibrated)']
roc_aucs = [roc_auc_v2, roc_auc_v3, roc_auc_v3_cal]
colors = ['steelblue', 'orange', 'forestgreen']
bars = ax1.bar(models, roc_aucs, color=colors, alpha=0.7)
ax1.set_ylabel('ROC-AUC', fontweight='bold')
ax1.set_title('ROC-AUC: V2 vs V3', fontweight='bold')
ax1.set_ylim(0.75, 0.85)
ax1.grid(axis='y', alpha=0.3)
for bar, val in zip(bars, roc_aucs):
    ax1.text(bar.get_x() + bar.get_width()/2, val + 0.003, f'{val:.4f}', 
             ha='center', va='bottom', fontweight='bold', fontsize=9)

# F1-Macro
ax2 = axes[0, 1]
f1_macros = [f1_macro_v2, f1_macro_v3, f1_macro_v3_cal]
bars = ax2.bar(models, f1_macros, color=colors, alpha=0.7)
ax2.set_ylabel('F1-Macro', fontweight='bold')
ax2.set_title('F1-Macro: V2 vs V3', fontweight='bold')
ax2.set_ylim(0.65, 0.75)
ax2.grid(axis='y', alpha=0.3)
for bar, val in zip(bars, f1_macros):
    ax2.text(bar.get_x() + bar.get_width()/2, val + 0.003, f'{val:.4f}', 
             ha='center', va='bottom', fontweight='bold', fontsize=9)

# Feature count
ax3 = axes[1, 0]
feature_counts = [len(v2_features_available), len(numeric_features), len(numeric_features)]
bars = ax3.bar(models, feature_counts, color=colors, alpha=0.7)
ax3.set_ylabel('N√∫mero de Features', fontweight='bold')
ax3.set_title('Features: V2 vs V3', fontweight='bold')
ax3.grid(axis='y', alpha=0.3)
for bar, val in zip(bars, feature_counts):
    ax3.text(bar.get_x() + bar.get_width()/2, val + 1, f'{int(val)}', 
             ha='center', va='bottom', fontweight='bold')

# Ganhos percentuais
ax4 = axes[1, 1]
gains = [0, (roc_auc_v3 - roc_auc_v2)*100, (roc_auc_v3_cal - roc_auc_v2)*100]
colors_gain = ['gray', 'orange', 'forestgreen']
bars = ax4.bar(models, gains, color=colors_gain, alpha=0.7)
ax4.set_ylabel('Ganho em ROC-AUC (%)', fontweight='bold')
ax4.set_title('Ganho Percentual vs V2', fontweight='bold')
ax4.axhline(y=0, color='black', linestyle='-', linewidth=1)
ax4.grid(axis='y', alpha=0.3)
for bar, val in zip(bars, gains):
    ax4.text(bar.get_x() + bar.get_width()/2, val + 0.1, f'{val:+.2f}%', 
             ha='center', va='bottom', fontweight='bold', fontsize=9)

plt.tight_layout()
plt.savefig('visualizations/sgd_v3_comparison.png', dpi=300, bbox_inches='tight')
plt.close()
print(f"‚úì Compara√ß√£o salva: visualizations/sgd_v3_comparison.png")

# 2. ROC Curves
fpr_v2, tpr_v2, _ = roc_curve(y_test, y_pred_proba_v2)
fpr_v3, tpr_v3, _ = roc_curve(y_test, y_pred_proba_v3)
fpr_v3_cal, tpr_v3_cal, _ = roc_curve(y_test, y_pred_proba_v3_cal)

plt.figure(figsize=(10, 8))
plt.plot(fpr_v2, tpr_v2, linewidth=2, label=f'V2 (AUC = {roc_auc_v2:.4f})', color='steelblue')
plt.plot(fpr_v3, tpr_v3, linewidth=2, label=f'V3 no calib (AUC = {roc_auc_v3:.4f})', 
         color='orange', linestyle='--')
plt.plot(fpr_v3_cal, tpr_v3_cal, linewidth=2, label=f'V3 calibrated (AUC = {roc_auc_v3_cal:.4f})', 
         color='forestgreen')
plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random', alpha=0.5)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')
plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')
plt.title('ROC Curve: V2 vs V3', fontsize=14, fontweight='bold')
plt.legend(loc="lower right", fontsize=11)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig('visualizations/roc_curve_v2_v3.png', dpi=300, bbox_inches='tight')
plt.close()
print(f"‚úì ROC Curve salva: visualizations/roc_curve_v2_v3.png")

# 3. Feature importance V3 (top 30)
plt.figure(figsize=(12, 10))
top_30 = feature_importance_v3.head(30)
colors_feat = ['green' if x > 0 else 'red' for x in top_30['coefficient']]
plt.barh(range(len(top_30)), top_30['coefficient'], color=colors_feat, alpha=0.7)
plt.yticks(range(len(top_30)), top_30['feature'])
plt.xlabel('Coeficiente', fontsize=12, fontweight='bold')
plt.ylabel('Feature', fontsize=12, fontweight='bold')
plt.title(f'Top 30 Features - SGD V3 ({len(numeric_features)} features)', 
          fontsize=14, fontweight='bold')
plt.axvline(x=0, color='black', linestyle='--', linewidth=1)
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.savefig('visualizations/feature_importance_v3.png', dpi=300, bbox_inches='tight')
plt.close()
print(f"‚úì Feature Importance salva: visualizations/feature_importance_v3.png")

# ===========================================================================
# ETAPA 11: SALVAR RESULTADOS
# ===========================================================================
print(f"\n{'='*70}")
print(f"ETAPA 11: SALVAR RESULTADOS")
print(f"{'='*70}")

# Salvar features V3
feature_importance_v3.to_csv('reports/sgd_v3_features.csv', index=False)
print(f"‚úì Features V3 salvas: reports/sgd_v3_features.csv")

# Relat√≥rio comparativo
with open('reports/sgd_v3_report.txt', 'w', encoding='utf-8') as f:
    f.write("="*80 + "\n")
    f.write("SGD CLASSIFIER V3 - QUICK WINS REPORT\n")
    f.write("="*80 + "\n\n")
    
    f.write("MELHORIAS IMPLEMENTADAS\n")
    f.write("-"*80 + "\n")
    f.write("1. Feature Engineering Avan√ßado:\n")
    f.write("   - geo_distance: dist√¢ncia euclidiana device <-> stop\n")
    f.write("   - stop_rate_per_sample: taxa normalizada por amostras\n")
    f.write("   - stop_density: densidade de eventos\n")
    f.write("   - stop_rate_squared: polin√¥mio quadr√°tico\n")
    f.write("   - stop_rate_log: transforma√ß√£o logar√≠tmica\n")
    f.write("   - time_period: 7 per√≠odos do dia\n")
    f.write("   - hour_weekday_interaction: intera√ß√£o hora x dia\n")
    f.write("   - distance_from_peak: dist√¢ncia do hor√°rio de pico\n")
    f.write("   - recency_scores: exponential decay\n")
    f.write("   - user_stop_recency_interaction: combina√ß√£o inteligente\n\n")
    
    f.write("2. Calibra√ß√£o de Probabilidades:\n")
    f.write("   - CalibratedClassifierCV com m√©todo sigmoid\n\n")
    
    f.write("3. Threshold Otimizado:\n")
    f.write("   - Precision >= 0.35 com recall maximizado\n\n")
    
    f.write("COMPARA√á√ÉO DE PERFORMANCE\n")
    f.write("-"*80 + "\n")
    f.write(f"{'Modelo':<25} | {'Features':<10} | {'ROC-AUC':<10} | {'Ganho':<10} | {'F1-Macro':<10}\n")
    f.write("-"*80 + "\n")
    f.write(f"{'V2 (Baseline)':<25} | {len(v2_features_available):<10} | {roc_auc_v2:<10.4f} | {'-':<10} | {f1_macro_v2:<10.4f}\n")
    f.write(f"{'V3 (Sem Calibra√ß√£o)':<25} | {len(numeric_features):<10} | {roc_auc_v3:<10.4f} | {(roc_auc_v3-roc_auc_v2)*100:+9.2f}% | {f1_macro_v3:<10.4f}\n")
    f.write(f"{'V3 (Com Calibra√ß√£o)':<25} | {len(numeric_features):<10} | {roc_auc_v3_cal:<10.4f} | {(roc_auc_v3_cal-roc_auc_v2)*100:+9.2f}% | {f1_macro_v3_cal:<10.4f}\n\n")
    
    f.write("M√âTRICAS DETALHADAS V3 FINAL\n")
    f.write("-"*80 + "\n")
    f.write(f"ROC-AUC:           {roc_auc_v3_cal:.4f}\n")
    f.write(f"Accuracy:          {accuracy_v3_cal:.4f}\n")
    f.write(f"Precision:         {precision_v3_cal:.4f}\n")
    f.write(f"Recall:            {recall_v3_cal:.4f}\n")
    f.write(f"F1-Score:          {f1_v3_cal:.4f}\n")
    f.write(f"F1-Macro:          {f1_macro_v3_cal:.4f}\n")
    f.write(f"Threshold:         {optimal_threshold:.2f}\n\n")
    
    f.write("MATRIZ DE CONFUS√ÉO\n")
    f.write("-"*80 + "\n")
    f.write(f"True Negatives:    {cm_v3_cal[0,0]:,}\n")
    f.write(f"False Positives:   {cm_v3_cal[0,1]:,}\n")
    f.write(f"False Negatives:   {cm_v3_cal[1,0]:,}\n")
    f.write(f"True Positives:    {cm_v3_cal[1,1]:,}\n\n")
    
    f.write("TOP 30 FEATURES V3\n")
    f.write("-"*80 + "\n")
    for i, row in feature_importance_v3.head(30).iterrows():
        marker = "[NEW]" if row['feature'] in ['geo_distance', 'stop_rate_per_sample', 'stop_density',
                                                 'stop_rate_squared', 'stop_rate_log', 'time_period',
                                                 'hour_weekday_interaction', 'distance_from_peak',
                                                 'user_recency_score', 'stop_recency_score',
                                                 'user_stop_recency_interaction'] else "     "
        f.write(f"{marker} {row['feature']:45s} | Coef: {row['coefficient']:+.6f}\n")

print(f"‚úì Relat√≥rio V3 salvo: reports/sgd_v3_report.txt")

# ===========================================================================
# CONCLUS√ÉO
# ===========================================================================
print(f"\n{'='*80}")
print(f"‚úÖ SGD CLASSIFIER V3 - QUICK WINS CONCLU√çDO!")
print(f"{'='*80}")

print(f"\nüéØ COMPARA√á√ÉO FINAL:")
print(f"   V2 (Baseline):         {roc_auc_v2:.4f} ROC-AUC ({len(v2_features_available)} features)")
print(f"   V3 (Sem Calibra√ß√£o):   {roc_auc_v3:.4f} ROC-AUC ({(roc_auc_v3-roc_auc_v2)*100:+.2f}%)")
print(f"   V3 (Com Calibra√ß√£o):   {roc_auc_v3_cal:.4f} ROC-AUC ({(roc_auc_v3_cal-roc_auc_v2)*100:+.2f}%) üéØ")

print(f"\nüí∞ GANHOS OBTIDOS:")
gain_pct = (roc_auc_v3_cal - roc_auc_v2) * 100
if gain_pct >= 2.0:
    print(f"   ‚úÖ EXCELENTE! Ganho de {gain_pct:+.2f}% supera meta de +2%")
elif gain_pct >= 1.0:
    print(f"   ‚úÖ BOM! Ganho de {gain_pct:+.2f}% dentro do esperado")
else:
    print(f"   ‚ö†Ô∏è  Ganho de {gain_pct:+.2f}% abaixo da meta")

print(f"\nüìÅ Arquivos gerados:")
print(f"   - visualizations/sgd_v3_comparison.png")
print(f"   - visualizations/roc_curve_v2_v3.png")
print(f"   - visualizations/feature_importance_v3.png")
print(f"   - reports/sgd_v3_report.txt")
print(f"   - reports/sgd_v3_features.csv")

print(f"\nüöÄ PR√ìXIMOS PASSOS:")
print(f"   FASE 2: Advanced Features (clusters geo, mais intera√ß√µes)")
print(f"   FASE 3: Ensembles (stacking com LightGBM)")
print(f"   Meta: 84-86% ROC-AUC")

print(f"\n‚úÖ QUICK WINS IMPLEMENTADO COM SUCESSO!")
