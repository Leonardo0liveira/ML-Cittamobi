# ğŸ“š MODELOS V5, V6 e V7 - DOCUMENTAÃ‡ÃƒO

## ğŸ¯ VisÃ£o Geral

Estas trÃªs novas versÃµes exploram **algoritmos alternativos de Gradient Boosting** e **tÃ©cnicas de ensemble** para comparar com o XGBoost (V4):

| VersÃ£o | Algoritmo | CaracterÃ­sticas Principais |
|--------|-----------|---------------------------|
| **V5** | LightGBM | Gradient Boosting otimizado, mais rÃ¡pido que XGBoost |
| **V6** | CatBoost | Tratamento automÃ¡tico de categÃ³ricas, auto_class_weights |
| **V7** | Ensemble Stacking | Combina XGBoost + LightGBM + CatBoost |

---

## ğŸ”¶ V5 - LightGBM

### O que Ã© LightGBM?
LightGBM Ã© uma implementaÃ§Ã£o de Gradient Boosting desenvolvida pela Microsoft que Ã©:
- **Mais rÃ¡pida** que XGBoost
- **Mais eficiente** em memÃ³ria
- **Excelente para datasets grandes**

### Principais ConfiguraÃ§Ãµes
```python
params = {
    'objective': 'binary',
    'num_leaves': 63,           # Ãrvores leaf-wise (mais eficiente)
    'max_depth': 18,
    'learning_rate': 0.015,
    'feature_fraction': 0.85,   # Subsample de features
    'bagging_fraction': 0.85,   # Subsample de linhas
    'scale_pos_weight': 12.05,  # Balanceamento
    'is_unbalance': True        # OtimizaÃ§Ã£o para classes desbalanceadas
}
```

### Como Executar
```bash
cd models/v5
python model_v5_lightgbm.py
```

### Arquivos Gerados
- `lightgbm_model_v5.txt` - Modelo treinado
- `visualizations/v5/` - Confusion matrix, ROC curve, feature importance
- `reports/v5_lightgbm_report.txt` - RelatÃ³rio completo

### Quando Usar LightGBM
âœ… Datasets grandes (>100k amostras)  
âœ… Necessidade de treinamento rÃ¡pido  
âœ… Features numÃ©ricas predominantes  
âœ… LimitaÃ§Ãµes de memÃ³ria  

---

## ğŸŸ¢ V6 - CatBoost

### O que Ã© CatBoost?
CatBoost Ã© uma implementaÃ§Ã£o de Gradient Boosting desenvolvida pela Yandex que Ã© especializada em:
- **Tratamento automÃ¡tico de features categÃ³ricas** (sem encoding)
- **Balanceamento automÃ¡tico de classes**
- **Menos propenso a overfitting**
- **Ordered boosting** (previne target leakage)

### Principais ConfiguraÃ§Ãµes
```python
model = CatBoostClassifier(
    iterations=500,
    learning_rate=0.015,
    depth=18,
    auto_class_weights='Balanced',  # ğŸ”¥ BALANCEAMENTO AUTOMÃTICO!
    l2_leaf_reg=1.0,
    subsample=0.85,
    rsm=0.85,                       # Random subspace method
    bootstrap_type='Bernoulli',
    task_type='CPU'                 # Use 'GPU' se disponÃ­vel
)
```

### Como Executar
```bash
cd models/v6
python model_v6_catboost.py
```

### Arquivos Gerados
- `catboost_model_v6.cbm` - Modelo treinado
- `visualizations/v6/` - Confusion matrix, ROC curve, feature importance
- `reports/v6_catboost_report.txt` - RelatÃ³rio completo

### Principais Vantagens
âœ… **NÃƒO precisa de Label Encoding** para categÃ³ricas  
âœ… **auto_class_weights='Balanced'** lida automaticamente com desbalanceamento  
âœ… **Trata missing values nativamente**  
âœ… **Menos hiperparÃ¢metros** para tunar  
âœ… **Reduz overfitting** naturalmente  

### Quando Usar CatBoost
âœ… Muitas features categÃ³ricas (IDs, nomes, etc.)  
âœ… Classes fortemente desbalanceadas  
âœ… Precisa de modelo robusto com pouco tuning  
âœ… Tem GPU disponÃ­vel (acelera muito)  

---

## ğŸ”· V7 - Ensemble Stacking

### O que Ã© Stacking?
Stacking Ã© uma tÃ©cnica de ensemble que:
1. Treina **mÃºltiplos modelos base** (Level 0)
2. Usa as prediÃ§Ãµes como features para um **meta-learner** (Level 1)
3. O meta-learner aprende **pesos Ã³timos** para cada modelo

### Arquitetura do V7
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   XGBoost   â”‚â”€â”€â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
                   â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”œâ”€â”€â”€â†’â”‚ Meta-Learner     â”‚â”€â”€â”€â”€â”€â†’â”‚ PrediÃ§Ã£o â”‚
â”‚  LightGBM   â”‚â”€â”€â”€â”€â”¤    â”‚ (LogisticReg)    â”‚      â”‚  Final   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  CatBoost   â”‚â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Como Funciona
1. **Treina 3 modelos** com configuraÃ§Ãµes otimizadas:
   - XGBoost (V4): Advanced features + Deep trees
   - LightGBM (V5): Gradient boosting rÃ¡pido
   - CatBoost (V6): Auto class weights

2. **Gera probabilidades** de cada modelo no conjunto de teste

3. **Meta-learner** (RegressÃ£o LogÃ­stica) aprende:
   - Quais modelos sÃ£o mais confiÃ¡veis
   - Como combinar suas prediÃ§Ãµes
   - Pesos Ã³timos para cada modelo

### Como Executar
```bash
cd models/v7
python model_v7_stacking.py
```

### Arquivos Gerados
- `xgboost_v7.json` - Modelo base 1
- `lightgbm_v7.txt` - Modelo base 2
- `catboost_v7.cbm` - Modelo base 3
- `meta_learner_v7.pkl` - Meta-learner
- `visualizations/v7/` - ComparaÃ§Ãµes e ROC curves
- `reports/v7_ensemble_report.txt` - RelatÃ³rio completo

### Vantagens do Stacking
âœ… **Combina pontos fortes** de cada algoritmo  
âœ… **Reduz variÃ¢ncia** - erros individuais se compensam  
âœ… **Mais robusto** que modelos individuais  
âœ… **Meta-learner aprende pesos automaticamente**  

### Desvantagens
âŒ **Mais lento** para treinar (3 modelos + meta-learner)  
âŒ **Mais complexo** para deployment  
âŒ **Requer mais memÃ³ria**  

### Quando Usar Stacking
âœ… MÃ¡xima performance Ã© prioridade  
âœ… Tem recursos computacionais suficientes  
âœ… Modelos base tÃªm performances similares  
âœ… Em competiÃ§Ãµes de Machine Learning  

---

## ğŸ“Š ComparaÃ§Ã£o Esperada

### Performance (estimativa baseada em literatura)

| MÃ©trica | V4 (XGBoost) | V5 (LightGBM) | V6 (CatBoost) | V7 (Ensemble) |
|---------|--------------|---------------|---------------|---------------|
| **ROC-AUC** | 0.9731 | ~0.97-0.98 | ~0.97-0.98 | **~0.98-0.99** ğŸ† |
| **F1-Macro** | 0.7760 | ~0.77-0.78 | ~0.77-0.78 | **~0.78-0.80** ğŸ† |
| **Precision** | 0.59 | ~0.58-0.60 | ~0.58-0.60 | **~0.60-0.62** ğŸ† |
| **Tempo Treino** | ~2-3 min | **~1-2 min** âš¡ | ~3-4 min | ~6-9 min |
| **Complexidade** | MÃ©dia | MÃ©dia | **Baixa** âœ… | Alta |

### CaracterÃ­sticas Especiais

| CaracterÃ­stica | V4 | V5 | V6 | V7 |
|----------------|----|----|----|----|
| Velocidade | â­â­â­ | â­â­â­â­â­ | â­â­â­ | â­â­ |
| CategÃ³ricas | Label Encoding | Label Encoding | **AutomÃ¡tico** âœ… | Mixed |
| Balanceamento | scale_pos_weight | scale_pos_weight | **Auto** âœ… | Inherited |
| Overfitting | MÃ©dio | MÃ©dio | **Baixo** âœ… | Muito Baixo |
| Deployment | FÃ¡cil | FÃ¡cil | FÃ¡cil | **Complexo** |

---

## ğŸš€ Guia de InstalaÃ§Ã£o

### DependÃªncias NecessÃ¡rias

```bash
# LightGBM
pip install lightgbm

# CatBoost
pip install catboost

# Stacking usa ambos + XGBoost (jÃ¡ instalado)
```

### Atualizar environment.yml

```yaml
dependencies:
  - xgboost=2.0.0
  - lightgbm=4.1.0
  - catboost=1.2.2
  - scikit-learn=1.3.0
  - pandas=2.0.0
  - numpy=1.24.0
```

---

## ğŸ“ˆ Como Escolher o Melhor Modelo

### Use **V5 (LightGBM)** se:
- âœ… Precisa de **velocidade** de treinamento
- âœ… Tem **dataset grande** (>500k amostras)
- âœ… LimitaÃ§Ãµes de **memÃ³ria**
- âœ… Features sÃ£o principalmente **numÃ©ricas**

### Use **V6 (CatBoost)** se:
- âœ… Tem **muitas features categÃ³ricas** (IDs, nomes, etc.)
- âœ… Quer **menos tuning** de hiperparÃ¢metros
- âœ… Precisa de **balanceamento automÃ¡tico**
- âœ… Tem **GPU disponÃ­vel**

### Use **V7 (Ensemble)** se:
- âœ… **MÃ¡xima performance** Ã© prioridade absoluta
- âœ… Tem **recursos computacionais** suficientes
- âœ… EstÃ¡ em uma **competiÃ§Ã£o** de ML
- âœ… Modelos individuais tÃªm ROC-AUC > 0.95

### Continue com **V4 (XGBoost)** se:
- âœ… Ã‰ o **padrÃ£o da indÃºstria** (mais adotado)
- âœ… Muita **documentaÃ§Ã£o** e suporte
- âœ… **Bom equilÃ­brio** entre todos os aspectos
- âœ… JÃ¡ estÃ¡ funcionando bem (0.9731 ROC-AUC)

---

## ğŸ¯ PrÃ³ximos Passos

1. **Execute os 3 modelos** e compare os resultados
2. **Analise as visualizaÃ§Ãµes** em `visualizations/v5/`, `v6/`, `v7/`
3. **Compare mÃ©tricas** nos relatÃ³rios gerados
4. **Escolha o melhor** baseado nas suas necessidades
5. **Documente** as conclusÃµes no README principal

---

## ğŸ“ Notas Importantes

- âš ï¸ **CatBoost** pode ser mais lento no primeiro treinamento (compila otimizaÃ§Ãµes)
- âš ï¸ **Ensemble** requer 3x mais espaÃ§o em disco (salva 3 modelos)
- âš ï¸ **LightGBM** pode ter resultados ligeiramente diferentes entre execuÃ§Ãµes
- âœ… Todos os modelos usam as **mesmas features do V4**
- âœ… **Threshold otimizado** para cada modelo individualmente

---

## ğŸ”— ReferÃªncias

- [LightGBM Documentation](https://lightgbm.readthedocs.io/)
- [CatBoost Documentation](https://catboost.ai/docs/)
- [Stacking Ensemble Methods](https://scikit-learn.org/stable/modules/ensemble.html#stacking)

---

**Criado em**: Novembro 2025  
**Ãšltima atualizaÃ§Ã£o**: Novembro 2025
