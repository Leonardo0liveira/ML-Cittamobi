{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c9750cf",
   "metadata": {},
   "source": [
    "# ETL Pipeline - Cittamobi ML Project\n",
    "\n",
    "Este notebook contÃ©m o processo completo de ETL (Extract, Transform, Load) para preparaÃ§Ã£o dos dados do projeto de Machine Learning da Cittamobi.\n",
    "\n",
    "## Etapas do Pipeline:\n",
    "1. **ImportaÃ§Ã£o de Bibliotecas**\n",
    "2. **Carregamento de Dados** (dataset_completo.csv + GTFS)\n",
    "3. **Limpeza e Tratamento de Dados**\n",
    "4. **ExtraÃ§Ã£o de Coordenadas**\n",
    "5. **Engenharia de Features**\n",
    "   - Features Temporais\n",
    "   - Features Geoespaciais\n",
    "   - Features de ServiÃ§o (GTFS)\n",
    "   - Features de AgregaÃ§Ã£o\n",
    "   - Features CÃ­clicas e de InteraÃ§Ã£o\n",
    "6. **CriaÃ§Ã£o da VariÃ¡vel Alvo** (lotacao_proxy_binaria)\n",
    "7. **ExportaÃ§Ã£o da Tabela Final**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8522d5",
   "metadata": {},
   "source": [
    "## 1. ImportaÃ§Ã£o de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba2269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holidays\n",
    "from geopy.distance import geodesic\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "# Ignorar avisos para manter a saÃ­da limpa\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar pandas para mostrar mais colunas\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"âœ… Bibliotecas importadas com sucesso.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726fd1e1",
   "metadata": {},
   "source": [
    "## 2. ConfiguraÃ§Ãµes e FunÃ§Ãµes Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e83bcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuso horÃ¡rio de SÃ£o Paulo\n",
    "TZ = 'America/Sao_Paulo'\n",
    "\n",
    "# Lista de feriados no Brasil (SP)\n",
    "br_holidays = holidays.Brazil(state='SP')\n",
    "\n",
    "def extract_coords(point_str):\n",
    "    \"\"\"Extrai latitude e longitude de uma string POINT(-lon -lat)\"\"\"\n",
    "    if pd.isna(point_str):\n",
    "        return None, None\n",
    "    \n",
    "    # Regex para encontrar os dois nÃºmeros (longitude e latitude)\n",
    "    match = re.search(r'POINT\\s*\\(\\s*(-?\\d+\\.?\\d*)\\s+(-?\\d+\\.?\\d*)\\s*\\)', str(point_str))\n",
    "    if match:\n",
    "        # A ordem no POINT Ã© (longitude, latitude)\n",
    "        lon = float(match.group(1))\n",
    "        lat = float(match.group(2))\n",
    "        return lat, lon\n",
    "    return None, None\n",
    "\n",
    "print(\"âœ… Constantes e funÃ§Ãµes auxiliares definidas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a007254",
   "metadata": {},
   "source": [
    "## 3. Carregamento de Dados\n",
    "\n",
    "### 3.1 Dataset Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04240a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o dataset principal\n",
    "try:\n",
    "    df_events = pd.read_csv('dataset_completo.csv')\n",
    "    \n",
    "    df_events['event_timestamp'] = pd.to_datetime(df_events['event_timestamp'], format='mixed').dt.tz_convert(TZ)\n",
    "    \n",
    "    # Filtrar apenas eventos relevantes\n",
    "    df_events = df_events[df_events['event_name'] == 'bstop_open'].copy()\n",
    "    \n",
    "    # Remover colunas que nÃ£o parecem Ãºteis para o modelo\n",
    "    cols_to_drop = ['event_date', 'ga_session_id', 'event_name', 'platform', \n",
    "                    'operating_system_version', 'mobile_brand_name', 'mobile_model_name', \n",
    "                    'mobile_marketing_name', 'traffic_source_source', 'ctm_country', \n",
    "                    'ctm_state', 'ctm_city', 'ctm_district', 'stop_address',\n",
    "                    'ctm_stop_fav', 'ctm_service_fav', 'headsign']\n",
    "    \n",
    "    # Filtrar colunas que realmente existem no DataFrame antes de dropar\n",
    "    existing_cols_to_drop = [col for col in cols_to_drop if col in df_events.columns]\n",
    "    df_events.drop(columns=existing_cols_to_drop, inplace=True)\n",
    "    \n",
    "    print(f\"âœ… Dataset 'dataset_completo.csv' carregado. Total de {len(df_events)} eventos 'bstop_open'.\")\n",
    "    print(\"   Timestamp convertido para 'America/Sao_Paulo'.\")\n",
    "    print(\"\\nðŸ“Š Preview dos dados:\")\n",
    "    print(df_events.head())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ Erro: Arquivo 'dataset_completo.csv' nÃ£o encontrado. Verifique o caminho.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Ocorreu um erro ao carregar ou processar 'dataset_completo.csv': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98b05d5",
   "metadata": {},
   "source": [
    "### 3.2 Arquivos GTFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c609da5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DicionÃ¡rio para armazenar os dataframes do GTFS\n",
    "gtfs_data = {}\n",
    "gtfs_files = [\n",
    "    'sptrans/routes.txt', \n",
    "    'sptrans/trips.txt', \n",
    "    'sptrans/stops.txt', \n",
    "    'sptrans/stop_times.txt', \n",
    "    'sptrans/frequencies.txt'\n",
    "]\n",
    "gtfs_keys = [f.split('/')[-1].replace('.txt', '') for f in gtfs_files]\n",
    "\n",
    "print(\"ðŸ“¥ Carregando arquivos GTFS...\")\n",
    "\n",
    "for file_path, key in zip(gtfs_files, gtfs_keys):\n",
    "    try:\n",
    "        gtfs_data[key] = pd.read_csv(\n",
    "            file_path, \n",
    "            header=0, \n",
    "            engine='python',\n",
    "            dtype=str\n",
    "        )\n",
    "        print(f\"  âœ… Arquivo '{file_path}' carregado com sucesso.\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"  âŒ Erro: Arquivo '{file_path}' nÃ£o encontrado.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Ocorreu um erro ao carregar '{file_path}': {e}\")\n",
    "\n",
    "# VariÃ¡veis individuais para facilitar o acesso\n",
    "df_routes = gtfs_data.get('routes')\n",
    "df_trips = gtfs_data.get('trips')\n",
    "df_stops = gtfs_data.get('stops')\n",
    "df_stop_times = gtfs_data.get('stop_times')\n",
    "df_frequencies = gtfs_data.get('frequencies')\n",
    "\n",
    "print(f\"\\nâœ… {len(gtfs_data)} arquivos GTFS carregados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4f8844",
   "metadata": {},
   "source": [
    "## 4. Limpeza e ExtraÃ§Ã£o de Coordenadas\n",
    "\n",
    "### 4.1 Coordenadas do GTFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80cd874",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_stops is not None:\n",
    "    try:\n",
    "        # Converter colunas de lat/lon para numÃ©rico\n",
    "        df_stops['stop_lat'] = pd.to_numeric(df_stops['stop_lat'])\n",
    "        df_stops['stop_lon'] = pd.to_numeric(df_stops['stop_lon'])\n",
    "        print(\"âœ… Coordenadas de 'df_stops' convertidas para numÃ©rico.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erro ao converter coordenadas de df_stops: {e}\")\n",
    "else:\n",
    "    print(\"âŒ ERRO: df_stops nÃ£o estÃ¡ carregado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662f4562",
   "metadata": {},
   "source": [
    "### 4.2 ExtraÃ§Ã£o de Coordenadas dos Eventos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecebdd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Aplicar a funÃ§Ã£o para extrair lat/lon\n",
    "    df_events[['device_lat', 'device_lon']] = df_events['device_point'].apply(\n",
    "        lambda x: pd.Series(extract_coords(x))\n",
    "    )\n",
    "    \n",
    "    df_events[['stop_lat_event', 'stop_lon_event']] = df_events['ctm_stop_point'].apply(\n",
    "        lambda x: pd.Series(extract_coords(x))\n",
    "    )\n",
    "    \n",
    "    # Dropar colunas originais de POINT\n",
    "    df_events.drop(columns=['device_point', 'ctm_stop_point'], inplace=True)\n",
    "    print(\"âœ… Coordenadas extraÃ­das de 'df_events' (device e stop).\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erro ao extrair coordenadas de df_events: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f07dcb1",
   "metadata": {},
   "source": [
    "### 4.3 CÃ¡lculo de DistÃ¢ncia Device-Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa761196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(row):\n",
    "    \"\"\"Calcula a distÃ¢ncia geodÃ©sica entre device e stop\"\"\"\n",
    "    if pd.isna(row['device_lat']) or pd.isna(row['stop_lat_event']):\n",
    "        return np.nan\n",
    "    \n",
    "    device_coords = (row['device_lat'], row['device_lon'])\n",
    "    stop_coords = (row['stop_lat_event'], row['stop_lon_event'])\n",
    "    \n",
    "    try:\n",
    "        return geodesic(device_coords, stop_coords).meters\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "print(\"\\nðŸ”„ Calculando 'dist_device_stop'...\")\n",
    "df_events['dist_device_stop'] = df_events.apply(calculate_distance, axis=1)\n",
    "print(\"âœ… Feature 'dist_device_stop' (em metros) criada.\")\n",
    "print(\"\\nðŸ“Š EstatÃ­sticas de distÃ¢ncia:\")\n",
    "print(df_events[['dist_device_stop']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9aa0a7",
   "metadata": {},
   "source": [
    "### 4.4 Spatial Join - Encontrar gtfs_stop_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b74d194",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”„ Iniciando 'spatial join' para encontrar o 'gtfs_stop_id' de cada evento...\")\n",
    "\n",
    "# 1. Preparar os dados de paradas do GTFS\n",
    "df_stops_valid = df_stops.dropna(subset=['stop_lat', 'stop_lon']).copy()\n",
    "df_stops_valid['stop_lat'] = df_stops_valid['stop_lat'].astype(float)\n",
    "df_stops_valid['stop_lon'] = df_stops_valid['stop_lon'].astype(float)\n",
    "\n",
    "stops_coords = df_stops_valid[['stop_lat', 'stop_lon']].values\n",
    "print(f\"  âœ… Criada Ã¡rvore de referÃªncia com {len(stops_coords)} paradas do GTFS.\")\n",
    "\n",
    "# 2. Preparar os dados de eventos\n",
    "df_events_valid = df_events.dropna(subset=['stop_lat_event', 'stop_lon_event']).copy()\n",
    "events_coords = df_events_valid[['stop_lat_event', 'stop_lon_event']].values\n",
    "print(f\"  âœ… {len(events_coords)} eventos vÃ¡lidos para o matching.\")\n",
    "\n",
    "# 3. Construir a Ãrvore (cKDTree)\n",
    "tree = cKDTree(stops_coords)\n",
    "\n",
    "# 4. Consultar a Ãrvore\n",
    "print(\"  ðŸ”„ Buscando paradas mais prÃ³ximas (Nearest Neighbors)...\")\n",
    "distances, idxs = tree.query(events_coords, k=1)\n",
    "print(\"  âœ… Busca concluÃ­da.\")\n",
    "\n",
    "# 5. Mapear os 'stop_id' de volta para os eventos\n",
    "matched_stop_ids = df_stops_valid.iloc[idxs]['stop_id'].values\n",
    "df_events_valid['gtfs_stop_id'] = matched_stop_ids\n",
    "\n",
    "# 6. Juntar o 'gtfs_stop_id' de volta ao DataFrame original\n",
    "df_events = df_events.merge(\n",
    "    df_events_valid[['gtfs_stop_id']], \n",
    "    left_index=True, \n",
    "    right_index=True, \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… 'gtfs_stop_id' adicionado ao df_events.\")\n",
    "print(f\"   Total de eventos com 'gtfs_stop_id' mapeado: {df_events['gtfs_stop_id'].notnull().sum()}\")\n",
    "print(f\"   Total de eventos sem 'gtfs_stop_id': {df_events['gtfs_stop_id'].isnull().sum()}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Preview de df_events com 'gtfs_stop_id':\")\n",
    "print(df_events.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd2efb1",
   "metadata": {},
   "source": [
    "## 5. CriaÃ§Ã£o da VariÃ¡vel Alvo (Target)\n",
    "\n",
    "### 5.1 Proxy de LotaÃ§Ã£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e3c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”„ Iniciando a criaÃ§Ã£o da variÃ¡vel alvo (lotacao_proxy)...\")\n",
    "\n",
    "# Janela de agregaÃ§Ã£o: 2 minutos\n",
    "agg_window = '2T'\n",
    "\n",
    "# Agrupar e contar usuÃ¡rios Ãºnicos\n",
    "df_events_indexed = df_events.set_index('event_timestamp')\n",
    "\n",
    "df_counts = df_events_indexed.groupby(\n",
    "    ['gtfs_stop_id', pd.Grouper(freq=agg_window)]\n",
    ")['user_pseudo_id'].nunique()\n",
    "\n",
    "df_proxy = df_counts.to_frame(name='user_count_5min')\n",
    "df_proxy.reset_index(inplace=True)\n",
    "\n",
    "print(f\"  âœ… AgregaÃ§Ã£o de {len(df_proxy)} janelas de {agg_window} por parada concluÃ­da.\")\n",
    "\n",
    "# Definir thresholds e criar classes\n",
    "bins = [0, 1, 2, np.inf]\n",
    "labels = ['Baixa', 'MÃ©dia', 'Alta']\n",
    "\n",
    "df_proxy['lotacao_proxy'] = pd.cut(\n",
    "    df_proxy['user_count_5min'], \n",
    "    bins=bins, \n",
    "    labels=labels, \n",
    "    right=True\n",
    ")\n",
    "print(\"  âœ… Classes 'Baixa', 'MÃ©dia', 'Alta' definidas.\")\n",
    "\n",
    "# Juntar o proxy de volta ao DataFrame original\n",
    "df_events['time_bin'] = df_events['event_timestamp'].dt.floor(agg_window)\n",
    "df_proxy.rename(columns={'event_timestamp': 'time_bin'}, inplace=True)\n",
    "\n",
    "df_final = df_events.merge(\n",
    "    df_proxy,\n",
    "    on=['time_bin', 'gtfs_stop_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"  âœ… VariÃ¡vel 'lotacao_proxy' juntada ao dataset principal.\")\n",
    "\n",
    "print(\"\\nðŸ“Š DistribuiÃ§Ã£o da VariÃ¡vel Alvo (lotacao_proxy):\")\n",
    "print(df_final['lotacao_proxy'].value_counts(normalize=True) * 100)\n",
    "\n",
    "print(f\"\\nâš ï¸  Eventos sem proxy (nulos): {df_final['lotacao_proxy'].isnull().sum()}\")\n",
    "\n",
    "df_final.drop(columns=['time_bin', 'user_count_5min'], inplace=True, errors='ignore')\n",
    "\n",
    "print(\"\\nðŸ“Š Preview do df_final:\")\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfb4a68",
   "metadata": {},
   "source": [
    "### 5.2 ConversÃ£o para ClassificaÃ§Ã£o BinÃ¡ria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4df1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”„ Adaptando para classificaÃ§Ã£o binÃ¡ria (DetecÃ§Ã£o de Evento Raro)...\")\n",
    "\n",
    "# Mapear classes para binÃ¡ria\n",
    "df_final['lotacao_proxy_binaria'] = df_final['lotacao_proxy'].map({\n",
    "    'Baixa': 'Baixa',\n",
    "    'MÃ©dia': 'Nao_Baixa',\n",
    "    'Alta': 'Nao_Baixa'\n",
    "})\n",
    "\n",
    "print(\"\\nðŸ“Š DistribuiÃ§Ã£o da Nova VariÃ¡vel Alvo (BinÃ¡ria):\")\n",
    "print(df_final['lotacao_proxy_binaria'].value_counts(normalize=True) * 100)\n",
    "\n",
    "print(\"\\nðŸ“Š Contagem Absoluta (BinÃ¡ria):\")\n",
    "print(df_final['lotacao_proxy_binaria'].value_counts())\n",
    "\n",
    "# Remover a coluna proxy original de 3 classes\n",
    "df_final.drop(columns=['lotacao_proxy'], inplace=True)\n",
    "\n",
    "print(\"\\nâœ… VariÃ¡vel alvo re-processada para classificaÃ§Ã£o binÃ¡ria.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a51c497",
   "metadata": {},
   "source": [
    "## 6. Engenharia de Features\n",
    "\n",
    "### 6.1 Features Temporais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7565f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”„ Criando features temporais...\")\n",
    "\n",
    "# Garantir que o timestamp Ã© o Ã­ndice\n",
    "if 'event_timestamp' in df_final.columns:\n",
    "    df_final.set_index('event_timestamp', inplace=True)\n",
    "\n",
    "# Features de Tempo\n",
    "df_final['time_hour'] = df_final.index.hour\n",
    "df_final['time_day_of_week'] = df_final.index.dayofweek  # Segunda=0, Domingo=6\n",
    "df_final['time_day_of_month'] = df_final.index.day\n",
    "df_final['time_month'] = df_final.index.month\n",
    "\n",
    "# Ã‰ Feriado?\n",
    "df_final['is_holiday'] = pd.Series(df_final.index.date).apply(lambda x: x in br_holidays).values\n",
    "df_final['is_holiday'] = df_final['is_holiday'].astype(int)\n",
    "\n",
    "# Ã‰ Fim de Semana?\n",
    "df_final['is_weekend'] = (df_final['time_day_of_week'] >= 5).astype(int)  # 5=SÃ¡bado, 6=Domingo\n",
    "\n",
    "# Ã‰ Hora de Pico?\n",
    "def is_peak(hour):\n",
    "    if (hour >= 6 and hour < 9) or (hour >= 17 and hour < 19):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "df_final['is_peak_hour'] = df_final['time_hour'].apply(is_peak)\n",
    "\n",
    "# Resetar o Ã­ndice\n",
    "df_final.reset_index(inplace=True)\n",
    "\n",
    "print(\"âœ… Features temporais criadas:\")\n",
    "print(\"   - time_hour, time_day_of_week, time_day_of_month, time_month\")\n",
    "print(\"   - is_holiday, is_weekend, is_peak_hour\")\n",
    "\n",
    "print(\"\\nðŸ“Š Preview:\")\n",
    "print(df_final[['event_timestamp', 'time_hour', 'time_day_of_week', 'is_holiday', 'is_peak_hour']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e1ce44",
   "metadata": {},
   "source": [
    "### 6.2 Features de ServiÃ§o (Headway do GTFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317e2c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”„ Criando features de headway (frequÃªncia de Ã´nibus)...\")\n",
    "\n",
    "try:\n",
    "    # 1. Limpar e Converter Frequencies\n",
    "    df_freq = df_frequencies.copy()\n",
    "    df_freq['headway_secs'] = pd.to_numeric(df_freq['headway_secs'])\n",
    "    df_freq['start_hour'] = df_freq['start_time'].str.split(':').str[0].astype(int)\n",
    "    \n",
    "    # 2. Limpar Stop Times\n",
    "    df_st = df_stop_times.copy()\n",
    "    \n",
    "    # 3. Juntar FrequÃªncias com Stop Times\n",
    "    print(\"  ðŸ”„ Iniciando merge (stop_times + frequencies)...\")\n",
    "    df_freq_merged = df_st[['trip_id', 'stop_id']].merge(\n",
    "        df_freq[['trip_id', 'start_hour', 'headway_secs']],\n",
    "        on='trip_id',\n",
    "        how='inner'\n",
    "    )\n",
    "    print(\"  âœ… Merge concluÃ­do.\")\n",
    "    \n",
    "    df_freq_merged = df_freq_merged.drop_duplicates(subset=['stop_id', 'start_hour', 'trip_id'])\n",
    "    \n",
    "    # 4. Calcular Headway MÃ©dio por Parada/Hora\n",
    "    print(\"  ðŸ”„ Calculando headway mÃ©dio por parada/hora...\")\n",
    "    df_headway_avg = df_freq_merged.groupby(\n",
    "        ['stop_id', 'start_hour']\n",
    "    )['headway_secs'].mean().to_frame(name='headway_avg_stop_hour')\n",
    "    \n",
    "    df_headway_avg.reset_index(inplace=True)\n",
    "    \n",
    "    print(f\"  âœ… Headway mÃ©dio calculado para {len(df_headway_avg)} combinaÃ§Ãµes.\")\n",
    "\n",
    "    # 5. Juntar ao df_final\n",
    "    print(\"  ðŸ”„ Juntando feature 'headway' ao dataset final...\")\n",
    "    df_final = df_final.merge(\n",
    "        df_headway_avg,\n",
    "        left_on=['gtfs_stop_id', 'time_hour'],\n",
    "        right_on=['stop_id', 'start_hour'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    df_final.drop(columns=['stop_id', 'start_hour'], inplace=True, errors='ignore')\n",
    "    \n",
    "    # Preencher nulos (paradas sem frequÃªncia) com valor alto (baixa frequÃªncia)\n",
    "    headway_nulos = df_final['headway_avg_stop_hour'].isnull().sum()\n",
    "    print(f\"  â„¹ï¸  {headway_nulos} eventos sem headway (preenchidos com 3600s).\")\n",
    "    df_final['headway_avg_stop_hour'].fillna(3600, inplace=True)\n",
    "    \n",
    "    print(\"  âœ… Feature 'headway_avg_stop_hour' criada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"  âš ï¸  Erro ao processar headway: {e}\")\n",
    "    print(\"  â„¹ï¸  Criando feature 'headway_avg_stop_hour' vazia (3600s).\")\n",
    "    df_final['headway_avg_stop_hour'] = 3600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf38430",
   "metadata": {},
   "source": [
    "### 6.3 Features CÃ­clicas (Temporal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2601c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”„ Criando features cÃ­clicas...\")\n",
    "\n",
    "# Features cÃ­clicas para hora\n",
    "df_final['hour_sin'] = np.sin(2 * np.pi * df_final['time_hour'] / 24)\n",
    "df_final['hour_cos'] = np.cos(2 * np.pi * df_final['time_hour'] / 24)\n",
    "\n",
    "# Features cÃ­clicas para dia da semana\n",
    "df_final['day_sin'] = np.sin(2 * np.pi * df_final['time_day_of_week'] / 7)\n",
    "df_final['day_cos'] = np.cos(2 * np.pi * df_final['time_day_of_week'] / 7)\n",
    "\n",
    "print(\"âœ… Features cÃ­clicas criadas:\")\n",
    "print(\"   - hour_sin, hour_cos\")\n",
    "print(\"   - day_sin, day_cos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3352dfd2",
   "metadata": {},
   "source": [
    "### 6.4 Features de InteraÃ§Ã£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f91234",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”„ Criando features de interaÃ§Ã£o...\")\n",
    "\n",
    "# InteraÃ§Ãµes entre features\n",
    "df_final['headway_x_hour'] = df_final['headway_avg_stop_hour'] * df_final['time_hour']\n",
    "df_final['headway_x_weekend'] = df_final['headway_avg_stop_hour'] * df_final['is_weekend']\n",
    "df_final['dist_x_peak'] = df_final['dist_device_stop'] * df_final['is_peak_hour']\n",
    "df_final['dist_x_weekend'] = df_final['dist_device_stop'] * df_final['is_weekend']\n",
    "\n",
    "print(\"âœ… Features de interaÃ§Ã£o criadas:\")\n",
    "print(\"   - headway_x_hour, headway_x_weekend\")\n",
    "print(\"   - dist_x_peak, dist_x_weekend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608f380c",
   "metadata": {},
   "source": [
    "### 6.5 Features de AgregaÃ§Ã£o por Parada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2109ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”„ Criando features de agregaÃ§Ã£o por parada...\")\n",
    "\n",
    "# 1. Taxa de eventos por parada\n",
    "stop_event_rate = df_final.groupby('gtfs_stop_id')['lotacao_proxy_binaria'].agg([\n",
    "    ('stop_event_rate', lambda x: (x == 'Nao_Baixa').mean()),\n",
    "    ('stop_event_count', lambda x: (x == 'Nao_Baixa').sum()),\n",
    "    ('stop_total_samples', 'count')\n",
    "]).reset_index()\n",
    "\n",
    "df_final = df_final.merge(stop_event_rate, on='gtfs_stop_id', how='left')\n",
    "\n",
    "# 2. EstatÃ­sticas de distÃ¢ncia por parada\n",
    "stop_dist_stats = df_final.groupby('gtfs_stop_id')['dist_device_stop'].agg([\n",
    "    ('stop_dist_mean', 'mean'),\n",
    "    ('stop_dist_std', 'std')\n",
    "]).reset_index()\n",
    "\n",
    "df_final = df_final.merge(stop_dist_stats, on='gtfs_stop_id', how='left')\n",
    "\n",
    "# 3. EstatÃ­sticas de headway por parada\n",
    "stop_headway_stats = df_final.groupby('gtfs_stop_id')['headway_avg_stop_hour'].agg([\n",
    "    ('stop_headway_mean', 'mean'),\n",
    "    ('stop_headway_std', 'std')\n",
    "]).reset_index()\n",
    "\n",
    "df_final = df_final.merge(stop_headway_stats, on='gtfs_stop_id', how='left')\n",
    "\n",
    "# Preencher NaN em std\n",
    "df_final['stop_dist_std'].fillna(0, inplace=True)\n",
    "df_final['stop_headway_std'].fillna(0, inplace=True)\n",
    "\n",
    "print(\"âœ… Features de agregaÃ§Ã£o criadas:\")\n",
    "print(\"   - stop_event_rate, stop_event_count, stop_total_samples\")\n",
    "print(\"   - stop_dist_mean, stop_dist_std\")\n",
    "print(\"   - stop_headway_mean, stop_headway_std\")\n",
    "\n",
    "print(\"\\nðŸ“Š Preview:\")\n",
    "print(df_final[['gtfs_stop_id', 'stop_event_rate', 'stop_dist_mean', 'stop_headway_mean']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3baabbd",
   "metadata": {},
   "source": [
    "## 7. PreparaÃ§Ã£o Final e Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea951484",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”„ Preparando dataset final...\")\n",
    "\n",
    "# 1. Remover linhas sem rÃ³tulo\n",
    "nulos_antes = df_final['lotacao_proxy_binaria'].isnull().sum()\n",
    "if nulos_antes > 0:\n",
    "    df_final.dropna(subset=['lotacao_proxy_binaria'], inplace=True)\n",
    "    print(f\"  â„¹ï¸  {nulos_antes} linhas sem rÃ³tulo removidas.\")\n",
    "\n",
    "# 2. Encoding da VariÃ¡vel Alvo\n",
    "target_map = {'Baixa': 0, 'Nao_Baixa': 1}\n",
    "df_final['target'] = df_final['lotacao_proxy_binaria'].map(target_map)\n",
    "print(\"  âœ… VariÃ¡vel alvo mapeada: 'Baixa'=0, 'Nao_Baixa'=1\")\n",
    "\n",
    "# 3. Encoding de VariÃ¡veis CategÃ³ricas\n",
    "categorical_features = ['gtfs_stop_id', 'time_day_of_week', 'time_hour']\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    df_final[col] = le.fit_transform(df_final[col].astype(str))\n",
    "    print(f\"  âœ… Feature '{col}' codificada como categÃ³rica.\")\n",
    "\n",
    "# 4. Ordenar por tempo (CRUCIAL para validaÃ§Ã£o temporal)\n",
    "df_final.sort_values('event_timestamp', inplace=True)\n",
    "\n",
    "print(f\"\\nâœ… Dataset final preparado!\")\n",
    "print(f\"   Total de amostras: {len(df_final)}\")\n",
    "print(f\"   Eventos positivos (1): {df_final['target'].sum()} ({df_final['target'].mean()*100:.2f}%)\")\n",
    "print(f\"   Total de features: {len(df_final.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521b73cb",
   "metadata": {},
   "source": [
    "## 8. ExportaÃ§Ã£o da Tabela Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e700c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ’¾ Exportando dataset processado...\")\n",
    "\n",
    "# Salvar em CSV\n",
    "output_file = 'dataset_cittamobi_final.csv'\n",
    "df_final.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"âœ… Dataset salvo em: {output_file}\")\n",
    "print(f\"   Tamanho: {len(df_final)} linhas x {len(df_final.columns)} colunas\")\n",
    "\n",
    "# Resumo das features criadas\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“‹ RESUMO DAS FEATURES CRIADAS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1ï¸âƒ£  Features Temporais (11):\")\n",
    "print(\"   - time_hour, time_day_of_week, time_day_of_month, time_month\")\n",
    "print(\"   - is_holiday, is_weekend, is_peak_hour\")\n",
    "print(\"   - hour_sin, hour_cos, day_sin, day_cos (cÃ­clicas)\")\n",
    "\n",
    "print(\"\\n2ï¸âƒ£  Features Geoespaciais (5):\")\n",
    "print(\"   - dist_device_stop (distÃ¢ncia em metros)\")\n",
    "print(\"   - device_lat, device_lon, stop_lat_event, stop_lon_event\")\n",
    "\n",
    "print(\"\\n3ï¸âƒ£  Features de ServiÃ§o (GTFS) (2):\")\n",
    "print(\"   - gtfs_stop_id (ID da parada mais prÃ³xima)\")\n",
    "print(\"   - headway_avg_stop_hour (frequÃªncia mÃ©dia de Ã´nibus)\")\n",
    "\n",
    "print(\"\\n4ï¸âƒ£  Features de InteraÃ§Ã£o (4):\")\n",
    "print(\"   - headway_x_hour, headway_x_weekend\")\n",
    "print(\"   - dist_x_peak, dist_x_weekend\")\n",
    "\n",
    "print(\"\\n5ï¸âƒ£  Features de AgregaÃ§Ã£o por Parada (7):\")\n",
    "print(\"   - stop_event_rate, stop_event_count, stop_total_samples\")\n",
    "print(\"   - stop_dist_mean, stop_dist_std\")\n",
    "print(\"   - stop_headway_mean, stop_headway_std\")\n",
    "\n",
    "print(\"\\n6ï¸âƒ£  VariÃ¡vel Alvo (2):\")\n",
    "print(\"   - target (0=Baixa, 1=Nao_Baixa)\")\n",
    "print(\"   - lotacao_proxy_binaria (categoria original)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ‰ ETL PIPELINE CONCLUÃDO COM SUCESSO!\")\n",
    "print(\"=\"*70)\n",
    "print(\"O dataset estÃ¡ pronto para treinamento de modelos de ML.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40887154",
   "metadata": {},
   "source": [
    "## 9. VerificaÃ§Ã£o Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebf9683",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ” VERIFICAÃ‡ÃƒO FINAL DO DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nðŸ“Š InformaÃ§Ãµes Gerais:\")\n",
    "print(df_final.info())\n",
    "\n",
    "print(\"\\nðŸ“ˆ EstatÃ­sticas Descritivas:\")\n",
    "print(df_final.describe())\n",
    "\n",
    "print(\"\\nðŸŽ¯ DistribuiÃ§Ã£o do Target:\")\n",
    "print(df_final['target'].value_counts())\n",
    "print(f\"\\nBalanceamento:\")\n",
    "print(df_final['target'].value_counts(normalize=True) * 100)\n",
    "\n",
    "print(\"\\nâš ï¸  Valores Nulos por Coluna:\")\n",
    "nulos = df_final.isnull().sum()\n",
    "if nulos.sum() > 0:\n",
    "    print(nulos[nulos > 0])\n",
    "else:\n",
    "    print(\"   Nenhum valor nulo encontrado! âœ…\")\n",
    "\n",
    "print(\"\\nðŸ“‹ Preview do Dataset Final:\")\n",
    "print(df_final.head(10))\n",
    "\n",
    "print(\"\\nâœ… VerificaÃ§Ã£o concluÃ­da!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
